%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Lauro Cesar Araujo at 2013-08-21 10:57:09 -0300 


%% Saved with string encoding Unicode (UTF-8) 



%% Minhas citações

@Article{Galli,
AUTHOR = {Galli, Carlo and Cusano, Claudio and Meleti, Marco and Donos, Nikolaos and Calciolari, Elena},
TITLE = {Topic Modeling for Faster Literature Screening Using Transformer-Based Embeddings},
JOURNAL = {Metrics},
VOLUME = {1},
YEAR = {2024},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/3042-5042/1/1/2},
ISSN = {3042-5042},
DOI = {10.3390/metrics1010002}
}

@article{Datchanamoorthy_2023,
  title={TEXT MINING: CLUSTERING USING BERT AND PROBABILISTIC TOPIC MODELING},
  author={Kavitha Datchanamoorthy and Anandha Mala. G. S and Padmavathi. B},
  journal={Social Informatics Journal},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267122800}
}

@misc{Devlin,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{Mohammadi,
author = {Mohammadi, Ehsan and Karami, Amir},
year = {2020},
month = {06},
pages = {},
title = {Exploring research trends in big data across disciplines: A text mining analysis},
volume = {48},
journal = {Journal of Information Science},
doi = {10.1177/0165551520932855}
}

@article{Xie,
title = {Monolingual and multilingual topic analysis using LDA and BERT embeddings},
journal = {Journal of Informetrics},
volume = {14},
number = {3},
pages = {101055},
year = {2020},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2020.101055},
url = {https://www.sciencedirect.com/science/article/pii/S1751157719305127},
author = {Qing Xie and Xinyuan Zhang and Ying Ding and Min Song},
keywords = {Topic evolution, Monolingual, Multilingual, Topic similarity relations, LDA, BERT, Embedding},
}

@misc{vaswani_2017,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{Dillan_2023,
  author={Dillan, Timothy and Fudholi, Dhomas Hatta},
  journal={IEEE Access}, 
  title={LDAViewer: An Automatic Language-Agnostic System for Discovering State-of-the-Art Topics in Research Using Topic Modeling, Bidirectional Encoder Representations From Transformers, and Entity Linking}, 
  year={2023},
  volume={11},
  number={},
  pages={59142-59163},
  keywords={Bibliographies;Systematics;Market research;Databases;Bibliometrics;Transformers;Filtering;Topic modeling;state-of-the-art discovery;development of knowledge;latent-dirichlet allocation;bidirectional encoder representations from transformers;entity linking},
  doi={10.1109/ACCESS.2023.3285116}
}



@Article{Weng_2022,
AUTHOR = {Weng, Min-Hsien and Wu, Shaoqun and Dyer, Mark},
TITLE = {Identification and Visualization of Key Topics in Scientific Publications with Transformer-Based Language Models and Document Clustering Methods},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {21},
ARTICLE-NUMBER = {11220},
URL = {https://www.mdpi.com/2076-3417/12/21/11220},
ISSN = {2076-3417},
DOI = {10.3390/app122111220}
}

@inbook{Glazkova,
author = {Glazkova, Anna},
year = {2021},
month = {05},
pages = {98-105},
title = {Identifying Topics of Scientific Articles with BERT-Based Approaches and Topic Modeling},
isbn = {978-3-030-75014-5},
doi = {10.1007/978-3-030-75015-2_10}
}

@inproceedings{Radford,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}


@Article{Kim,
  author={Keungoui Kim and Dieter F. Kogler and Sira Maliphol},
  title={{Identifying interdisciplinary emergence in the science of science: combination of network analysis and BERTopic}},
  journal={Palgrave Communications},
  year=2024,
  volume={11},
  number={1},
  pages={1-15},
  month={December},
  keywords={},
  doi={10.1057/s41599-024-03044-},
  url={https://ideas.repec.org/a/pal/palcom/v11y2024i1d10.1057_s41599-024-03044-y.html}
}

@misc{Mikolov,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@inproceedings{Pennington,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}


@article{George,
  author = {George, Lijimol and Sumathy, P.},
  title = {An integrated clustering and BERT framework for improved topic modeling},
  journal = {International Journal of Information Technology},
  year = {2023},
  volume = {15},
  number = {4},
  pages = {2187--2195},
  doi = {10.1007/s41870-023-01268-w},
  url = {https://doi.org/10.1007/s41870-023-01268-w}, 
}

@misc{Peters,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1802.05365}, 
}

@article{Jung,
    doi = {10.1371/journal.pone.0304680},
    author = {Jung, Hae Sun AND Lee, Haein AND Woo, Young Seok AND Baek, Seo Yeon AND Kim, Jang Hyun},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Expansive data, extensive model: Investigating discussion topics around LLM through unsupervised machine learning in academic papers and news},
    year = {2024},
    month = {05},
    volume = {19},
    url = {https://doi.org/10.1371/journal.pone.0304680},
    pages = {1-18},
    abstract = {This study presents a comprehensive exploration of topic modeling methods tailored for large language model (LLM) using data obtained from Web of Science and LexisNexis from June 1, 2020, to December 31, 2023. The data collection process involved queries focusing on LLMs, including “Large language model,” “LLM,” and “ChatGPT.” Various topic modeling approaches were evaluated based on performance metrics, including diversity and coherence. latent Dirichlet allocation (LDA), nonnegative matrix factorization (NMF), combined topic models (CTM), and bidirectional encoder representations from Transformers topic (BERTopic) were employed for performance evaluation. Evaluation metrics were computed across platforms, with BERTopic demonstrating superior performance in diversity and coherence across both LexisNexis and Web of Science. The experiment result reveals that news articles maintain a balanced coverage across various topics and mainly focus on efforts to utilize LLM in specialized domains. Conversely, research papers are more concise and concentrated on the technology itself, emphasizing technical aspects. Through the insights gained in this study, it becomes possible to investigate the future path and the challenges that LLMs should tackle. Additionally, they could offer considerable value to enterprises that utilize LLMs to deliver services.},
    number = {5},
}

@article{Blei,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = apr,
pages = {77–84},
numpages = {8}
}

@Inbook{Bengio,
author="Bengio, Yoshua
and Schwenk, Holger
and Sen{\'e}cal, Jean-S{\'e}bastien
and Morin, Fr{\'e}deric
and Gauvain, Jean-Luc",
editor="Holmes, Dawn E.
and Jain, Lakhmi C.",
title="Neural Probabilistic Language Models",
bookTitle="Innovations in Machine Learning: Theory and Applications",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="137--186",
abstract="A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.",
isbn="978-3-540-33486-6",
doi="10.1007/3-540-33486-6_6",
url="https://doi.org/10.1007/3-540-33486-6_6"
}


@article{Polyzos,
title = {Twitter and market efficiency in energy markets: Evidence using LDA clustered topic extraction},
journal = {Energy Economics},
volume = {114},
pages = {106264},
year = {2022},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2022.106264},
url = {https://www.sciencedirect.com/science/article/pii/S0140988322004017},
author = {Efstathios Polyzos and Fang Wang},
keywords = {Market efficiency, Twitter, Energy markets, LDA topic extraction},
abstract = {We use an extended sample of tweets relating to energy markets in order to examine and quantify the existence of market efficiency. The tweets are used as a proxy for publicly available information and we examine the degree to which this information determines market movements on the next trading day for nine energy market indices. We mine the topics of increasing and decreasing days using latent Dirichlet allocation and find that the topics of tweets in increasing and decreasing days differ. We validate our approach by feeding the extracted topics into three classifier machines and find that the classifiers provide forecasts on market movements with accuracy 57.83% (39.02%) in bull (bear) markets. Our findings support the presence of semi-strong efficiency, since we find evidence of price movements not reflecting public information, while the asymmetry of forecast accuracy over increasing and decreasing markets suggests a different rate of information propagation across market regimes. Our findings can provide useful input to valuation models linked to market efficiency.}
}

@article{Mifrah,
author = {Mifrah, Sara and Benlahmar, EL Habib},
year = {2020},
month = {08},
pages = {},
title = {Topic Modeling Coherence: A Comparative Study between LDA and NMF Models using COVID’19 Corpus},
journal = {International Journal of Advanced Trends in Computer Science and Engineering},
doi = {10.30534/ijatcse/2020/231942020}
}

@article{Deerwester,
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
title = {Indexing by latent semantic analysis},
journal = {Journal of the American Society for Information Science},
volume = {41},
number = {6},
pages = {391-407},
doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
year = {1990}
}

@misc{Likhareva,
      title={Empowering Interdisciplinary Research with BERT-Based Models: An Approach Through SciBERT-CNN with Topic Modeling}, 
      author={Darya Likhareva and Hamsini Sankaran and Sivakumar Thiyagarajan},
      year={2024},
      eprint={2404.13078},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13078}, 
}

@misc{Grootendorst_2022,
      title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure}, 
      author={Maarten Grootendorst},
      year={2022},
      eprint={2203.05794},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.05794}, 
}

@misc{Reimers,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10084}, 
}

@Article{Gana,
AUTHOR = {Gana, Bady and Leiva-Araos, Andrés and Allende-Cid, Héctor and García, José},
TITLE = {Leveraging LLMs for Efficient Topic Reviews},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {17},
ARTICLE-NUMBER = {7675},
URL = {https://www.mdpi.com/2076-3417/14/17/7675},
ISSN = {2076-3417},
ABSTRACT = {This paper presents the topic review (TR), a novel semi-automatic framework designed to enhance the efficiency and accuracy of literature reviews. By leveraging the capabilities of large language models (LLMs), TR addresses the inefficiencies and error-proneness of traditional review methods, especially in rapidly evolving fields. The framework significantly improves literature review processes by integrating advanced text mining and machine learning techniques. Through a case study approach, TR offers a step-by-step methodology that begins with query generation and refinement, followed by semi-automated text mining to identify relevant articles. LLMs are then employed to extract and categorize key themes and concepts, facilitating an in-depth literature analysis. This approach demonstrates the transformative potential of natural language processing in literature reviews. With an average similarity of 69.56% between generated and indexed keywords, TR effectively manages the growing volume of scientific publications, providing researchers with robust strategies for complex text synthesis and advancing knowledge in various domains. An expert analysis highlights a positive Fleiss’ Kappa score, underscoring the significance and interpretability of the results.},
DOI = {10.3390/app14177675}
}

@article{Meng,
title = {Demand-side energy management reimagined: A comprehensive literature analysis leveraging large language models},
journal = {Energy},
volume = {291},
pages = {130303},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.130303},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224000744},
author = {Fanyue Meng and Zhaoyuan Lu and Xiang Li and Wei Han and Jieyang Peng and Xiufeng Liu and Zhibin Niu},
keywords = {Demand side energy management, Bibliometric analysis, Energy efficiency, Topic evolution}
}

@article{Maaten,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@InProceedings{Campello,
author="Campello, Ricardo J. G. B.
and Moulavi, Davoud
and Sander, Joerg",
editor="Pei, Jian
and Tseng, Vincent S.
and Cao, Longbing
and Motoda, Hiroshi
and Xu, Guandong",
title="Density-Based Clustering Based on Hierarchical Density Estimates",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="160--172"
}

@book{Dresch,
  author = {Dresch, Aline and Lacerda, Daniel and Antunes, Junico},
  title = {Design Science Research: Método de Pesquisa para Avanço da Ciência e Tecnologia},
  year = {2015},
  publisher = {Editora FGV},
  isbn = {978-85-8260-298-0},
  doi = {10.13140/2.1.2264.2885}
}

@misc{Chung,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.11416}, 
}

@misc{Kozlowski,
      title={Generative AI for automatic topic labelling}, 
      author={Diego Kozlowski and Carolina Pradier and Pierre Benz},
      year={2024},
      eprint={2408.07003},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.07003}, 
}

@misc{OpenAI,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{KimSolar,
      title={SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling}, 
      author={Dahyun Kim and Chanjun Park and Sanghoon Kim and Wonsung Lee and Wonho Song and Yunsu Kim and Hyeonwoo Kim and Yungi Kim and Hyeonju Lee and Jihoo Kim and Changbae Ahn and Seonghoon Yang and Sukyung Lee and Hyunbyung Park and Gyoungjin Gim and Mikyoung Cha and Hwalsuk Lee and Sunghun Kim},
      year={2024},
      eprint={2312.15166},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.15166}, 
}

@misc{GrootendorstKeyBert,
  author = {Grootendorst, Maarten},
  title = {KeyBERT: Minimal Keyword Extraction with BERT},
  year = {2020},
  url = {https://zenodo.org/records/8388690},
  note = {Accessed on 28 November 2023}
}

@misc{Chen,
  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  year={2023},
  eprint={2309.07597},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{Wijanto, title={Topic Modeling for Scientific Articles: Exploring Optimal Hyperparameter Tuning in BERT}, volume={14}, url={https://ijaseit.insightsociety.org/index.php/ijaseit/article/view/19347}, DOI={10.18517/ijaseit.14.3.19347}, number={3}, journal={International Journal on Advanced Science, Engineering and Information Technology}, author={Wijanto, Maresha Caroline and Widiastuti , Ika and Yong , Hwan-Seung}, year={2024}, month={Jun.}, pages={912–919} }


@misc{Liu,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@inproceedings{MacQueen,
  author = {MacQueen, J.B.},
  title = {Some Methods for Classification and Analysis of Multivariate Observations},
  booktitle = {Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability},
  volume = {1},
  series = {Statistics},
  year = {1967},
  pages = {281--297},
  publisher = {University of California Press},
  address = {Berkeley},
  url = {http://projecteuclid.org/euclid.bsmsp/1200512992}
}

@article{Salton,
title = {Term-weighting approaches in automatic text retrieval},
journal = {Information Processing \& Management},
volume = {24},
number = {5},
pages = {513-523},
year = {1988},
issn = {0306-4573},
doi = {https://doi.org/10.1016/0306-4573(88)90021-0},
url = {https://www.sciencedirect.com/science/article/pii/0306457388900210},
author = {Gerard Salton and Christopher Buckley},
}

@article{Conneau,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{McInnes,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03426}, 
}

@misc{Wang_2023,
      title={WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings}, 
      author={Zijie J. Wang and Fred Hohman and Duen Horng Chau},
      year={2023},
      eprint={2306.09328},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.09328}, 
}

%% Fim das minhas citações