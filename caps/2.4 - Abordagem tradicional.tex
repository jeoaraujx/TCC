\subsection{Abordagens Tradicionais de Modelagem de Tópicos}

Com o crescimento exponencial de dados textuais e a consequente necessidade de organizar informação em larga escala, a modelagem de tópicos consolidou-se como uma técnica fundamental na área de \gls{pln}. Em termos gerais, trata-se de um conjunto de métodos estatísticos cujo objetivo é identificar estruturas semânticas latentes\footnote{O termo "latente" significa que os tópicos não são diretamente observáveis, mas sim inferidos estatisticamente a partir dos padrões de coocorrência de palavras no \textit{corpus}.} — denominadas \textit{tópicos} — em coleções de documentos. Assim, essas técnicas permitem inferir distribuições temáticas que não são explicitamente observáveis, mas que emergem a partir de regularidades no uso do vocabulário.

Entre as abordagens iniciais destacam-se três marcos históricos: a \gls{lsa}, a \gls{plsa} e a \gls{lda}. Esses métodos não apenas moldaram a compreensão inicial sobre a representação semântica de textos, como também estabeleceram fundamentos conceituais e metodológicos que orientaram o desenvolvimento de modelos mais avançados.

A \gls{lsa}, proposta por \citeonline{Deerwester_1990}, parte da decomposição de matrizes termo-documento por meio da técnica de \gls{svd}\footnote{A \gls{svd} é uma técnica de álgebra linear para a decomposição de matrizes que permite encontrar a melhor aproximação de uma matriz por outra de posto inferior, sendo fundamental para a redução de dimensionalidade em espaços vetoriais de termos.}. Nesse enquadramento, documentos e termos são projetados em um espaço vetorial de dimensionalidade reduzida, o que permite atenuar ruídos lexicais e capturar relações de similaridade latentes. Apesar de sua relevância histórica, a linearidade da \gls{lsa} e sua insensibilidade a variações contextuais limitam seu desempenho em cenários onde relações semânticas complexas são determinantes \cite{George_2023, Xie_2020}.

Com o intuito de superar parte dessas limitações, \citeonline{Hofmann_1999,Hofmann_2001} introduziram a \gls{plsa}, que reformulou a representação semântica a partir de um modelo probabilístico. Nessa abordagem, cada ocorrência de palavra em um documento é modelada como proveniente de um tópico latente, de forma que a probabilidade conjunta de palavra $w$ e documento $d$ é expressa como:
\[
P(w, d) = \sum_{z \in Z} P(z|d) \, P(w|z),
\]
onde $z$ representa o conjunto de tópicos latentes. Embora tenha representado um avanço em relação à \gls{lsa}, a \gls{plsa} apresenta limitações notáveis, em especial no que se refere à escalabilidade: o número de parâmetros cresce linearmente com a quantidade de documentos, o que compromete sua generalização e a torna suscetível a \textit{overfitting} \cite{Datchanamoorthy_2023}.

A evolução natural desse paradigma ocorreu com a formulação da \gls{lda}, proposta por \citeonline{Blei_2003}. Ao contrário da \gls{plsa}, a \gls{lda} incorpora uma camada Bayesiana por meio da utilização de distribuições de \textit{Dirichlet} como \textit{priors}\footnote{A \gls{lda} é um modelo generativo Bayesiano. O uso das distribuições de \textit{Dirichlet} (uma distribuição de probabilidade sobre outras distribuições) permite ao modelo tratar as misturas de tópicos nos documentos e as misturas de palavras nos tópicos como variáveis aleatórias, conferindo maior robustez e melhor generalização.}. Essa estrutura permite regularizar o modelo e definir uma distribuição de tópicos não apenas a nível de documento, mas também a nível de \textit{corpus}, resultando em maior robustez e interpretabilidade. A \gls{lda} parte da premissa de que cada documento é representado como uma mistura de tópicos, e cada tópico, por sua vez, é caracterizado por uma distribuição de palavras. Essa formulação tornou o modelo amplamente aplicável em diferentes domínios, como saúde pública \cite{Mifrah_2020} e eficiência energética \cite{Polyzos_2022}.

Apesar de sua influência, tanto a \gls{lsa} quanto a \gls{plsa} e a \gls{lda} compartilham limitações estruturais. Todas operam no paradigma de \textit{bag-of-words}\footnote{O \textit{Bag-of-Words} (Saco de Palavras) é um modelo de representação de texto que ignora a ordem e a estrutura gramatical das palavras, tratando um documento apenas como um conjunto (ou multiconjunto) de suas palavras e suas frequências.} (saco de palavras), que ignora a ordem e o contexto local das palavras. Segundo \citeonline{George_2023, Xie_2020}, isso frequentemente conduz a representações semânticas superficiais em textos técnicos ou multilíngues. \citeonline{Datchanamoorthy_2023} também reitera que a sensibilidade da \gls{lda} à definição do número de tópicos ($K$) representa um desafio adicional: valores reduzidos podem fundir tópicos distintos em um único, enquanto valores elevados podem fragmentar temas coesos em subtemas artificiais.

\begin{citacao}
A sensibilidade do \gls{lda} ao parâmetro do número de temas ($K$) é uma de suas desvantagens. Encontrar o valor ideal para ($K$) pode ser desafiador. O modelo pode simplificar excessivamente e combinar diferentes temas em um só se ($K$) for configurado muito baixo. No entanto, se ($K$) for configurado muito alto, o modelo pode se tornar muito complexo e produzir temas errôneos \cite[Tradução nossa]{Datchanamoorthy_2023}.
\end{citacao}

Essas restrições evidenciam que, embora fundamentais, tais técnicas falham em capturar o significado contextual profundo e a ordem das palavras. Essa limitação estrutural torna-os insuficientes para tarefas que exigem uma compreensão semântica robusta, especialmente em bases textuais heterogêneos ou multilíngues onde a ambiguidade lexical é alta, destacando a necessidade de abordagens que superem o paradigma \textit{bag-of-words}.