\chapter[PROJETO DE DESENVOLVIMENTO]{PROJETO DE DESENVOLVIMENTO}

O processo de desenvolvimento do artefato proposto segue uma abordagem iterativa e integrada, utilizando técnicas avançadas de processamento de linguagem natural (PLN) e aprendizado de máquina para melhorar a análise e classificação de publicações científicas na plataforma SIMCC. Essa metodologia busca otimizar a identificação e exploração de temas emergentes em grandes volumes de dados, superando as limitações das abordagens tradicionais de busca por palavras-chave. A integração dessas tecnologias permite uma análise mais rica, contextual e eficiente, adequando-se melhor às necessidades de plataformas acadêmicas como o SIMCC, que lidam com dados interdisciplinares e multilíngues.

A seguir, apresentamos as sub-seções detalhadas que descrevem as etapas-chave do desenvolvimento do artefato. Cada uma delas aborda um aspecto crucial do pipeline proposto, como o pré-processamento dos dados, a geração de embeddings, a redução de dimensionalidade, o agrupamento de tópicos, a rotulagem semântica e a visualização interativa. Essas etapas são projetadas para oferecer uma solução robusta, capaz de lidar com a complexidade e volume de dados típicos em plataformas acadêmicas, proporcionando resultados mais precisos e insights mais profundos sobre os dados.

\section{Tecnologias Utilizadas}
A seção Tecnologias Utilizadas descreve as ferramentas essenciais para o desenvolvimento do artefato, com foco na análise semântica e modelagem de tópicos das publicações científicas do SIMCC. A plataforma SIMCC, que centraliza dados científicos extraídos dos currículos Lattes dos pesquisadores, é complementada pelo uso do PostgreSQL, escolhido por sua robustez, flexibilidade e suporte a tipos de dados avançados, permitindo o armazenamento eficiente de embeddings de alta dimensionalidade. O Python, utilizado para o desenvolvimento do back-end, oferece uma ampla gama de bibliotecas para processamento de linguagem natural e aprendizado de máquina, facilitando a integração com o PostgreSQL e a implementação do pipeline do projeto. Essa combinação de tecnologias garante uma solução escalável, eficiente e capaz de realizar análises semânticas precisas, melhorando a identificação e visualização de tópicos emergentes.

\subsection{Base de dados}
A base de dados utilizada neste estudo provém da plataforma \textit{Sistema de Mapeamento de Competências Científicas da Bahia (SIMCC)}, uma iniciativa da Secretaria Estadual de Ciência, Tecnologia e Inovação da Bahia. O \textit{SIMCC} é uma plataforma que congrega dados sobre as produções científicas e técnicas de profissionais lotados nas Instituições de Ensino e Pesquisa da Bahia. Esse sistema foi desenvolvido para centralizar e organizar informações relacionadas a pesquisas acadêmicas, programas de pós-graduação e outras iniciativas científicas.

As informações alimentadoras da plataforma são extraídas principalmente dos currículos \textit{Lattes} dos pesquisadores, o que garante que os dados estejam sempre atualizados e em sintonia com as produções mais recentes. A plataforma também oferece módulos que permitem o acesso a dados de pesquisa e programas de pós-graduação, facilitando a análise e o planejamento estratégico das atividades de pesquisa, inovação e criação de novos projetos dentro da comunidade acadêmica.

A plataforma \textit{SIMCC} é extremamente intuitiva, permitindo fácil navegação e acesso aos dados. Além disso, ela oferece funcionalidades adicionais, como a organização de baremas de avaliação para processos seletivos e a gestão de uma taxonomia de dados, o que possibilita uma análise mais estruturada e detalhada das produções científicas. Através dessa base de dados, o \textit{SIMCC} fornece uma visão abrangente da ciência e tecnologia produzidas na Bahia, com foco no potencial de inovação e colaboração entre as instituições de ensino e pesquisa.

No contexto deste trabalho, a base de dados do \textit{SIMCC} serve como a fonte principal de publicações científicas a serem analisadas, permitindo a aplicação das técnicas avançadas de modelagem de tópicos e de análise semântica para a identificação de temas emergentes e tendências científicas. A utilização dessa base possibilita que o sistema seja adaptado a um conjunto real de dados, promovendo uma avaliação eficaz e uma experiência de análise mais precisa.

\subsection{Banco de dados}
A escolha do PostgreSQL para o armazenamento e gerenciamento da base de dados do SIMCC é fundamentada em sua robustez, flexibilidade e suporte a operações complexas, essenciais para o desenvolvimento do artefato proposto. O PostgreSQL se destaca por sua capacidade de lidar com grandes volumes de dados e por ser altamente extensível, o que o torna ideal para integrar técnicas avançadas de processamento de linguagem natural (PLN) e o uso de embeddings.

O PostgreSQL oferece suporte nativo a tipos de dados avançados, como arrays e JSON, que são fundamentais para armazenar embeddings de alta dimensionalidade gerados por modelos como S-BERT ou BERT. Essas representações vetoriais podem ser manipuladas de forma eficiente dentro do banco de dados, sem a necessidade de conversões complexas. Além disso, a extensibilidade do PostgreSQL permite a integração com ferramentas de PLN, como spaCy e Hugging Face, e a utilização de extensões como o pgvector, que facilita a consulta e a comparação de vetores de embeddings diretamente no banco de dados. Isso é particularmente importante para operações de similaridade semântica e agrupamento de tópicos, essenciais para a modelagem de temas no SIMCC.

O PostgreSQL também se destaca pela sua alta performance e escalabilidade, permitindo consultas rápidas mesmo em grandes volumes de dados textuais. Sua capacidade de executar consultas complexas e sua eficiência em indexação, por meio de Índices GiST e GIN, tornam o banco de dados ideal para manipular e realizar operações em dados de embeddings, garantindo uma solução eficiente para o processamento de publicações científicas. Essa combinação de características torna o PostgreSQL a escolha ideal para integrar técnicas de PLN e aprendizado de máquina no SIMCC, permitindo uma análise semântica e uma modelagem de tópicos mais precisa e escalável.

\subsection{Pipeline e camada de Back-end}
Para o desenvolvimento da camada de back-end do projeto, será utilizado Python, uma das linguagens de programação mais populares e poderosas no campo de processamento de linguagem natural (PLN) e aprendizado de máquina. Python foi escolhido devido à sua flexibilidade, extensibilidade e vasta comunidade de desenvolvedores, o que facilita a implementação de soluções complexas e a integração com diversas bibliotecas especializadas. O Python possui uma grande variedade de bibliotecas que são amplamente utilizadas para tarefas de PLN, modelagem de tópicos e aprendizado de máquina, tornando-o a linguagem ideal para o desenvolvimento do pipeline deste projeto.

Além disso, o Python também permitirá a integração com o PostgreSQL, utilizando bibliotecas como psycopg2 ou SQLAlchemy para consultar e armazenar dados, tanto os embeddings gerados quanto os tópicos e rótulos semânticos resultantes da análise. O uso do Python para o desenvolvimento do back-end proporciona um ambiente flexível e eficiente para implementar todo o fluxo do projeto, desde o pré-processamento dos dados até a visualização interativa, integrando as técnicas de modelagem de tópicos com as potentes ferramentas de PLN e aprendizado de máquina. A escolha dessa linguagem e suas bibliotecas permite um desenvolvimento ágil e a construção de soluções escaláveis e de fácil manutenção.

\section{Arquitetura da solução}
A arquitetura da solução proposta para a análise de textos acadêmicos no contexto do \textit{SIMCC} envolve um pipeline completo que abrange desde o pré-processamento dos dados até a visualização interativa dos tópicos. O processo inicia com a coleta de dados de um banco de dados \textit{PostgreSQL}, seguida de etapas de pré-processamento, como \textit{tokenização}, remoção de \textit{stopwords}, \textit{lematização} e \textit{normalização}, utilizando ferramentas como \textit{spaCy} e \textit{NLTK}. A extração de características é realizada por meio do \textit{TfidfVectorizer}, que transforma os textos em representações numéricas, destacando palavras relevantes e reduzindo ruídos. A geração de \textit{embeddings} é realizada utilizando o \textit{GPT-4} e o modelo \textit{paraphrase-multilingual-MiniLM-L12-v2}, que oferecem \textit{embeddings} densos e multilingues, capturando relações semânticas complexas. A redução de dimensionalidade é realizada com o \textit{UMAP}, que preserva estruturas locais e globais, enquanto o \textit{HDBSCAN} é utilizado para agrupar os tópicos, identificando \textit{clusters} de textos semelhantes. A rotulagem dos tópicos é aprimorada pelo \textit{GPT-4}, que gera rótulos semanticamente ricos e contextualizados. Por fim, a visualização interativa é realizada com o \textit{WizMap}, que permite a exploração de tópicos em diferentes níveis de granularidade, utilizando tecnologias como \textit{WebGL} e \textit{Web Workers} para escalabilidade.

\subsection{Pré-processamento dos Dados}
Após a coleta e importação dos dados de um banco de dados PostgreSQL\footnote{Encontrado em: \url{https://www.postgresql.org/}}, o pré-processamento dos dados é a primeira etapa essencial para garantir a qualidade e a consistência dos dados textuais antes de qualquer análise ou modelagem. Nessa fase, aplicamos várias técnicas de limpeza e preparação, utilizando ferramentas especializadas, para preparar os dados textuais para a transformação em embeddings:

\begin{itemize}
    \item \textbf{Tokenização:} A tokenização é o processo de dividir o texto em unidades menores, chamadas de tokens, como palavras ou frases. Ferramentas como spaCy e NLTK são utilizadas para realizar a tokenização. Para a tokenização mais eficiente e focada em idiomas específicos, utilizamos o tokenizador do spaCy, que é otimizado para diversos idiomas e oferece alta precisão.
    \item \textbf{Remoção de Stopwords:} As stopwords são palavras que não agregam valor semântico significativo ao contexto do texto, como preposições, artigos e conjunções. A remoção dessas palavras é feita por meio das funcionalidades do spaCy e NLTK, que já contêm listas pré-definidas de stopwords, embora seja possível ajustar conforme o contexto.
    \item \textbf{Stemming e Lematização:} O stemming é o processo de reduzir as palavras à sua raiz, enquanto a lematização busca o lema (forma base) das palavras, considerando seu contexto gramatical. Para essa tarefa, o spaCy e o NLTK são altamente eficientes. O spaCy oferece lematização robusta, enquanto o NLTK pode ser usado para stemming em conjunto com seu dicionário de raízes. A lematização é preferida em muitos casos, pois mantém o contexto semântico.
    \item \textbf{Normalização de Texto:} A normalização envolve várias técnicas, incluindo a conversão de todo o texto para minúsculas, remoção de caracteres especiais e normalização de acentuação. Regex pode ser utilizada em conjunto com spaCy para limpar textos.
\end{itemize}

\subsection{Extração de características (feature extraction)}
Após o pré-processamento dos dados, que inclui etapas de tokenização, remoção de stopwords, lematização e normalização do texto, a próxima etapa essencial para a análise dos dados textuais é a vetorização. A vetorização transforma o texto em uma representação numérica, permitindo que os algoritmos de aprendizado de máquina e modelos de PLN, como o BERTopic e GPT-4, possam processar e analisar os dados de forma eficiente. No contexto deste projeto, utilizamos o TfidfVectorizer, uma das abordagens mais populares para vetorização de textos.

O Term Frequency-Inverse Document Frequency (TfidfVectorizer)\cite{Salton} é utilizado para transformar os textos limpos em uma matriz esparsa de características. Ele mede a importância relativa de uma palavra em um documento, levando em conta tanto sua frequência dentro do documento quanto a frequência inversa de ocorrência nos documentos do corpus. Isso ajuda a destacar palavras significativas para um determinado tópico e reduzir a influência de termos muito comuns, como "e", "de", "para", entre outros.

O \textit{TfidfVectorizer}\footnote{Encontrado em: \url{https://scikit-learn.org/1.5/api/sklearn.feature_extraction.html}} foi escolhido por sua capacidade de capturar a importância contextual das palavras, o que é essencial para identificar os tópicos e padrões relevantes nas publicações científicas. Ele oferece várias vantagens:

\begin{itemize}
    \item \textbf{Equilíbrio entre Frequência Local e Global:} O modelo ajuda a balancear a frequência local de uma palavra (dentro de um único documento) com sua frequência global (em todo o corpus). Isso torna o modelo eficiente para identificar termos relevantes, que podem ser úteis para a análise de tópicos, como palavras que são específicas para um contexto, mas não são comuns em todos os textos.

    \item \textbf{Redução de Ruído:} A configuração de \textbf{ngram\_range=(2,3)} no \textit{TfidfVectorizer} ajuda a capturar bigramas e trigramas (sequências de 2 ou 3 palavras), em vez de palavras isoladas, o que é fundamental para entender melhor as relações semânticas mais complexas dentro dos textos. Isso é útil em textos acadêmicos, onde contextos e significados completos muitas vezes dependem de frases ou combinações de palavras.

    \item \textbf{Ajuste no Peso de Palavras:} O \textbf{use\_idf=True} e \textbf{smooth\_idf=True} garantem que o modelo leve em conta a relevância das palavras nos documentos e suavize as palavras com baixa frequência que possam ser consideradas ruidosas. O \textbf{sublinear\_tf=True} aplica uma transformação logarítmica à contagem de palavras, o que ajuda a lidar com a discrepância de palavras que ocorrem com alta frequência.
\end{itemize}

Em resumo, o TfidfVectorizer oferece um bom equilíbrio entre eficiência computacional e precisão na captura do conteúdo semântico, tornando-o uma excelente escolha para a fase de vetorização neste projeto. Embora haja alternativas poderosas como o Word2Vec ou CountVectorizer\footnote{Encontrado em: \url{https://scikit-learn.org/1.5/api/sklearn.feature_extraction.html}}, o TfidfVectorizer é particularmente adequado para a análise de textos acadêmicos em grandes volumes, proporcionando uma solução escalável e eficaz para o SIMCC.

\subsection{Geração de Embeddings}
A geração de embeddings é uma etapa essencial no pipeline de análise de textos, sendo responsável por transformar dados textuais em representações numéricas densas que capturam relações contextuais e semânticas. No contexto deste projeto, serão utilizadas duas abordagens independentes: a API do \textbf{OpenAI GPT-4} e o modelo \textbf{paraphrase-multilingual-MiniLM-L12-v2}. Ambas as técnicas oferecem vantagens específicas e serão exploradas para diferentes propósitos dentro do pipeline.

A API do GPT-4 da OpenAI representa uma das mais avançadas ferramentas para geração de embeddings atualmente disponíveis. O GPT-4 é um modelo de linguagem de grande escala (LLM) baseado em uma arquitetura de transformers, projetado para compreender e gerar texto com extrema precisão. Quando utilizado para embeddings, ele transforma texto em vetores densos que representam relações semânticas em um espaço vetorial.

Como funciona o processo com o GPT-4:
\begin{itemize}
    \item \textbf{Pré-processamento do texto:} O texto é tokenizado para dividir o conteúdo em subunidades, como palavras ou subpalavras, utilizando um tokenizador específico da OpenAI.
    \item \textbf{Camadas de atenção no GPT-4:} O texto tokenizado é processado por múltiplas camadas de atenção, onde o modelo aprende as relações entre as palavras no contexto global do texto.
    \item \textbf{Geração do vetor:} Em vez de gerar texto (como normalmente o GPT-4 faz), um vetor numérico fixo é extraído da saída intermediária do modelo, capturando a essência semântica do texto fornecido.
\end{itemize}

A utilização do GPT-4 destaca-se como uma poderosa ferramenta para a geração de embeddings devido à sua precisão semântica superior, versatilidade e capacidade multilingue. Em termos de precisão, o modelo é altamente eficaz na captura de relações semânticas complexas e nuances contextuais. Isso é particularmente relevante para textos acadêmicos, onde os significados muitas vezes dependem de interpretações detalhadas e de conexões contextuais sutis entre termos e conceitos. A habilidade do GPT-4 de processar informações em contextos bidirecionais e de capturar dependências globais no texto contribui significativamente para a criação de embeddings de alta qualidade que refletem fielmente o significado subjacente dos textos analisados.

Outro ponto de destaque do GPT-4 é sua capacidade multilingue. O modelo é treinado em uma vasta gama de idiomas, incluindo português, inglês e espanhol, permitindo alinhar semanticamente conceitos expressos em diferentes línguas. Essa habilidade torna-o ideal para corpora acadêmicos multilíngues, como os encontrados na plataforma SIMCC, onde a integração de publicações em diferentes idiomas é essencial para identificar tendências e tópicos emergentes de forma abrangente.

Apesar de suas vantagens, o uso do GPT-4 apresenta desafios. Um dos principais é o custo elevado, uma vez que o modelo é acessado via API e os custos por requisição podem se acumular significativamente ao processar grandes volumes de dados textuais. Além disso, o tempo de inferência do GPT-4 é maior em comparação a modelos mais leves, como o \textbf{paraphrase-multilingual-MiniLM-L12-v2}, o que pode ser um problema em aplicações que exigem alta escalabilidade e respostas em tempo real. Outro desafio é a dependência de infraestrutura, já que o modelo requer conectividade constante com a API da OpenAI, o que pode limitar seu uso em ambientes com restrições de rede ou orçamentos limitados.

Por outro lado, o modelo paraphrase-multilingual-MiniLM-L12-v2\footnote{Encontrado em: \url{https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}}\cite{Reimers}, desenvolvido pela equipe Sentence-Transformers, é uma alternativa otimizada para geração de embeddings. Baseado na arquitetura \textbf{MiniLM}, ele é projetado para ser mais leve e rápido que modelos como BERT, mantendo alta precisão em tarefas de similaridade semântica e análise contextual. Este modelo foi pré-treinado em dados multilingues, o que o torna ideal para corpora que incluem português e inglês.

Como funciona o processo com \textbf{paraphrase-multilingual-MiniLM-L12-v2}:
\begin{itemize}
    \item \textbf{Tokenização:} O texto é dividido em subpalavras utilizando o tokenizador WordPiece, projetado para lidar com línguas complexas e variadas.
    \item \textbf{Arquitetura MiniLM:} O texto tokenizado é processado por 12 camadas de atenção, otimizadas para gerar embeddings densos que capturam relações semânticas locais e globais.
    \item \textbf{Geração de embeddings contextuais:} O modelo gera vetores densos para sentenças completas, que representam o texto no espaço semântico, preservando nuances contextuais.
\end{itemize}

Nesse contexto, o \textbf{paraphrase-multilingual-MiniLM-L12-v2} é uma solução altamente eficiente para geração de embeddings, destacando-se por sua leveza e velocidade. Por ser otimizado para tarefas de processamento de linguagem natural, o modelo é significativamente mais rápido do que alternativas maiores, como o GPT-4 ou o XLM-RoBERTa\footnote{Encontrado em: \url{https://huggingface.co/FacebookAI/xlm-roberta-large}} \cite{Conneau}, o que o torna ideal para aplicações que exigem alta eficiência computacional. Essa rapidez é particularmente vantajosa em cenários de grande escala, como o SIMCC, onde é necessário lidar com grandes volumes de dados de maneira ágil e consistente. 

Outra vantagem importante do modelo é seu suporte multilingue, permitindo lidar eficazmente com textos em português, inglês e outras línguas. Essa característica é essencial em projetos como o SIMCC, que envolvem publicações científicas produzidas em diferentes idiomas. Além disso, por ser possível executar o modelo localmente, ele oferece custo reduzido, eliminando despesas associadas a APIs externas e tornando-o uma solução econômica e escalável para análises em larga escala. A combinação de velocidade e baixo custo o torna uma excelente opção para tarefas que demandam processamento em tempo real, como a geração de tópicos ou a análise semântica.

No entanto, o \textbf{paraphrase-multilingual-MiniLM-L12-v2} apresenta algumas limitações. Sua precisão, embora suficiente para a maioria das tarefas gerais, pode ser inferior à de modelos maiores como o GPT-4 em contextos que envolvem textos altamente técnicos ou interdisciplinares, onde nuances mais complexas precisam ser capturadas. Isso pode limitar seu desempenho em análises que exigem extrema precisão semântica.

\subsection{Redução de Dimensionalidade}
A redução de dimensionalidade é uma etapa fundamental em pipelines que lidam com dados de alta dimensionalidade, como embeddings gerados para textos. Ela transforma os dados em representações mais compactas, preservando informações essenciais e eliminando redundâncias ou ruídos. No projeto, utilizamos o \textit{Uniform Manifold Approximation and Projection (UMAP)}\footnote{Econtrado em: \url{https://umap-learn.readthedocs.io/en/latest/basic_usage.html}} \cite{McInnes} devido à sua capacidade de preservar tanto as relações locais quanto globais entre os dados, o que é crucial para a análise semântica e a modelagem de tópicos.

O UMAP é um algoritmo de redução de dimensionalidade baseado em princípios matemáticos da teoria de grafos e geometria algébrica. Ele funciona em várias etapas:
    \begin{itemize}
        \item \textbf{Construção do Grafo de Alta Dimensionalidade:} O UMAP começa representando os dados originais como um grafo ponderado em alta dimensionalidade. Cada ponto de dado é conectado aos seus vizinhos mais próximos com pesos que indicam a proximidade entre os pontos. Essa etapa é realizada usando algoritmos como \textit{k-nearest neighbors (k-NN)}, que identificam os pontos mais próximos no espaço original.
        \item \textbf{Definição de uma Distribuição de Probabilidade Local:} Uma distribuição de probabilidade local é ajustada ao redor de cada ponto, calculando a probabilidade de uma conexão entre pontos baseando-se na distância.
        \item \textbf{Projeção para Baixa Dimensionalidade:} O algoritmo então otimiza um grafo em um espaço de menor dimensionalidade. Ele tenta preservar a estrutura do grafo original, minimizando a diferença entre os dois grafos usando uma função de perda. Esse processo garante que as relações locais (proximidade entre pontos próximos) e globais (estrutura dos clusters) sejam mantidas.
        \item \textbf{Representação Final:} Os dados são projetados no espaço de menor dimensionalidade (normalmente 2D ou 3D), criando uma representação compacta que pode ser utilizada para agrupamentos (como no HDBSCAN) ou visualizações interativas.    
    \end{itemize}

O \textit{UMAP} utiliza várias técnicas essenciais para a redução de dimensionalidade. A primeira delas é o \textit{\textbf{k-Nearest Neighbors (k-NN)}}, que identifica os pontos mais próximos no espaço de alta dimensionalidade para criar um grafo local. A escolha do número de vizinhos (k) é um parâmetro crítico, pois influencia o equilíbrio entre a preservação das relações locais e globais. Em seguida, o algoritmo utiliza um otimizador de gradiente para ajustar o grafo em baixa dimensionalidade, minimizando as diferenças estruturais entre os grafos original e projetado.

Além disso, o \textit{UMAP} aplica geometria algébrica, assumindo que os dados em alta dimensionalidade estão distribuídos em um espaço chamado \textbf{manifold}. Ele projeta esses dados em uma variedade de baixa dimensionalidade, preservando tanto as relações locais quanto as estruturas globais dos dados. Essa combinação de técnicas permite ao \textit{UMAP} gerar representações compactas e de alta qualidade, mantendo a integridade semântica dos dados.

Portanto, ele é ideal para o pipeline proposto no \textit{SIMCC} devido à sua capacidade de lidar eficientemente com embeddings densos e multilingues gerados pelos modelos de linguagem. Sua habilidade em preservar estruturas semânticas tanto locais quanto globais permite que clusters de tópicos sejam mais coerentes e representativos. Além disso, a combinação de escalabilidade, flexibilidade e qualidade de projeção o torna uma escolha natural para visualizações interativas, como no caso do \textit{WizMap}, onde os tópicos serão exibidos como clusters organizados.

Dessa forma, ele integra perfeitamente ao restante do pipeline, fornecendo representações compactas e confiáveis que servem como base para o agrupamento de tópicos com o \textit{HDBSCAN} e a análise semântica enriquecida pelo \textit{GPT-4}. Essa combinação garante que a redução de dimensionalidade não seja apenas uma etapa intermediária, mas um componente estratégico para melhorar a eficácia e eficiência da modelagem de tópicos.

\subsection{Agrupamento de Tópicos (Clustering)}
O agrupamento de tópicos também é uma etapa essencial no pipeline, onde textos com características semelhantes são agrupados para formar clusters temáticos. Esses clusters são agrupamentos naturais de dados que compartilham características semelhantes. No contexto de modelagem de tópicos, cada cluster representa um grupo de textos que abordam temas relacionados, com base em suas representações numéricas (embeddings). No contexto do projeto, utilizamos o \textbf{\textit{Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)}} devido à sua capacidade de lidar com dados complexos, densidades variáveis e ruídos, oferecendo agrupamentos precisos e adaptáveis.

O HDBSCAN é uma extensão hierárquica do algoritmo \textbf{\textit{Density-Based Spatial Clustering of Applications with Noise (DBSCAN)}}, projetado para detectar clusters em dados de densidades variáveis sem exigir que o número de clusters seja definido previamente. O funcionamento do HDBSCAN envolve várias etapas:

\begin{itemize}
    \item \textbf{Construção do Grafo de Distâncias:} O HDBSCAN inicia criando um grafo onde os pontos de dados são conectados com base na distância entre eles, calculada a partir dos embeddings. Para isso, utiliza a distância métrica escolhida (como euclidiana ou coseno).

    \item \textbf{Árvore de Alcance Mínimo \textit{(Minimum Spanning Tree)}:} A partir do grafo, o algoritmo constrói uma árvore de alcance mínimo, que conecta todos os pontos com a menor soma de pesos (distâncias). Isso cria uma representação hierárquica das relações entre os pontos.
    
    \item \textbf{Densidade e Ruído:} A densidade de cada cluster é avaliada com base no número de pontos próximos e suas distâncias. Pontos que não atingem a densidade mínima são considerados ruídos e não são atribuídos a nenhum cluster.

    \item \textbf{\textit{Cluster} Condensado:} A árvore é podada iterativamente para remover arestas que representam ruídos, revelando os clusters restantes. Esses agrupamentos finais são obtidos com base em densidades locais, permitindo identificar clusters de diferentes formas e tamanhos.

    \item \textbf{\textit{Soft Clustering} (Agrupamento Suave):} O HDBSCAN fornece uma saída de probabilidade que indica a força de associação de cada ponto ao cluster, em vez de uma associação rígida. Isso é útil para lidar com pontos que podem estar na fronteira entre clusters.
\end{itemize}

O HDBSCAN se destaca por sua capacidade de lidar com densidades variadas e formatos complexos de clusters, ao contrário de técnicas como o \textit{k-means}, que assumem densidade uniforme e formas esféricas. Essa adaptabilidade é crucial para dados heterogêneos, como textos acadêmicos. Outra vantagem significativa é sua robustez ao ruído, já que o HDBSCAN identifica pontos que não se encaixam em nenhum cluster e os trata como ruídos, em vez de forçá-los a se agrupar, o que melhora a qualidade geral dos tópicos formados. Além disso, o algoritmo não requer que o número de clusters seja pré-definido, determinando-o automaticamente com base nos dados, eliminando a necessidade de suposições iniciais que podem ser imprecisas. O \textit{soft clustering} é outra característica marcante, pois fornece uma probabilidade de associação para cada ponto, permitindo interpretações mais flexíveis e precisas, especialmente em cenários onde os limites entre \textit{clusters} não são claros. Por fim, o HDBSCAN é altamente escalável, capaz de lidar com grandes volumes de dados textuais, como os do SIMCC, de forma eficiente e robusta.

Comparado a outras técnicas, o HDBSCAN supera o \textit{k-means} \cite{MacQueen}, que exige o número de clusters como entrada e é limitado a formas esféricas, e o DBSCAN, que não é eficaz em densidades variadas ou em dados de alta dimensionalidade. Além disso, é mais flexível e escalável do que o \textit{Agglomerative Clustering}, que é adequado para pequenos conjuntos de dados, mas menos eficiente em grandes volumes. Essas características tornam o HDBSCAN uma escolha superior para a modelagem de tópicos em corpora complexos e extensos, como os encontrados na plataforma SIMCC.

Estudos recentes, como os de \cite{Gana} e \cite{Jung}, destacam o sucesso do HDBSCAN em tarefas de modelagem de tópicos, especialmente quando combinado com UMAP para reduzir a dimensionalidade antes do agrupamento. Essa combinação oferece clusters mais coesos e representativos.

\subsection{Rotulagem de Tópicos}

No processo de modelagem temática, a etapa de rotulagem desempenha um papel essencial, garantindo que os clusters sejam identificados com rótulos representativos e facilmente interpretáveis. Para este projeto, o GPT-4 foi escolhido como ferramenta principal para enriquecer semanticamente os rótulos, aproveitando sua habilidade avançada de gerar textos contextualmente precisos e alinhados com o significado subjacente dos tópicos.

Uma vez que os clusters de tópicos são formados pelo BERTopic, suas palavras-chave representativas (identificadas por métodos como o c-TF-IDF) são enviadas para a API da OpenAI. O GPT-4 interpreta essas palavras-chave e o contexto geral do cluster, gerando rótulos mais descritivos, claros e semanticamente ricos. Esse processo transcende a rotulagem padrão automatizada do BERTopic, que utiliza apenas as palavras mais frequentes ou relevantes de cada cluster para gerar nomes, frequentemente resultando em rótulos genéricos ou pouco informativos.

O GPT-4 oferece vantagens significativas para a rotulagem de tópicos, destacando-se pela sua capacidade de contextualização e coerência. Graças ao treinamento em um extenso corpus textual, o GPT-4 consegue interpretar não apenas as palavras-chave de um cluster, mas também as relações contextuais entre elas. Isso permite a criação de rótulos mais detalhados e representativos, capturando o significado subjacente dos tópicos de maneira precisa. Essa capacidade é especialmente útil para lidar com temas complexos, onde as conexões contextuais são fundamentais para a interpretação.

Outro ponto forte do GPT-4 é sua habilidade de enriquecer semanticamente os rótulos, indo além das limitações de abordagens estatísticas tradicionais, como o c-TF-IDF utilizado no BERTopic. Em clusters interdisciplinares, o GPT-4 é capaz de identificar conexões que podem não ser evidentes apenas pelas palavras-chave, oferecendo rótulos mais ricos e informativos. Além disso, sua flexibilidade multilingue é crucial para projetos como o SIMCC, permitindo a geração de rótulos consistentes e semânticos em idiomas como português, inglês e espanhol. Essa capacidade de traduzir conceitos entre idiomas de forma precisa reforça sua aplicação em contextos multilíngues. Por fim, o GPT-4 possibilita rotulagem personalizada, permitindo ajustes nas instruções para gerar rótulos detalhados ou simplificados, dependendo da necessidade do projeto ou do público-alvo, tornando o processo ainda mais versátil e adaptado aos objetivos específicos da análise.

A técnica padrão de rotulagem do BERTopic utiliza algoritmos como c-TF-IDF para identificar as palavras mais relevantes de cada cluster. Embora eficiente, essa abordagem é limitada à seleção automática de palavras-chave, o que muitas vezes resulta em rótulos genéricos ou pouco descritivos, especialmente em tópicos complexos ou com sobreposição temática. O BERTopic não considera o contexto semântico amplo ou as relações entre palavras no cluster, tornando os rótulos menos precisos em temas interdisciplinares.

Por outro lado, o GPT-4 supera essas limitações ao interpretar os clusters de maneira contextual e ao enriquecer os rótulos com descrições detalhadas. Por exemplo, se um cluster contém palavras como "transformers", "BERT" e "PLN", o BERTopic poderia gerar um rótulo genérico como "Modelos de Transformadores". Já o GPT-4 poderia contextualizar essas palavras para criar um rótulo mais informativo, como "Aplicações de Modelos de Transformadores em Processamento de Linguagem Natural".

A integração do GPT-4 no pipeline para rotulagem de tópicos representa um avanço significativo em termos de qualidade e usabilidade dos rótulos gerados. Comparado à técnica padrão do BERTopic, ele oferece uma compreensão mais profunda e contextual dos clusters, resultando em rótulos mais representativos e úteis para análise. Essa abordagem é especialmente vantajosa para a plataforma SIMCC, onde a clareza e a precisão semântica dos tópicos são cruciais para facilitar a exploração e a compreensão das produções científicas indexadas.

\subsection{Visualização Interativa (WizMap)}
O \textit{WizMap} é uma ferramenta avançada de visualização interativa projetada para explorar e interpretar grandes volumes de \textit{embeddings} em espaços de alta dimensionalidade, como descrito por \cite{Wang}. Essa solução é particularmente eficaz para lidar com os desafios apresentados pelo volume e pela complexidade de \textit{embeddings} gerados por modelos de aprendizado de máquina, permitindo que pesquisadores compreendam a estrutura global e local desses dados. O \textit{WizMap} adota uma abordagem de visualização inspirada em mapas, integrando técnicas como o uso de contornos de densidade e resumos adaptativos de embeddings em múltiplas resoluções.

No contexto do SIMCC, a aplicação do \textit{WizMap} oferece uma oportunidade única para tornar a exploração de tópicos emergentes mais intuitiva e acessível. Após a modelagem de tópicos e a geração de \textit{embeddings} com o BERTopic, combinada com técnicas de redução de dimensionalidade, como o UMAP, o \textit{WizMap}\footnote{Encontrado em: \url{https://poloclub.github.io/wizmap/}} organiza os tópicos em uma interface visual. Essa interface utiliza \textit{quadtree} para segmentar os dados e sumarizar os tópicos em diferentes níveis de granularidade, permitindo que os usuários explorem \textit{clusters} de tópicos em detalhes ou obtenham uma visão geral, ajustando a profundidade da análise conforme necessário.

Comparado a métodos tradicionais, como \textit{scatter plots} estáticos ou projeções simples, o \textit{WizMap} se destaca pela capacidade de manipular milhões de pontos em navegadores sem a necessidade de infraestrutura pesada, graças ao uso de tecnologias como \textit{WebGL}\footnote{Econtrado em: \url{https://get.webgl.org/}} e \textit{Web Workers}\footnote{Encontrado em: \url{https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers}}. Isso torna a ferramenta altamente escalável e adequada para o \textit{SIMCC}, que lida com grandes volumes de dados acadêmicos. Além disso, a funcionalidade de buscar palavras-chave diretamente no painel de pesquisa e a possibilidade de observar mudanças temporais nos \textit{embeddings} ampliam significativamente as capacidades analíticas, conforme ilustrado por \cite{Wang}. Essa abordagem aprimora a experiência do usuário, promovendo insights mais profundos e contribuindo para a eficiência na análise de publicações científicas.

\begin{figure}[h]
    \caption{\label{wizmap-schema}Diagrama esquemático detalhado da ferramenta Wizmap para visualização de dados.}
    \begin{center}
        \includegraphics[scale=0.65]{figs/Wizmap.png}
    \end{center}
    \legend{Fonte: \citeonline[Traduzido, p. 1]{Wang}}
\end{figure}