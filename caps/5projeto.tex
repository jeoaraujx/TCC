\chapter[PROJETO DE DESENVOLVIMENTO]{PROJETO DE DESENVOLVIMENTO}

O desenvolvimento do artefato proposto neste trabalho segue uma abordagem estruturada e incremental, alinhada à metodologia Design Science Research (DSR) detalhada anteriormente. O objetivo é construir e validar um pipeline computacional que venha a aprimorar e complementar a análise de publicações científicas na plataforma do Sistema de Mapeamento de Competências Científicas da Bahia (SIMCC) \footnote{Encontrado em: \url{https://simcc.uesc.br/}}.

A plataforma SIMCC, conforme descrito por \citeonline{Santos_2024}, já possui uma arquitetura robusta para o mapeamento de competências, integrando fontes de dados heterogêneas como a Plataforma Lattes, a Plataforma Sucupira e o Journal Citation Reports (JCR). Seu sistema atual utiliza um processo de Extração, Transformação e Carga (ETL) para consolidar as informações em um banco de dados PostgreSQL e já emprega técnicas de Processamento de Linguagem Natural (PLN) para a recuperação de informações baseadas em termos e palavras-chave.

A busca lexical existente é uma ferramenta poderosa para a recuperação de informações diretas, onde o usuário já sabe quais termos procurar. No entanto, como aponta \citeonline{Jorge_2025}, a grande variação terminológica em domínios científicos complexos representa um desafio para a descoberta de conexões temáticas que não são óbvias. Nesse sentido, há uma oportunidade de incrementar a plataforma com uma nova camada de análise semântica, que permita ao usuário explorar o conhecimento de forma mais intuitiva e visual.

Este projeto de desenvolvimento detalha, portanto, a construção de um pipeline que representa uma evolução para a arquitetura do SIMCC. A solução proposta não visa substituir, mas sim enriquecer a funcionalidade de busca atual, introduzindo a modelagem de tópicos moderna com o BERTopic e o poder de contextualização do GPT-4. O resultado final é a geração de um mapa de clusters interativo, o WizMap \footnote{Encontrado em: \url{}}, que modifica a forma como os temas são descobertos na plataforma. O foco é transcender a lista de resultados tradicional, permitindo que os usuários naveguem visualmente pelas principais áreas de pesquisa, identifiquem temas emergentes e compreendam as relações entre os diferentes campos do conhecimento de maneira orgânica.

As subseções a seguir descrevem as etapas-chave do desenvolvimento deste artefato, abordando a arquitetura da solução, as tecnologias empregadas e o fluxo de processamento, desde a ingestão e pré-processamento dos dados até a geração dos tópicos, a rotulagem semântica e a visualização interativa dos resultados.

\section{Tecnologias Utilizadas}
O desenvolvimento do artefato proposto neste TCC assenta-se sobre a combinação da infraestrutura tecnológica já consolidada da plataforma SIMCC com um novo pipeline de análise semântica e visualização de dados, construído com ferramentas de ponta em Processamento de Linguagem Natural (PLN) e aprendizado de máquina. Esta seção detalha as tecnologias que compõem tanto a base da plataforma quanto o pipeline de inovação proposto.

\subsection{Base Tecnológica da Plataforma SIMCC}

A arquitetura do SIMCC, que serve como ponto de partida para este trabalho, foi projetada para ser robusta e escalável, utilizando um conjunto de tecnologias de mercado para a gestão de dados acadêmicos.

\begin{itemize}
    \item \textbf{Banco de Dados (PostgreSQL)}: A escolha do PostgreSQL como Sistema de Gerenciamento de Banco de Dados (SGBD) para o SIMCC é estratégica. Conforme detalhado por \citeonline[p. 255]{Jorge_2025}, o sistema aproveita os recursos nativos de PLN textual do PostgreSQL para indexação e busca. A sua capacidade de lidar com grandes volumes de dados e sua extensibilidade o tornam ideal para armazenar não apenas os dados estruturados extraídos dos currículos Lattes, mas também para suportar as representações vetoriais (embeddings) que são centrais neste projeto.
    \item \textbf{Orquestração de Dados (Apache Hop)}: Para a extração, transformação e carga (ETL) dos dados de fontes diversas como a Plataforma Lattes, Sucupira e JCR, o SIMCC utiliza o Apache Hop. Essa ferramenta é responsável por automatizar e coordenar o fluxo de ingestão de dados, garantindo a consistência e a qualidade das informações que alimentarão o pipeline de análise (\citeonline{Santos_2024}).
    \item \textbf{Linguagem de Back-end (Python)}: O back-end da plataforma foi desenvolvido em Python, uma escolha que se alinha perfeitamente aos objetivos deste TCC. A linguagem oferece um ecossistema maduro para ciência de dados e PLN, com bibliotecas como a NLTK, já em uso no SIMCC, e as bibliotecas \textit{Transformers} e \textit{BERTopic}, que são o cerne deste trabalho.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Arquitetura SIMCC.png}
    \caption{Arquitetura Geral do SIMCC}
    \label{fig:Arquitetura SIMCC}
    \legend{Fonte: \citeonline[p. 678]{Santos_2024}}
\end{figure}

\subsection{Pipeline de Modelagem e Análise de Tópicos}
Sobre a base existente do SIMCC, este projeto implementa um novo pipeline focado na descoberta e análise de conhecimento, utilizando as seguintes tecnologias:

\begin{itemize}
    \item \textbf{Modelagem de Tópicos (BERTopic)}: Para a identificação dos temas latentes nas publicações científicas, foi escolhido o BERTopic. Diferentemente de abordagens clássicas como o LDA, o BERTopic é um modelo moderno que utiliza embeddings contextuais para agrupar documentos com base na similaridade semântica. Sua arquitetura modular, que combina a geração de embeddings com Sentence-BERT (SBERT), a redução de dimensionalidade com UMAP e a clusterização com HDBSCAN, permite a extração de tópicos mais coerentes e representativos, sendo ideal para a complexidade e a variação terminológica dos dados acadêmicos.
    \item \textbf{Enriquecimento Semântico (GPT-4)}: Uma das principais inovações deste pipeline é o uso do GPT-4 para aprimorar a interpretabilidade dos tópicos. Após o BERTopic identificar os clusters temáticos e extrair palavras-chave, o GPT-4 é utilizado para gerar rótulos descritivos, concisos e semanticamente ricos para cada tópico. Esta etapa transforma uma lista de palavras-chave, muitas vezes ambígua, em um título claro e compreensível, agregando um valor analítico significativo e facilitando a compreensão dos resultados pelo usuário final.
    \item \textbf{Visualização Interativa (WizMap)}: Para apresentar os resultados da modelagem de tópicos de forma intuitiva, a ferramenta WizMap foi selecionada. Conforme descrito por \citeonline{Wang_2023}, o WizMap é uma solução de visualização escalável projetada para explorar grandes volumes de embeddings em uma interface inspirada em mapas. Em vez de apresentar os tópicos em listas ou gráficos estáticos, o WizMap permite que o usuário navegue por um mapa de clusters, explore as relações entre os temas, aplique zoom para investigar sub-tópicos e identifique visualmente as áreas de maior densidade de pesquisa, incrementando a forma como o conhecimento é descoberto na plataforma.
\end{itemize}

\section{Arquitetura da solução}
A arquitetura da solução proposta foi projetada para se integrar de maneira fluida à infraestrutura existente da plataforma SIMCC, adicionando uma nova camada de análise semântica e exploração de conhecimento. O pipeline é composto por uma sequência de etapas modulares que transformam os dados textuais brutos das publicações científicas em um mapa de tópicos interativo e semanticamente rico. O fluxo completo, desde a ingestão dos dados até a visualização, é detalhado nas subseções a seguir.

\subsection{Coleta e Pré-processamento dos Dados}
O ponto de partida do pipeline são os dados já consolidados no banco de dados PostgreSQL da plataforma SIMCC. Conforme documentado por \citeonline{Santos_2024}, esses dados, que incluem títulos, resumos e metadados de publicações científicas, já passaram por um rigoroso processo de ETL. Para adequá-los à modelagem de tópicos, uma etapa de pré-processamento textual é executada, consistindo em:

\begin{itemize}
    \item \textbf{Limpeza e Normalização:} Remoção de caracteres especiais, conversão de todo o texto para minúsculas e padronização de acentuação para garantir consistência.
    \item \textbf{Tokenização:} Divisão dos textos (títulos e resumos) em unidades menores (tokens), como palavras ou sentenças.
    \item \textbf{Remoção de Stopwords:} Exclusão de palavras funcionalmente importantes mas semanticamente vazias (e.g., "o", "de", "para", "com"), que poderiam gerar ruído na análise de tópicos.
    \item \textbf{Lematização:} Redução das palavras à sua forma canônica (lema) para agrupar diferentes flexões de um mesmo termo (e.g., "pesquisas", "pesquisou" e "pesquisando" são reduzidos a "pesquisar"), consolidando assim o vocabulário.
\end{itemize}

\subsection{Geração de Embeddings Contextuais}
Após o pré-processamento, os textos são transformados em representações vetoriais numéricas, conhecidas como embeddings. Esta é a etapa fundamental que permite a análise semântica. Para este fim, utiliza-se o modelo \textit{Sentence-BERT} (SBERT), especificamente a variante \textbf{\textit{paraphrase-multilingual-MiniLM-L12-v2}}\footnote{Encontrado em: \url{https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}}. A escolha deste modelo se justifica por sua alta eficiência e por sua capacidade de gerar embeddings semanticamente consistentes em múltiplos idiomas, uma característica essencial para a base de dados multilíngue do SIMCC. Cada documento (publicação) é, então, representado por um vetor denso em um espaço de alta dimensionalidade, onde a proximidade entre vetores indica similaridade semântica.

\begin{itemize}
    \item \textbf{Redução de Dimensionalidade com UMAP:} Os embeddings de alta dimensionalidade são projetados em um espaço de menor dimensão utilizando o algoritmo UMAP (Uniform Manifold Approximation and Projection). Esta técnica é crucial por preservar tanto a estrutura local quanto a global dos dados, garantindo que as relações semânticas entre os documentos sejam mantidas de forma fidedigna.
    \item \textbf{Clusterização com HDBSCAN:} No espaço de dimensionalidade reduzida, os vetores dos documentos são agrupados pelo algoritmo HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise). Diferentemente de métodos como o K-Means, o HDBSCAN é capaz de identificar clusters de diferentes formas e densidades, além de classificar documentos que não pertencem a nenhum grupo coeso como ruído (outliers). Cada cluster denso de documentos formado nesta etapa representa um tópico latente.
    \item \textbf{Extração de Palavras-Chave com c-TF-IDF:} Para tornar os clusters interpretáveis, o BERTopic emprega uma variação do TF-IDF, chamada c-TF-IDF (class-based Term Frequency-Inverse Document Frequency). Este método trata todos os documentos de um cluster como um único documento e calcula a importância das palavras, destacando os termos que são mais representativos de cada tópico em comparação com os demais.
\end{itemize}

\subsection{Rotulagem Semântica com GPT-4}

Apesar da eficácia do c-TF-IDF, a lista de palavras-chave gerada pode ser insuficiente para transmitir o significado completo de um tópico. Para superar essa limitação, o pipeline integra o GPT-4. As palavras-chave mais relevantes de cada tópico, identificadas na etapa anterior, são enviadas como entrada para a API do GPT-4. O LLM, então, utiliza sua capacidade de compreensão contextual e síntese para gerar um rótulo temático — um título curto, descritivo e humanamente inteligível — para cada cluster. Esta etapa enriquece drasticamente a interpretabilidade dos resultados, transformando agrupamentos de dados em conceitos claros e navegáveis.

\subsection{Visualização Interativa com WizMap}
A etapa final da arquitetura consiste na apresentação dos resultados. Os dados processados—incluindo os embeddings dos documentos, os clusters de tópicos e os rótulos gerados pelo GPT-4—são carregados na ferramenta de visualização WizMap \footnote{Encontrado em: \url{https://github.com/poloclub/wizmap}}. Esta plataforma renderiza um mapa bidimensional interativo onde cada ponto representa uma publicação científica, e os clusters de pontos formam "regiões" temáticas. O usuário pode:

\begin{itemize}
    \item \textbf{Navegar visualmente} pelo mapa para ter uma visão geral do cenário da pesquisa.
    \item \textbf{Aplicar zoom} em áreas densas para explorar sub-tópicos.
    \item \textbf{Passar o mouse} sobre os clusters para ver os rótulos gerados pelo GPT-4.
    \item \textbf{Buscar por palavras-chave} para destacar documentos relevantes diretamente no mapa.
\end{itemize}

Essa abordagem transforma a análise de dados em uma experiência exploratória, permitindo a descoberta de conhecimento de forma intuitiva e visual, complementando as funcionalidades de busca já existentes na plataforma SIMCC.

\begin{figure}[h]
    \caption{\label{wizmap-schema}Ferramenta Wizmap para visualização de dados.}
    \begin{center}
        \includegraphics[scale=0.65]{figs/Wizmap.png}
    \end{center}
    \legend{Fonte: \citeonline[Traduzido, p. 1]{Wang_2023}}
\end{figure}