\chapter[PROJETO DE DESENVOLVIMENTO]{PROJETO DE DESENVOLVIMENTO}

O desenvolvimento do artefato proposto neste trabalho segue uma abordagem estruturada e incremental, alinhada à metodologia Design Science Research (DSR) detalhada anteriormente. O objetivo é construir e validar um pipeline computacional que venha a aprimorar e complementar a análise de publicações científicas na plataforma do Sistema de Mapeamento de Competências Científicas da Bahia (SIMCC).

A plataforma SIMCC, conforme descrito por \citeonline{Santos_2024}, já possui uma arquitetura robusta para o mapeamento de competências, integrando fontes de dados heterogêneas como a Plataforma Lattes, a Plataforma Sucupira e o Journal Citation Reports (JCR). Seu sistema atual utiliza um processo de Extração, Transformação e Carga (ETL) para consolidar as informações em um banco de dados PostgreSQL e já emprega técnicas de Processamento de Linguagem Natural (PLN) para a recuperação de informações baseadas em termos e palavras-chave.

A busca lexical existente é uma ferramenta poderosa para a recuperação de informações diretas, onde o usuário já sabe quais termos procurar. No entanto, como aponta \citeonline{Jorge_2025}, a grande variação terminológica em domínios científicos complexos representa um desafio para a descoberta de conexões temáticas que não são óbvias. Nesse sentido, há uma oportunidade de incrementar a plataforma com uma nova camada de análise semântica, que permita ao usuário explorar o conhecimento de forma mais intuitiva e visual.

Este projeto de desenvolvimento detalha, portanto, a construção de um pipeline que representa uma evolução para a arquitetura do SIMCC. A solução proposta não visa substituir, mas sim enriquecer a funcionalidade de busca atual, introduzindo a modelagem de tópicos moderna com o BERTopic e o poder de contextualização do GPT-4. O resultado final é a geração de um mapa de clusters interativo, o WizMap, que modifica a forma como os temas são descobertos na plataforma. O foco é transcender a lista de resultados tradicional, permitindo que os usuários naveguem visualmente pelas principais áreas de pesquisa, identifiquem temas emergentes e compreendam as relações entre os diferentes campos do conhecimento de maneira orgânica.

As subseções a seguir descrevem as etapas-chave do desenvolvimento deste artefato, abordando a arquitetura da solução, as tecnologias empregadas e o fluxo de processamento, desde a ingestão e pré-processamento dos dados até a geração dos tópicos, a rotulagem semântica e a visualização interativa dos resultados.

\section{Tecnologias Utilizadas}
O desenvolvimento do artefato proposto neste TCC assenta-se sobre a combinação da infraestrutura tecnológica já consolidada da plataforma SIMCC com um novo pipeline de análise semântica e visualização de dados, construído com ferramentas de ponta em Processamento de Linguagem Natural (PLN) e aprendizado de máquina. Esta seção detalha as tecnologias que compõem tanto a base da plataforma quanto o pipeline de inovação proposto.

\subsection{Base Tecnológica da Plataforma SIMCC}

A arquitetura do SIMCC, que serve como ponto de partida para este trabalho, foi projetada para ser robusta e escalável, utilizando um conjunto de tecnologias de mercado para a gestão de dados acadêmicos.

\begin{itemize}
    \item \textbf{Banco de Dados (PostgreSQL)}: A escolha do PostgreSQL como Sistema de Gerenciamento de Banco de Dados (SGBD) para o SIMCC é estratégica. Conforme detalhado por \citeonline[p. 255]{Jorge_2025}, o sistema aproveita os recursos nativos de PLN textual do PostgreSQL para indexação e busca. A sua capacidade de lidar com grandes volumes de dados e sua extensibilidade o tornam ideal para armazenar não apenas os dados estruturados extraídos dos currículos Lattes, mas também para suportar as representações vetoriais (embeddings) que são centrais neste projeto.
    \item \textbf{Orquestração de Dados (Apache Hop)}: Para a extração, transformação e carga (ETL) dos dados de fontes diversas como a Plataforma Lattes, Sucupira e JCR, o SIMCC utiliza o Apache Hop. Essa ferramenta é responsável por automatizar e coordenar o fluxo de ingestão de dados, garantindo a consistência e a qualidade das informações que alimentarão o pipeline de análise (\citeonline{Santos_2024}).
    \item \textbf{Linguagem de Back-end (Python)}: O back-end da plataforma foi desenvolvido em Python, uma escolha que se alinha perfeitamente aos objetivos deste TCC. A linguagem oferece um ecossistema maduro para ciência de dados e PLN, com bibliotecas como a NLTK, já em uso no SIMCC, e as bibliotecas \textit{Transformers} e \textit{BERTopic}, que são o cerne deste trabalho.
\end{itemize}

\subsection{Pipeline de Modelagem e Análise de Tópicos}
Sobre a base existente do SIMCC, este projeto implementa um novo pipeline focado na descoberta e análise de conhecimento, utilizando as seguintes tecnologias:

\begin{itemize}
    \item \textbf{Modelagem de Tópicos (BERTopic)}: Para a identificação dos temas latentes nas publicações científicas, foi escolhido o BERTopic. Diferentemente de abordagens clássicas como o LDA, o BERTopic é um modelo moderno que utiliza embeddings contextuais para agrupar documentos com base na similaridade semântica. Sua arquitetura modular, que combina a geração de embeddings com Sentence-BERT (SBERT), a redução de dimensionalidade com UMAP e a clusterização com HDBSCAN, permite a extração de tópicos mais coerentes e representativos, sendo ideal para a complexidade e a variação terminológica dos dados acadêmicos.
    \item \textbf{Enriquecimento Semântico (GPT-4)}: Uma das principais inovações deste pipeline é o uso do GPT-4 para aprimorar a interpretabilidade dos tópicos. Após o BERTopic identificar os clusters temáticos e extrair palavras-chave, o GPT-4 é utilizado para gerar rótulos descritivos, concisos e semanticamente ricos para cada tópico. Esta etapa transforma uma lista de palavras-chave, muitas vezes ambígua, em um título claro e compreensível, agregando um valor analítico significativo e facilitando a compreensão dos resultados pelo usuário final.
    \item \textbf{Visualização Interativa (WizMap)}: Para apresentar os resultados da modelagem de tópicos de forma intuitiva, a ferramenta WizMap foi selecionada. Conforme descrito por \citeonline{Wang_2023}, o WizMap é uma solução de visualização escalável projetada para explorar grandes volumes de embeddings em uma interface inspirada em mapas. Em vez de apresentar os tópicos em listas ou gráficos estáticos, o WizMap permite que o usuário navegue por um mapa de clusters, explore as relações entre os temas, aplique zoom para investigar sub-tópicos e identifique visualmente as áreas de maior densidade de pesquisa, incrementando a forma como o conhecimento é descoberto na plataforma.
\end{itemize}

\section{Arquitetura da solução}
A arquitetura da solução proposta foi projetada para se integrar de maneira fluida à infraestrutura existente da plataforma SIMCC, adicionando uma nova camada de análise semântica e exploração de conhecimento. O pipeline é composto por uma sequência de etapas modulares que transformam os dados textuais brutos das publicações científicas em um mapa de tópicos interativo e semanticamente rico. O fluxo completo, desde a ingestão dos dados até a visualização, é detalhado nas subseções a seguir.

\subsection{Coleta e Pré-processamento dos Dados}
O ponto de partida do pipeline são os dados já consolidados no banco de dados PostgreSQL da plataforma SIMCC. Conforme documentado por \citeonline{Santos_2024}, esses dados, que incluem títulos, resumos e metadados de publicações científicas, já passaram por um rigoroso processo de ETL. Para adequá-los à modelagem de tópicos, uma etapa de pré-processamento textual é executada, consistindo em:

\begin{itemize}
    \item \textbf{Limpeza e Normalização:} Remoção de caracteres especiais, conversão de todo o texto para minúsculas e padronização de acentuação para garantir consistência.
    \item \textbf{Tokenização:} Divisão dos textos (títulos e resumos) em unidades menores (tokens), como palavras ou sentenças.
    \item \textbf{Remoção de Stopwords:} Exclusão de palavras funcionalmente importantes mas semanticamente vazias (e.g., "o", "de", "para", "com"), que poderiam gerar ruído na análise de tópicos.
    \item \textbf{Lematização:} Redução das palavras à sua forma canônica (lema) para agrupar diferentes flexões de um mesmo termo (e.g., "pesquisas", "pesquisou" e "pesquisando" são reduzidos a "pesquisar"), consolidando assim o vocabulário.
\end{itemize}

\subsection{Geração de Embeddings Contextuais}
Após o pré-processamento, os textos são transformados em representações vetoriais numéricas, conhecidas como embeddings. Esta é a etapa fundamental que permite a análise semântica. Para este fim, utiliza-se o modelo \textit{Sentence-BERT} (SBERT), especificamente a variante \textbf{\textit{paraphrase-multilingual-MiniLM-L12-v2}}. A escolha deste modelo se justifica por sua alta eficiência e por sua capacidade de gerar embeddings semanticamente consistentes em múltiplos idiomas, uma característica essencial para a base de dados multilíngue do SIMCC. Cada documento (publicação) é, então, representado por um vetor denso em um espaço de alta dimensionalidade, onde a proximidade entre vetores indica similaridade semântica.

\begin{itemize}
    \item \textbf{Redução de Dimensionalidade com UMAP:} Os embeddings de alta dimensionalidade são projetados em um espaço de menor dimensão utilizando o algoritmo UMAP (Uniform Manifold Approximation and Projection). Esta técnica é crucial por preservar tanto a estrutura local quanto a global dos dados, garantindo que as relações semânticas entre os documentos sejam mantidas de forma fidedigna.
    \item \textbf{Clusterização com HDBSCAN:} No espaço de dimensionalidade reduzida, os vetores dos documentos são agrupados pelo algoritmo HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise). Diferentemente de métodos como o K-Means, o HDBSCAN é capaz de identificar clusters de diferentes formas e densidades, além de classificar documentos que não pertencem a nenhum grupo coeso como ruído (outliers). Cada cluster denso de documentos formado nesta etapa representa um tópico latente.
    \item \textbf{Extração de Palavras-Chave com c-TF-IDF:} Para tornar os clusters interpretáveis, o BERTopic emprega uma variação do TF-IDF, chamada c-TF-IDF (class-based Term Frequency-Inverse Document Frequency). Este método trata todos os documentos de um cluster como um único documento e calcula a importância das palavras, destacando os termos que são mais representativos de cada tópico em comparação com os demais.
\end{itemize}

\subsection{Redução de Dimensionalidade}
A redução de dimensionalidade é uma etapa fundamental em pipelines que lidam com dados de alta dimensionalidade, como embeddings gerados para textos. Ela transforma os dados em representações mais compactas, preservando informações essenciais e eliminando redundâncias ou ruídos. No projeto, utilizamos o \textit{Uniform Manifold Approximation and Projection (UMAP)}\footnote{Econtrado em: \url{https://umap-learn.readthedocs.io/en/latest/basic_usage.html}} \citeonline{McInnes_2018} devido à sua capacidade de preservar tanto as relações locais quanto globais entre os dados, o que é crucial para a análise semântica e a modelagem de tópicos.

O UMAP é um algoritmo de redução de dimensionalidade baseado em princípios matemáticos da teoria de grafos e geometria algébrica. Ele funciona em várias etapas:
    \begin{itemize}
        \item \textbf{Construção do Grafo de Alta Dimensionalidade:} O UMAP começa representando os dados originais como um grafo ponderado em alta dimensionalidade. Cada ponto de dado é conectado aos seus vizinhos mais próximos com pesos que indicam a proximidade entre os pontos. Essa etapa é realizada usando algoritmos como \textit{k-nearest neighbors (k-NN)}, que identificam os pontos mais próximos no espaço original.
        \item \textbf{Definição de uma Distribuição de Probabilidade Local:} Uma distribuição de probabilidade local é ajustada ao redor de cada ponto, calculando a probabilidade de uma conexão entre pontos baseando-se na distância.
        \item \textbf{Projeção para Baixa Dimensionalidade:} O algoritmo então otimiza um grafo em um espaço de menor dimensionalidade. Ele tenta preservar a estrutura do grafo original, minimizando a diferença entre os dois grafos usando uma função de perda. Esse processo garante que as relações locais (proximidade entre pontos próximos) e globais (estrutura dos clusters) sejam mantidas.
        \item \textbf{Representação Final:} Os dados são projetados no espaço de menor dimensionalidade (normalmente 2D ou 3D), criando uma representação compacta que pode ser utilizada para agrupamentos (como no HDBSCAN) ou visualizações interativas.    
    \end{itemize}

O \textit{UMAP} utiliza várias técnicas essenciais para a redução de dimensionalidade. A primeira delas é o \textit{\textbf{k-Nearest Neighbors (k-NN)}}, que identifica os pontos mais próximos no espaço de alta dimensionalidade para criar um grafo local. A escolha do número de vizinhos (k) é um parâmetro crítico, pois influencia o equilíbrio entre a preservação das relações locais e globais. Em seguida, o algoritmo utiliza um otimizador de gradiente para ajustar o grafo em baixa dimensionalidade, minimizando as diferenças estruturais entre os grafos original e projetado.

Além disso, o \textit{UMAP} aplica geometria algébrica, assumindo que os dados em alta dimensionalidade estão distribuídos em um espaço chamado \textbf{manifold}. Ele projeta esses dados em uma variedade de baixa dimensionalidade, preservando tanto as relações locais quanto as estruturas globais dos dados. Essa combinação de técnicas permite ao \textit{UMAP} gerar representações compactas e de alta qualidade, mantendo a integridade semântica dos dados.

Portanto, ele é ideal para o pipeline proposto no \textit{SIMCC} devido à sua capacidade de lidar eficientemente com embeddings densos e multilingues gerados pelos modelos de linguagem. Sua habilidade em preservar estruturas semânticas tanto locais quanto globais permite que clusters de tópicos sejam mais coerentes e representativos. Além disso, a combinação de escalabilidade, flexibilidade e qualidade de projeção o torna uma escolha natural para visualizações interativas, como no caso do \textit{WizMap}, onde os tópicos serão exibidos como clusters organizados.

Dessa forma, ele integra perfeitamente ao restante do pipeline, fornecendo representações compactas e confiáveis que servem como base para o agrupamento de tópicos com o \textit{HDBSCAN} e a análise semântica enriquecida pelo \textit{GPT-4}. Essa combinação garante que a redução de dimensionalidade não seja apenas uma etapa intermediária, mas um componente estratégico para melhorar a eficácia e eficiência da modelagem de tópicos.

\subsection{Agrupamento de Tópicos (Clustering)}
O agrupamento de tópicos também é uma etapa essencial no pipeline, onde textos com características semelhantes são agrupados para formar clusters temáticos. Esses clusters são agrupamentos naturais de dados que compartilham características semelhantes. No contexto de modelagem de tópicos, cada cluster representa um grupo de textos que abordam temas relacionados, com base em suas representações numéricas (embeddings). No contexto do projeto, utilizamos o \textbf{\textit{Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)}} devido à sua capacidade de lidar com dados complexos, densidades variáveis e ruídos, oferecendo agrupamentos precisos e adaptáveis.

O HDBSCAN é uma extensão hierárquica do algoritmo \textbf{\textit{Density-Based Spatial Clustering of Applications with Noise (DBSCAN)}}, projetado para detectar clusters em dados de densidades variáveis sem exigir que o número de clusters seja definido previamente. O funcionamento do HDBSCAN envolve várias etapas:

\begin{itemize}
    \item \textbf{Construção do Grafo de Distâncias:} O HDBSCAN inicia criando um grafo onde os pontos de dados são conectados com base na distância entre eles, calculada a partir dos embeddings. Para isso, utiliza a distância métrica escolhida (como euclidiana ou coseno).

    \item \textbf{Árvore de Alcance Mínimo \textit{(Minimum Spanning Tree)}:} A partir do grafo, o algoritmo constrói uma árvore de alcance mínimo, que conecta todos os pontos com a menor soma de pesos (distâncias). Isso cria uma representação hierárquica das relações entre os pontos.
    
    \item \textbf{Densidade e Ruído:} A densidade de cada cluster é avaliada com base no número de pontos próximos e suas distâncias. Pontos que não atingem a densidade mínima são considerados ruídos e não são atribuídos a nenhum cluster.

    \item \textbf{\textit{Cluster} Condensado:} A árvore é podada iterativamente para remover arestas que representam ruídos, revelando os clusters restantes. Esses agrupamentos finais são obtidos com base em densidades locais, permitindo identificar clusters de diferentes formas e tamanhos.

    \item \textbf{\textit{Soft Clustering} (Agrupamento Suave):} O HDBSCAN fornece uma saída de probabilidade que indica a força de associação de cada ponto ao cluster, em vez de uma associação rígida. Isso é útil para lidar com pontos que podem estar na fronteira entre clusters.
\end{itemize}

O HDBSCAN se destaca por sua capacidade de lidar com densidades variadas e formatos complexos de clusters, ao contrário de técnicas como o \textit{k-means}, que assumem densidade uniforme e formas esféricas. Essa adaptabilidade é crucial para dados heterogêneos, como textos acadêmicos. Outra vantagem significativa é sua robustez ao ruído, já que o HDBSCAN identifica pontos que não se encaixam em nenhum cluster e os trata como ruídos, em vez de forçá-los a se agrupar, o que melhora a qualidade geral dos tópicos formados. Além disso, o algoritmo não requer que o número de clusters seja pré-definido, determinando-o automaticamente com base nos dados, eliminando a necessidade de suposições iniciais que podem ser imprecisas. O \textit{soft clustering} é outra característica marcante, pois fornece uma probabilidade de associação para cada ponto, permitindo interpretações mais flexíveis e precisas, especialmente em cenários onde os limites entre \textit{clusters} não são claros. Por fim, o HDBSCAN é altamente escalável, capaz de lidar com grandes volumes de dados textuais, como os do SIMCC, de forma eficiente e robusta.

Comparado a outras técnicas, o HDBSCAN supera o \textit{k-means} \citeonline{MacQueen_1967}, que exige o número de clusters como entrada e é limitado a formas esféricas, e o DBSCAN, que não é eficaz em densidades variadas ou em dados de alta dimensionalidade. Além disso, é mais flexível e escalável do que o \textit{Agglomerative Clustering}, que é adequado para pequenos conjuntos de dados, mas menos eficiente em grandes volumes. Essas características tornam o HDBSCAN uma escolha superior para a modelagem de tópicos em corpora complexos e extensos, como os encontrados na plataforma SIMCC.

Estudos recentes, como os de \citeonline{Gana_2024} e \citeonline{Jung_2024}, destacam o sucesso do HDBSCAN em tarefas de modelagem de tópicos, especialmente quando combinado com UMAP para reduzir a dimensionalidade antes do agrupamento. Essa combinação oferece clusters mais coesos e representativos.

\subsection{Rotulagem de Tópicos}

No processo de modelagem temática, a etapa de rotulagem desempenha um papel essencial, garantindo que os clusters sejam identificados com rótulos representativos e facilmente interpretáveis. Para este projeto, o GPT-4 foi escolhido como ferramenta principal para enriquecer semanticamente os rótulos, aproveitando sua habilidade avançada de gerar textos contextualmente precisos e alinhados com o significado subjacente dos tópicos.

Uma vez que os clusters de tópicos são formados pelo BERTopic, suas palavras-chave representativas (identificadas por métodos como o c-TF-IDF) são enviadas para a API da OpenAI. O GPT-4 interpreta essas palavras-chave e o contexto geral do cluster, gerando rótulos mais descritivos, claros e semanticamente ricos. Esse processo transcende a rotulagem padrão automatizada do BERTopic, que utiliza apenas as palavras mais frequentes ou relevantes de cada cluster para gerar nomes, frequentemente resultando em rótulos genéricos ou pouco informativos.

O GPT-4 oferece vantagens significativas para a rotulagem de tópicos, destacando-se pela sua capacidade de contextualização e coerência. Graças ao treinamento em um extenso corpus textual, o GPT-4 consegue interpretar não apenas as palavras-chave de um cluster, mas também as relações contextuais entre elas. Isso permite a criação de rótulos mais detalhados e representativos, capturando o significado subjacente dos tópicos de maneira precisa. Essa capacidade é especialmente útil para lidar com temas complexos, onde as conexões contextuais são fundamentais para a interpretação.

Outro ponto forte do GPT-4 é sua habilidade de enriquecer semanticamente os rótulos, indo além das limitações de abordagens estatísticas tradicionais, como o c-TF-IDF utilizado no BERTopic. Em clusters interdisciplinares, o GPT-4 é capaz de identificar conexões que podem não ser evidentes apenas pelas palavras-chave, oferecendo rótulos mais ricos e informativos. Além disso, sua flexibilidade multilingue é crucial para projetos como o SIMCC, permitindo a geração de rótulos consistentes e semânticos em idiomas como português, inglês e espanhol. Essa capacidade de traduzir conceitos entre idiomas de forma precisa reforça sua aplicação em contextos multilíngues. Por fim, o GPT-4 possibilita rotulagem personalizada, permitindo ajustes nas instruções para gerar rótulos detalhados ou simplificados, dependendo da necessidade do projeto ou do público-alvo, tornando o processo ainda mais versátil e adaptado aos objetivos específicos da análise.

A técnica padrão de rotulagem do BERTopic utiliza algoritmos como c-TF-IDF para identificar as palavras mais relevantes de cada cluster. Embora eficiente, essa abordagem é limitada à seleção automática de palavras-chave, o que muitas vezes resulta em rótulos genéricos ou pouco descritivos, especialmente em tópicos complexos ou com sobreposição temática. O BERTopic não considera o contexto semântico amplo ou as relações entre palavras no cluster, tornando os rótulos menos precisos em temas interdisciplinares.

Por outro lado, o GPT-4 supera essas limitações ao interpretar os clusters de maneira contextual e ao enriquecer os rótulos com descrições detalhadas. Por exemplo, se um cluster contém palavras como "transformers", "BERT" e "PLN", o BERTopic poderia gerar um rótulo genérico como "Modelos de Transformadores". Já o GPT-4 poderia contextualizar essas palavras para criar um rótulo mais informativo, como "Aplicações de Modelos de Transformadores em Processamento de Linguagem Natural".

A integração do GPT-4 no pipeline para rotulagem de tópicos representa um avanço significativo em termos de qualidade e usabilidade dos rótulos gerados. Comparado à técnica padrão do BERTopic, ele oferece uma compreensão mais profunda e contextual dos clusters, resultando em rótulos mais representativos e úteis para análise. Essa abordagem é especialmente vantajosa para a plataforma SIMCC, onde a clareza e a precisão semântica dos tópicos são cruciais para facilitar a exploração e a compreensão das produções científicas indexadas.

\subsection{Visualização Interativa (WizMap)}
O \textit{WizMap} é uma ferramenta avançada de visualização interativa projetada para explorar e interpretar grandes volumes de \textit{embeddings} em espaços de alta dimensionalidade, como descrito por \citeonline{Wang_2023}. Essa solução é particularmente eficaz para lidar com os desafios apresentados pelo volume e pela complexidade de \textit{embeddings} gerados por modelos de aprendizado de máquina, permitindo que pesquisadores compreendam a estrutura global e local desses dados. O \textit{WizMap} adota uma abordagem de visualização inspirada em mapas, integrando técnicas como o uso de contornos de densidade e resumos adaptativos de embeddings em múltiplas resoluções.

No contexto do SIMCC, a aplicação do \textit{WizMap} oferece uma oportunidade única para tornar a exploração de tópicos emergentes mais intuitiva e acessível. Após a modelagem de tópicos e a geração de \textit{embeddings} com o BERTopic, combinada com técnicas de redução de dimensionalidade, como o UMAP, o \textit{WizMap}\footnote{Encontrado em: \url{https://poloclub.github.io/wizmap/}} organiza os tópicos em uma interface visual. Essa interface utiliza \textit{quadtree} para segmentar os dados e sumarizar os tópicos em diferentes níveis de granularidade, permitindo que os usuários explorem \textit{clusters} de tópicos em detalhes ou obtenham uma visão geral, ajustando a profundidade da análise conforme necessário.

Comparado a métodos tradicionais, como \textit{scatter plots} estáticos ou projeções simples, o \textit{WizMap} se destaca pela capacidade de manipular milhões de pontos em navegadores sem a necessidade de infraestrutura pesada, graças ao uso de tecnologias como \textit{WebGL}\footnote{Econtrado em: \url{https://get.webgl.org/}} e \textit{Web Workers}\footnote{Encontrado em: \url{https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers}}. Isso torna a ferramenta altamente escalável e adequada para o \textit{SIMCC}, que lida com grandes volumes de dados acadêmicos. Além disso, a funcionalidade de buscar palavras-chave diretamente no painel de pesquisa e a possibilidade de observar mudanças temporais nos \textit{embeddings} ampliam significativamente as capacidades analíticas, conforme ilustrado por \citeonline{Wang_2023}. Essa abordagem aprimora a experiência do usuário, promovendo insights mais profundos e contribuindo para a eficiência na análise de publicações científicas.

\begin{figure}[h]
    \caption{\label{wizmap-schema}Diagrama esquemático detalhado da ferramenta Wizmap para visualização de dados.}
    \begin{center}
        \includegraphics[scale=0.65]{figs/Wizmap.png}
    \end{center}
    \legend{Fonte: \citeonline[Traduzido, p. 1]{Wang_2023}}
\end{figure}