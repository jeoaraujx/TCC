\chapter[REFERENCIAL TEÓRICO]{REFERENCIAL TEÓRICO}
O referencial teórico deste estudo abordará diversos aspectos cruciais relacionados à Ciência da Informação, Análise de Publicações Científicas, Processamento de Linguagem Natural (PLN), Modelagem de Tópicos, e Modelos de Linguagem de Grande Escala (LLMs). 

\input{caps/2.1 - Ciência da Informação}

\input{caps/2.2 - Processamento de Linguagem}

\input{caps/2.3 - Representação vetorial}

\input{caps/2.4 - Abordagem tradicional}

% \section{Abordagens Tradicionais de Modelagem de Tópicos}

% Com o crescimento exponencial de dados textuais e a consequente necessidade de organizar informação em larga escala, a modelagem de tópicos consolidou-se como uma técnica fundamental na área de \gls{PLN}. Em termos gerais, trata-se de um conjunto de métodos estatísticos cujo objetivo é identificar estruturas semânticas latentes — denominadas \textit{tópicos} — em coleções de documentos. Assim, essas técnicas permitem inferir distribuições temáticas que não são explicitamente observáveis, mas que emergem a partir de regularidades no uso do vocabulário. Essa perspectiva abriu caminho para aplicações em áreas diversas, desde ciências sociais até biomedicina (\citeonline{Jung_2024}).

% Entre as abordagens iniciais destacam-se três marcos históricos: a \textit{Latent Semantic Analysis} \gls{LSA}, a \textit{Probabilistic Latent Semantic Analysis} \gls{PLSA} e a \textit{Latent Dirichlet Allocation} \gls{LDA}. Esses métodos não apenas moldaram a compreensão inicial sobre a representação semântica de textos, como também estabeleceram fundamentos conceituais e metodológicos que orientaram o desenvolvimento de modelos mais avançados.

% A \gls{LSA}, proposta por \citeonline{Deerwester_1990}, parte da decomposição de matrizes termo-documento por meio da técnica de \textit{Singular Value Decomposition} (\gls{SVD}). Nesse enquadramento, documentos e termos são projetados em um espaço vetorial de dimensionalidade reduzida, o que permite atenuar ruídos lexicais e capturar relações de similaridade latentes. Apesar de sua relevância histórica, a linearidade da \gls{LSA} e sua insensibilidade a variações contextuais limitam seu desempenho em cenários onde relações semânticas complexas são determinantes (\citeonline{George_2023, Xie_2020}).

% Com o intuito de superar parte dessas limitações, \citeonline{Hofmann_1999,Hofmann_2001} introduziram a \gls{PLSA}, que reformulou a representação semântica a partir de um modelo probabilístico. Nessa abordagem, cada ocorrência de palavra em um documento é modelada como proveniente de um tópico latente, de forma que a probabilidade conjunta de palavra $w$ e documento $d$ é expressa como:
% \[
% P(w, d) = \sum_{z \in Z} P(z|d) \, P(w|z),
% \]
% onde $z$ representa o conjunto de tópicos latentes. Embora tenha representado um avanço em relação à \gls{LSA}, a \gls{PLSA} apresenta limitações notáveis, em especial no que se refere à escalabilidade: o número de parâmetros cresce linearmente com a quantidade de documentos, o que compromete sua generalização e a torna suscetível a \textit{overfitting} (\citeonline{Datchanamoorthy_2023}).

% A evolução natural desse paradigma ocorreu com a formulação da \gls{LDA}, proposta por \citeonline{Blei_2003}. Ao contrário da \gls{PLSA}, a \gls{LDA} incorpora uma camada Bayesiana por meio da utilização de distribuições de \textit{Dirichlet} como \textit{priors}. Essa estrutura permite regularizar o modelo e definir uma distribuição de tópicos não apenas a nível de documento, mas também a nível de corpus, resultando em maior robustez e interpretabilidade. A \gls{LDA} parte da premissa de que cada documento é representado como uma mistura de tópicos, e cada tópico, por sua vez, é caracterizado por uma distribuição de palavras. Essa formulação tornou o modelo amplamente aplicável em diferentes domínios, como saúde pública (\citeonline{Mifrah_2020}) e eficiência energética (\citeonline{Polyzos_2022}).

% Apesar de sua influência, tanto a \gls{LSA} quanto a \gls{PLSA} e a \gls{LDA} compartilham limitações estruturais. Todas operam no paradigma de \textit{bag-of-words}, que ignora a ordem e o contexto local das palavras, o que frequentemente conduz a representações semânticas superficiais em textos técnicos ou multilíngues (\citeonline{George_2023, Xie_2020}). Além disso, a sensibilidade da \gls{LDA} à definição do número de tópicos ($K$) representa um desafio adicional: valores reduzidos podem fundir tópicos distintos em um único, enquanto valores elevados podem fragmentar temas coesos em subtemas artificiais (\citeonline{Datchanamoorthy_2023}).

% \begin{citacao}
% A sensibilidade do LDA ao parâmetro do número de temas ($K$) é uma de suas desvantagens. Encontrar o valor ideal para ($K$) pode ser desafiador. O modelo pode simplificar excessivamente e combinar diferentes temas em um só se ($K$) for configurado muito baixo. No entanto, se ($K$) for configurado muito alto, o modelo pode se tornar muito complexo e produzir temas errôneos (\citeonline[Traduzido]{Datchanamoorthy_2023}).
% \end{citacao}

% Essas restrições evidenciam que, embora fundamentais, tais técnicas não capturam relações profundas e não lineares entre palavras e tópicos. Esse cenário motivou a emergência de abordagens modernas baseadas em \textit{embedding} e arquiteturas de \textit{transformer} (\citeonline{vaswani_2017, Devlin_2019, Radford_2018}), que oferecem maior sensibilidade contextual e escalabilidade para corpora heterogêneos e de grande volume.

% \section{BERTopic: Uma Abordagem Moderna}

% O \textit{Bidirectional Encoder Representations from Transformers} \gls{BERT}, introduzido por \citeonline{Devlin_2019}, marcou um avanço significativo no campo do \textit{Natural Language Processing} \gls{NLP}. Baseado na arquitetura de \textit{Transformers} (\citeonline{vaswani_2017}), o BERT emprega o mecanismo de \textit{self-attention} para capturar relações contextuais entre palavras em um texto. Diferentemente de abordagens anteriores, que analisavam sequências de maneira unidirecional, o BERT considera simultaneamente o contexto à esquerda e à direita de cada palavra, resultando em \textit{embeddings} ricos e contextuais. Essa característica tornou o BERT amplamente utilizado em tarefas como classificação de texto, análise de sentimentos e resposta a perguntas.

% Apesar de sua relevância, o BERT não foi projetado para tarefas de similaridade semântica entre sentenças ou documentos, pois os vetores que gera não são diretamente comparáveis em termos de proximidade semântica (\citeonline{Reimers_2019}). Essa limitação levou ao desenvolvimento do \textit{Sentence-BERT} \gls{S-BERT}, uma variante que adapta o BERT ao treinamento em redes siamesas (\textit{Siamese Networks}) e funções de perda específicas, como \textit{triplet loss}. O resultado é a produção de \textit{embeddings} que são calculados por meio de técnicas como \textit{Class-based Term Frequency-Inverse Document Frequency} (c-TF-IDF), que ajusta os pesos das palavras com base em suas frequências e relevâncias dentro de um corpus, elas podem ser comparadas de forma eficiente por meio de medidas de distância, como \textit{cosine similarity}, viabilizando tarefas de busca semântica e agrupamento de documentos.

% Sobre essa base, \citeonline{Grootendorst_2022} propôs o \textit{BERTopic}, que não deve ser entendido como um único modelo, mas como um \textit{pipeline} que integra diferentes técnicas complementares para a modelagem de tópicos. Esse arranjo inicia-se pela geração de \textit{embeddings} com o S-BERT, etapa que garante representações semânticas adequadas para comparação entre artigos científicos. Essa combinação permite que o BERTopic identifique tópicos de maneira dinâmica e precisa, mesmo em grandes volumes de dados textuais diversificados (\citeonline{George_2023, Jung_2024, Datchanamoorthy_2023}).

% \begin{figure}[h]
%     \centering
%     \caption{\label{bertopic-eschema}Diagrama esquemático detalhado da comparação de métricas de avaliação entre modelos.}
%     \includegraphics[scale=0.8]{figs/Bertopic.png}
%     \caption*{\footnotesize Fonte: (\citeonline[p. 7, Tradução nossa]{Jung_2024})}
% \end{figure}

% Como observado no diagrama comparativo entre modelos, a etapa de redução de dimensionalidade no pipeline utiliza o \textit{Uniform Manifold Approximation and Projection} \gls{UMAP} (\citeonline{McInnes_2018}, uma técnica que projeta vetores de alta dimensionalidade em um espaço reduzido. Essa abordagem se fundamenta em princípios teóricos de geometria Riemanniana e topologia algébrica , o que a diferencia de métodos mais antigos, como o \textit{t-SNE} (\citeonline{Maaten_2008}), e lhe confere maior escalabilidade e eficiência para a análise de grandes volumes de dados. O UMAP opera em duas fases principais: primeiro, constrói um grafo ponderado que representa a estrutura topológica dos dados em alta dimensão; em seguida, projeta esse grafo para um espaço de baixa dimensão, otimizando o layout para minimizar a entropia cruzada entre as duas representações. Essa metodologia é crucial para preservar tanto as estruturas locais quanto as globais do corpus, garantindo a coesão semântica dos dados. Ao aplicar o UMAP ao conjunto de publicações científicas da plataforma SIMCC, é possível manter a fidelidade das relações entre os documentos, um requisito fundamental para a subsequente fase de agrupamento do BERTopic.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/UMAP.png}
%     \caption{Diagrama ilustrativo do UMAP, que demonstra a relação entre os hiperparâmetros \textit{n\_neighbors} e \textit{min\_dist} e a representação visual dos dados. O parâmetro \textit{n\_neighbors} controla a balança entre a preservação da estrutura global (valores altos) e local (valores baixos), enquanto \textit{min\_dist} ajusta a densidade dos agrupamentos, determinando a proximidade entre os pontos no espaço de baixa dimensionalidade. Esta visualização é crucial para otimizar o algoritmo e garantir que a estrutura semântica das publicações científicas seja fielmente representada para a subsequente etapa de agrupamento.}
%     \caption*{\footnotesize Fonte: (\citeonline[p. 24]{McInnes_2018})}
%     \label{fig:UMAP}
% \end{figure}

% Com os vetores de alta dimensionalidade reduzidos pelo UMAP, a etapa subsequente é o agrupamento por meio do \textit{Hierarchical Density-Based Spatial Clustering of Applications with Noise} \gls{HDBSCAN}. Diferentemente de métodos clássicos como o K-Means, que assume \textit{clusters} esféricos e de densidade uniforme, o HDBSCAN é um algoritmo de agrupamento baseado em densidade que não faz suposições prévias sobre a forma ou a densidade dos agrupamentos (\citeonline{Campello_2013}). Sua arquitetura hierárquica constrói uma árvore de conectividade que reflete a estrutura de densidade subjacente dos dados, permitindo a identificação de \textit{clusters} de densidade variável. Essa capacidade é particularmente relevante para a análise de publicações científicas, onde a distribuição dos tópicos tende a ser heterogênea. O HDBSCAN também se destaca por sua robustez ao tratar documentos que não se ajustam a nenhum padrão temático, classificando-os como outliers de forma intrínseca, sem a necessidade de um passo de pós-processamento. Essa característica é especialmente relevante em contextos de produção científica, onde coexistem tanto publicações centrais com alta densidade de tópicos quanto trabalhos periféricos ou com temas emergentes. Essa abordagem garante que a sua análise não apenas identifique os tópicos dominantes, mas também lide eficientemente com a diversidade e o ruído natural do corpus da plataforma SIMCC.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/HDBSCAN.png}
%     \caption{Figura ilustrativa de um \textit{dataset} sintético com quatro \textit{clusters} e ruído de fundo. A imagem demonstra o tipo de desafio que o algoritmo HDBSCAN é capaz de superar, como a identificação de agrupamentos de densidades e formas variadas, além de tratar \textit{outliers} de forma eficiente. Este comportamento é ideal para a análise de publicações científicas, onde a distribuição dos tópicos tende a ser heterogênea e não segue padrões geométricos rígidos.}
%     \caption*{\footnotesize Fonte: (\citeonline[p. 16]{Campello_2013})}
%     \label{fig:HDBSCAN}
% \end{figure}

% Por fim, o BERTopic aplica o \textit{class-based Term Frequency-Inverse Document Frequency} (c-TF-IDF), que trata cada cluster como um único documento. Essa abordagem destaca termos distintivos de cada grupo, permitindo identificar palavras-chave representativas mesmo quando não são as mais frequentes (\citeonline{Gana_2024, Grootendorst_2022}). O c-TF-IDF, portanto, fornece uma base interpretável para a descrição de cada tópico.

% A combinação dessas etapas — \textit{embeddings} com S-BERT, redução de dimensionalidade com UMAP, clusterização com HDBSCAN e representação com c-TF-IDF — estabelece um fluxo robusto para a modelagem de tópicos. No contexto deste trabalho, esse \textit{pipeline} constitui o núcleo do processo de análise das publicações científicas indexadas na plataforma SIMCC, servindo de ponto de partida para a integração com modelos de linguagem de grande escala, como o GPT-4, que será empregado para enriquecer semanticamente os rótulos dos tópicos e aprimorar sua interpretabilidade.

% \section{Modelos de Linguagem de Grande Escala (LLMs)}

% Os Modelos de Linguagem de Grande Escala (LLMs) constituem um marco no avanço do Processamento de Linguagem Natural (PLN), permitindo análises textuais sofisticadas e interpretações semânticas em volumes de dados sem precedentes. Fundamentados em arquiteturas baseadas em \textit{transformers}, como o BERT, GPT e suas variantes, esses modelos utilizam aprendizado profundo para construir representações contextuais dinâmicas de palavras e sentenças. Ao transformar o texto em \textit{embeddings} semânticos, capturam relações latentes complexas entre elementos linguísticos, servindo de alicerce para tarefas como sumarização automática, classificação de documentos e modelagem de tópicos (\citeonline{Meng_2024, Gana_2024}). 

% Enquanto LLMs podem ser definidos de forma geral como sistemas de PLN capazes de aprender distribuições linguísticas a partir de grandes corpora não anotados, o modo como tais modelos realizam o pré-treinamento e o ajuste fino (\textit{fine-tuning}) difere significativamente entre arquiteturas. As primeiras tentativas de modelos sequenciais, como Redes Neurais Recorrentes (RNNs) e Redes de Memória de Longo-Curto Prazo (LSTMs), apresentavam limitações na captura de dependências de longo alcance. Esse problema foi mitigado com a introdução do \textit{Transformer} por \citeonline{vaswani_2017}, cuja operação se baseia no Mecanismo de Atenção (\textit{self-attention}, permitindo atribuir diferentes pesos às palavras do contexto e, consequentemente, capturar relações semânticas globais de maneira mais eficiente.

% O treinamento de LLMs ocorre tipicamente em duas etapas complementares. Na fase de \textit{pré-treinamento}, emprega-se aprendizado não supervisionado para expor o modelo a trilhões de palavras em dados textuais como documentos e dados da internet, consolidando padrões gerais da linguagem. Dois paradigmas se destacam nesse processo: (i) a Modelagem de Linguagem Autorregressiva, como no GPT, onde o modelo aprende a prever o próximo token a partir de uma sequência de tokens anteriores; e (ii) a Modelagem de Linguagem Mascarada, como no BERT, em que lacunas são ocultadas e o modelo deve inferi-las a partir do contexto bidirecional (\citeonline{Devlin_2019, Jung_2024}). Em seguida, ocorre o \textit{fine-tuning}, etapa supervisionada em que o modelo é ajustado a tarefas específicas, como classificação de textos, análise de sentimentos ou sumarização, garantindo robustez e especialização (\cite{Gana_2024}).

% \begin{itemize}
%     \item \textbf{Modelagem de Linguagem Autorregressiva:} Modelos como o GPT (\textit{Generative Pre-trained Transformer}) seguem um fluxo sequencial unidirecional, prevendo cada token com base nos anteriores. Essa abordagem favorece a coerência narrativa e a fluidez na geração textual, aspectos essenciais em tarefas de criação de conteúdo (\citeonline{Radford_2018, Jung_2024}).
    
%     \item \textbf{Modelagem de Linguagem Mascarada:} Modelos como o BERT (\textit{Bidirectional Encoder Representations from Transformers}) aplicam mascaramento aleatório em tokens, forçando o modelo a interpretar o contexto bidirecionalmente. Tal característica possibilita uma maior sensibilidade semântica, útil em tarefas como inferência textual e modelagem de tópicos (\citeonline{Devlin_2019, Datchanamoorthy_2023}).
% \end{itemize}

% Entre os LLMs mais avançados, destaca-se o \textbf{GPT-4}, evolução do \textbf{GPT-1} desenvolvido pela OpenAI por \citeonline{Radford_2018}, que introduziu sua arquitetura a partir de um trabalho seminal. Sua estrutura permanece fundamentada no paradigma \textit{transformer} \citeonline{vaswani_2017}, mas incorpora modificações substanciais em relação às versões anteriores. Embora a documentação oficial seja limitada por razões proprietárias, o \textit{Technical Report} da OpenAI \cite{OpenAI_2023} e análises independentes \citeonline{Liu_2023, Achiam_2023} sugerem que o GPT-4 conta com bilhões de parâmetros adicionais em comparação ao GPT-3, além de maior profundidade de camadas de atenção e mecanismos otimizados de paralelização no treinamento distribuído. Essas melhorias resultam em avanços na capacidade de raciocínio semântico, na robustez diante de contextos ambíguos e na generalização para tarefas pouco definidas.

% Outro aspecto relevante é o aprimoramento nos métodos de alinhamento e segurança (\textit{alignment}), alcançados por meio de técnicas como o \textit{Reinforcement Learning with Human Feedback} (RLHF), que possibilitam ao modelo produzir respostas mais consistentes com critérios humanos de qualidade e relevância (\citeonline{OpenAI_2023, Ouyang_2022}). Além disso, o GPT-4 demonstra melhor desempenho em cenários multilíngues e em tarefas de alto nível cognitivo, como resolução de problemas em exames padronizados e síntese de conhecimento interdisciplinar (\citeonline{Achiam_2023}). Essas características tornam o modelo especialmente adequado para aplicações acadêmicas e científicas, onde a precisão semântica e a interpretabilidade das respostas são fundamentais.

% Ao compararmos o \textbf{BERTopic} e o GPT-4, evidenciam-se diferenças fundamentais na natureza e aplicação de cada modelo. O BERTopic, embora baseado em \textit{embeddings} derivados de modelos como BERT, concentra-se em identificar e organizar tópicos latentes a partir de representações vetoriais de documentos, utilizando algoritmos já citados nas seções anteriores. Seu ponto forte está na capacidade de estruturar grandes volumes de dados em \textit{clusters} semanticamente coerentes \citeonline{Grootendorst_2022}. Já o GPT-4, além de gerar representações contextuais sofisticadas, pode ser utilizado para atribuir rótulos semânticos refinados a tais \textit{clusters}, ampliando a interpretabilidade dos tópicos e permitindo a construção de narrativas explicativas sobre tendências detectadas nos dados \citeonline{Meng_2024, Galli_2024}.

% No contexto deste projeto, a integração de ambos os modelos se mostra justificada. Enquanto o BERTopic viabiliza a organização automática de grandes corpora textuais provenientes da plataforma SIMCC, o GPT-4 agrega valor na etapa de rotulagem, interpretação semântica e análise contextual aprofundada. Tal combinação potencializa tanto a acurácia quanto a inteligibilidade dos resultados, conciliando rigor metodológico com clareza interpretativa. Além disso, a aplicação conjunta favorece a detecção de padrões emergentes em múltiplos idiomas, aspecto essencial dada a heterogeneidade linguística base de dados.

% Portanto, ao invés de restringir-se a abordagens puramente estatísticas ou unicamente gerativas, este trabalho adota uma perspectiva híbrida, combinando técnicas de modelagem de tópicos e de raciocínio semântico avançado, buscando suprir lacunas de interpretabilidade.
