\section{A Evolução das Representações Vetoriais em PLN}

O progresso na área de \gls{pln} tem sido caracterizado pela investigação de representações vetoriais capazes de capturar não apenas a estrutura sintática, mas também os aspectos semânticos e contextuais dos textos. A evolução dessas representações partiu de abordagens estáticas para modelos dinâmicos baseados em contexto.

\subsection{Embeddings Estáticos: Limitações do Bag-of-Words}

As primeiras abordagens de sucesso, como o \textit{Word2Vec} proposto por \citeonline{Mikolov_2013}\footnote{O \textit{Word2Vec} (2013) foi seminal por introduzir duas arquiteturas eficientes, \textit{Skip-gram} e \textit{CBOW}, que aprendem vetores de palavras prevendo o contexto em que elas aparecem, baseando-se na hipótese distribucional.} e o \textit{GloVe} proposto por \citeonline{Pennington_2014}\footnote{O \textit{GloVe} (2014), ou \enquote{Global Vectors}, diferencia-se por combinar as estatísticas globais de coocorrência de palavras (como o LSA) com a modelagem baseada em janelas de contexto (como o \textit{Word2Vec}), capturando relações lineares entre palavras.}, consolidaram o conceito de \textit{embeddings}. Nesse caso, a operação algébrica subtrai o vetor de \enquote{Homem} do vetor de \enquote{Rei}, isolando o conceito de realeza, e adiciona o vetor de \enquote{Mulher}, resultando em uma representação vetorial espacialmente próxima à de \enquote{Rainha}. Isso demonstra que o modelo é capaz de codificar conceitos abstratos, como gênero, através da direção e distância entre os vetores. Estes consistem em vetores em espaços de alta dimensionalidade capazes de representar o significado aproximado de uma palavra.

A contribuição desses modelos foi permitir a quantificação do significado semântico. Em vez de tratar palavras como identificadores discretos (como em uma abordagem \textit{bag-of-words}), os \textit{embeddings} posicionam termos com significados similares próximos uns dos outros no espaço vetorial. Isso permite que relações semânticas sejam capturadas matematicamente, como no exemplo clássico \enquote{Rei - Homem + Mulher $\approx$ Rainha} \cite{Mikolov_2013}. \citeonline{Xie_2020} na literatura de \gls{pln} refere-se a este espaço vetorial como um \enquote{espaço semântico}.

O aprendizado desses vetores ocorre através do treinamento de redes neurais em tarefas de previsão de contexto, conforme ilustrado na Figura \ref{fig:cbow_skipgram}. O artigo seminal de \citeonline{Mikolov_2013} introduziram duas arquiteturas principais:

\begin{enumerate}
    \item \textbf{\gls{cbow}:} A arquitetura prevê a palavra atual (saída) com base em uma janela de palavras do contexto (entrada).
    \item \textbf{\textit{Skip-gram}:} A arquitetura inverte a lógica e usa a palavra atual (entrada) para prever as palavras do contexto (saída).
\end{enumerate}

É importante destacar que os \textit{embeddings} não são o produto final, mas sim um subproduto do treinamento: os vetores aprendidos na camada oculta da rede (\textit{PROJECTION} na figura) tornam-se a representação semântica da palavra, como indica \citeonline[p. 4]{Mikolov_2013}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/Cbow_skipgram.png} 
    \caption{Arquiteturas \gls{cbow} e \textit{Skip-gram}.}
    \label{fig:cbow_skipgram}
    \legend{Fonte: \citeonline[p. 5]{Mikolov_2013}}
\end{figure}

Apesar da utilidade em capturar similaridades lexicais, esses modelos apresentavam a limitação de atribuir um único vetor fixo a cada termo, independentemente do contexto de ocorrência. Por exemplo, a palavra \enquote{banco} teria a mesma representação vetorial em \enquote{banco financeiro} e \enquote{banco da praça}. Tal restrição, usualmente referida como o problema da ambiguidade do significado da palavra (\textit{ambiguity of word meaning}), compromete a precisão em tarefas que exigem desambiguação semântica.

\subsection{A Revolução dos Transformadores e o Mecanismo de Atenção}

Uma mudança significativa no paradigma ocorreu com a introdução do modelo de Transformadores (\textit{Transformers}), proposto por \citeonline{vaswani_2017} no artigo seminal \textit{Attention Is All You Need}\footnote{Este artigo é considerado um dos trabalhos mais influentes da \gls{pln} moderna. Sua principal contribuição foi propor uma arquitetura de rede neural que dispensa totalmente as camadas recorrentes (RNN) e convolucionais, baseando-se unicamente em mecanismos de atenção para modelar dependências globais entre a entrada e a saída \cite[p. 1]{vaswani_2017}.}. Essa arquitetura diferencia-se das \gls{rnn} e convolucionais, por fundamentar-se inteiramente no mecanismo de autoatenção (\textit{self-attention}).

Por meio da autoatenção, o modelo atribui pesos diferenciados a \textit{tokens} em uma sequência, permitindo processar simultaneamente e de forma bidirecional a totalidade do contexto textual. A arquitetura do Transformador, conforme apresentado na Figura \ref{fig:transformer}, segue uma estrutura de codificador-decodificador (\textit{encoder-decoder}). O lado esquerdo do diagrama representa o Codificador, enquanto o lado direito representa o Decodificador.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/Transformadores.png} 
    \caption{Arquitetura do modelo Transformador.}
    \label{fig:transformer}
    \legend{Fonte: \citeonline[p. 3]{vaswani_2017}} 
\end{figure}


O \textbf{Codificador} (\textit{Encoder}) é composto por uma pilha de N camadas idênticas (no artigo original, N=6). Cada camada, por sua vez, é composta por duas subcamadas principais: um mecanismo de autoatenção \textit{multi-head} (\textit{multi-head self-attention}) e uma rede neural \textit{feed-forward} (rede neural de alimentação direta) simples e totalmente conectada. Conexões residuais seguidas de normalização de camada (\textit{Add \& Norm}) são aplicadas ao redor de cada subcamada.

O \textbf{Decodificador} (\textit{Decoder}), de forma similar, é uma pilha de N camadas. Além das duas subcamadas presentes no codificador, o decodificador insere uma terceira subcamada, que realiza a atenção \textit{multi-head} sobre a saída da pilha do codificador. Crucialmente, a subcamada de autoatenção do decodificador é \enquote{mascarada} (\textit{Masked Multi-Head Attention}). Esse mascaramento é o que garante que a previsão para uma posição \textit{i} só possa depender das saídas conhecidas em posições anteriores a \textit{i}, preservando a propriedade autorregressiva do modelo.

Embora a arquitetura completa do Transformador tenha sido projetada para tarefas de transdução de sequência (como a tradução automática), foi a sua pilha de \textbf{Codificadores} (\textit{Encoder}) que se mostrou revolucionária para tarefas de \textit{compreensão} de linguagem. A capacidade do Codificador de processar texto de forma bidirecional e gerar representações numéricas ricas em contexto estabeleceu a base para uma nova classe de modelos focados exclusivamente na representação semântica, como será detalhado a seguir.

\subsection{Embeddings Contextuais: BERT e SBERT}
\label{sec:bert_sbert}

Sobre a base arquitetônica dos Transformadores, foram desenvolvidos os modelos pré-treinados, entre os quais se destaca o \gls{bert}, introduzido por \citeonline{Devlin_2019}. O \gls{bert} utiliza a arquitetura do Codificador (\textit{Encoder}) do Transformador para gerar representações de linguagem.

A inovação fundamental do \gls{bert} foi o pré-treinamento bidirecional, que diferentemente de abordagens anteriores, como o \gls{gpt} de \citeonline{Radford_2018}, que utilizava um treinamento unidirecional (da esquerda para a direita), o \gls{bert} foi projetado para \enquote{pré-treinar representações profundamente bidirecionais, condicionando conjuntamente o contexto esquerdo e direito em todas as camadas} como apontam \citeonline[p. 1, Traduzido]{Devlin_2019}.

Para alcançar essa bidirecionalidade sem que o modelo \enquote{visse a resposta}, \citeonline{Devlin_2019} introduziu o objetivo do \gls{mlm}\footnote{O \gls{mlm} é inspirado na tarefa \textit{Cloze} \cite{Taylor_1953}, onde o modelo deve prever palavras que foram omitidas (mascaradas) de uma sentença, usando o contexto de ambas as direções (esquerda e direita) para fazer a previsão \cite[p. 1]{Devlin_2019}.}. A Figura \ref{fig:bert_vs_gpt} ilustra a diferença fundamental entre as arquiteturas de pré-treinamento, mostrando como o \gls{bert} é capaz de processar informações de toda a sequência em todas as suas camadas.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/Bert_vs_gpt.png} 
    \caption{Diferenças nas arquiteturas de pré-treinamento. \gls{bert} é bidirecional, \gls{gpt} é unidirecional (da esquerda para a direita) e \gls{elmo}.}
    \label{fig:bert_vs_gpt}
    \legend{Fonte: \citeonline[p. 13]{Devlin_2019}}
\end{figure}

Apesar do desempenho em tarefas de classificação, a arquitetura do original do \gls{bert} apresentou limitações para tarefas de busca de similaridade semântica ou \textit{clustering}. Conforme observador por \citeonline{Reimers_2019}, o uso do \gls{bert} \enquote{requer que ambas as sentenças sejam alimentadas na rede, o que causa um overhead computacional massivo}. Uma busca de similaridade em 10.000 sentenças, por exemplo, exigiria cerca de 50 milhões de inferências (aproximadamente 65 horas), tornando-o inviável para grandes bases de dados. Além disso, estudos empíricos demonstraram que usar os \textit{embeddings} \enquote{crus} do \gls{bert} (seja pela média das saídas ou pelo vetor do \textit{token} `[CLS]`) produz resultados insatisfatórios, muitas vezes piores do que os \textit{embeddings} estáticos como o \textit{GloVe}.

Para solucionar essa questão, \citeonline{Reimers_2019} propuseram o \gls{sbert}. Ele modifica o \gls{bert} pré-treinado, adicionando uma operação de \textit{pooling} (sendo a média, \textit{MEAN-strategy}, a mais comum) à saída do \gls{bert} para criar um \textit{embedding} de sentença de tamanho fixo.

Crucialmente, o \gls{sbert} utiliza redes siamesas\footnote{Redes siamesas são uma arquitetura onde duas ou mais redes neurais idênticas (com pesos compartilhados) processam entradas diferentes de forma independente. Elas são otimizadas para aprender uma função de similaridade, aproximando os vetores de saída para entradas similares e afastando-os para entradas diferentes.} para fazer o \textit{fine-tuning} desses \textit{embeddings} de sentença. A Figura \ref{fig:sbert_arch} ilustra a arquitetura de inferência do \gls{sbert}, onde duas sentenças (A e B) são processadas por redes \gls{bert} idênticas (com pesos compartilhados), gerando vetores de sentença \textbf{u} e \textbf{v}. Esses vetores podem, então, ser comparados eficientemente usando uma medida de similaridade, como a similaridade de cosseno (\textit{cosine-similarity}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/Sbert_arch.png} 
    \caption{Arquitetura de inferência do \gls{sbert} para computar similaridade.}
    \label{fig:sbert_arch}
    \legend{Fonte: Adaptado de \citeonline[p. 3]{Reimers_2019}}
\end{figure}

\citeonline{Reimers_2019} demonstraram que essa abordagem reduz o custo computacional de encontrar o par mais similar em 10.000 sentenças de 65 horas (com \gls{bert}) para cerca de 5 segundos. Essa otimização para similaridade de sentenças permite o uso eficiente desses vetores em cenários multilíngues. A utilização de modelos pré-treinados em múltiplos idiomas (como o \textit{paraphrase-multilingual-MiniLM-L12-v2}\footnote{Disponível em: \url{https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}.}) torna-se particularmente relevante, visto que tais modelos produzem \textit{embeddings} semanticamente consistentes mesmo em diferentes idiomas.