\chapter[REFERENCIAL TEÓRICO]{REFERENCIAL TEÓRICO}
O referencial teórico deste estudo abordará diversos aspectos cruciais relacionados à Ciência da Informação, Análise de Publicações Científicas, Processamento de Linguagem Natural (PLN), Modelagem de Tópicos, e Modelos de Linguagem de Grande Escala (LLMs). 
\section{Ciência da Informação e Análise de Publicações Científicas}
A produção científica global tem experimentado um crescimento exponencial nas últimas décadas, impulsionada pelo aumento de colaborações interdisciplinares, avanços tecnológicos e facilidades na disseminação de conhecimento. Esse crescimento resulta em um volume significativo de artigos acadêmicos, dificultando o acesso, a organização e a análise das informações disponíveis. \cite{Galli} destacam que o número de publicações acadêmicas dobrou em muitos campos nos últimos 15 anos, com uma média global de aproximadamente 2,5 milhões de artigos publicados anualmente \cite{Kim}. Esse cenário, enquanto promissor para o avanço do conhecimento, gera desafios relacionados à busca, seleção e categorização de informações relevantes em meio a um vasto conjunto de dados.

Esse crescimento exponencial da produção científica global tem sido acompanhado por um aumento na diversidade linguística das publicações. Embora o inglês seja amplamente considerado o idioma padrão da ciência, muitas contribuições relevantes são publicadas em outros idiomas, como português, espanhol e chinês, representando uma fração significativa da produção acadêmica em suas respectivas regiões. \cite{Xie} destacam que essa diversidade linguística, embora enriquecedora, cria barreiras significativas para pesquisadores que dependem de fontes em múltiplos idiomas, dificultando o acesso e a citação de trabalhos importantes.

\begin{citacao}
A maioria dos estudos até agora sobre análise de tópicos tem sido baseada em publicações em inglês e tem dependido fortemente da análise de evolução de tópicos baseada em citações. \cite[Traduzido]{Xie}.
\end{citacao}

\cite{Xie} ressaltam que metodologias baseadas em citações não são adequadas para analisar relações de tópicos de pesquisa multilíngues, entre os principais desafios relacionados a essa diversidade estão a dificuldade de indexação adequada de publicações em idiomas não ingleses, a limitada visibilidade em bases de dados globais e os problemas semânticos causados por traduções automáticas imprecisas. Sistemas de busca tradicionais, como Scopus e Web of Science, frequentemente priorizam publicações em inglês, o que leva à sub-representação de artigos em outras línguas. Além disso, os modelos de mineração de dados e categorização padrão falham em alinhar termos e conceitos entre diferentes idiomas, resultando em tópicos fragmentados e menos informativos.

Essa problemática também impacta diretamente a citação de trabalhos em idiomas não ingleses. Muitos artigos científicos publicados em português ou chinês, por exemplo, permanecem invisíveis em revisões sistemáticas ou análises de tendências globais, reduzindo sua relevância no cenário acadêmico internacional. Segundo \cite{Xie}, essa limitação é agravada pela falta de ferramentas que possam lidar eficazmente com a integração semântica de publicações multilíngues, dificultando a descoberta de conexões entre ideias ou estudos relacionados.

Nesse contexto, o processamento de linguagem natural (PLN) surge como uma solução promissora para superar essas barreiras. Técnicas modernas de PLN, como os modelos baseados em transformadores (BERT, OpenAI GPT), são capazes de lidar com textos em diferentes idiomas e alinhar semanticamente informações de fontes diversas (Weng et al., 2022). Por exemplo, \cite{Dillan} e \cite{Xie} demonstram que ferramentas avançadas de PLN permitem não apenas traduzir termos, mas também preservar contextos e significados em diferentes línguas, melhorando a categorização e a análise de publicações multilíngues.

\begin{citacao}
Um método recente popular é a extração de temas acadêmicos por meio de modelagem de tópicos adequada dentro de publicações limitadas ao longo de vários anos.\cite[Traduzido]{Xie}.
\end{citacao}

Os benefícios do PLN incluem a capacidade de lidar com dados não estruturados, identificar padrões semânticos complexos e adaptar-se a diferentes contextos, viabilizando a identificação de padrões temáticos comuns, independentemente da língua de origem. Em comparação com métodos tradicionais de busca e tradução, o PLN oferece maior precisão e relevância, especialmente ao lidar com publicações em idiomas como português e chinês, que frequentemente são negligenciados em bases de dados globais \cite{Glazkova}.

\cite{Xie} mostram que a integração de modelos como o OpenAI GPT-4 e o BERT possibilita a criação de descrições temáticas enriquecidas, facilitando a compreensão e a tomada de decisões. Assim, ao promover a inclusão de publicações em diferentes idiomas no ecossistema acadêmico global, o PLN contribui para uma ciência mais equitativa e diversificada, superando desafios relacionados à citação e ao reconhecimento de trabalhos científicos além do inglês. 

\section{Transformadores e Embeddings no Contexto do PLN}
O processamento de linguagem natural (PLN) tem evoluído significativamente com o uso de técnicas como embeddings e transformadores. A compreensão da linguagem natural abrange uma ampla gama de tarefas diversas, como implicação textual, resposta a perguntas, avaliação de similaridade semântica e classificação de documentos \cite{Radford}. Essas inovações são fundamentais para capturar nuances semânticas e contextuais, superando limitações de abordagens estatísticas tradicionais (\cite{Kim}.

\begin{citacao}
    Um componente essencial para alcançar a compreensão semântica são os embeddings — representações numéricas que codificam o significado de palavras ou mesmo frases — que são essenciais na PNL para capturar relacionamentos complexos entre palavras e frases usando arquiteturas especiais conhecidas como transformadores.\cite[Traduzido]{Galli}. 
\end{citacao}

Esses vetores preservam relações semânticas e contextuais entre os elementos textuais. Diferentemente de métodos anteriores, como a bag-of-words, que apenas contabilizam a frequência de palavras sem capturar seu significado, os embeddings são capazes de refletir a semântica latente das palavras, permitindo uma análise mais rica e contextualizada.

Modelos como Word2Vec \cite{Mikolov} e GloVe \cite{Pennington} foram pioneiros no desenvolvimento de embeddings distribuídos, mostrando que palavras usadas em contextos semelhantes tendem a ter representações vetoriais próximas. No entanto, esses métodos apresentam limitações, como a incapacidade de capturar o contexto dinâmico em que uma palavra ocorre \cite{Devlin}. Por exemplo, a palavra "banco", pode referir-se a uma instituição financeira ou a um assento, dependendo do contexto. Para superar essas limitações, surgiram modelos baseados em transformadores, como o BERT, um modelo de processamento de linguagem natural proposto por pesquisadores do Google \cite{George}, que utilizam embeddings contextuais para adaptar o significado das palavras ao seu entorno \cite{Devlin}.

\begin{citacao}
    Ao contrário dos modelos recentes de representação de linguagem, o BERT foi projetado para pré-treinar representações bidirecionais profundas de texto não rotulado, condicionando conjuntamente o contexto esquerdo e direito em todas as camadas.\cite[Traduzido]{Devlin}. 
\end{citacao}

Em complemento, os transformadores surgem como arquiteturas de rede neural projetadas para processar sequências de texto, modelando relações de longo alcance entre palavras. Introduzidos por \cite{vaswani_2017} no trabalho "Attention is All You Need", os transformadores revolucionaram o PLN ao introduzir o mecanismo de self-attention. Esse mecanismo permite que o modelo avalie a relevância de cada palavra em uma sequência em relação às demais, capturando dependências contextuais de maneira eficiente.

Diferentemente de arquiteturas anteriores, como redes recorrentes (RNNs), os transformadores processam o texto em paralelo, o que aumenta a eficiência computacional e permite lidar com grandes volumes de dados \cite{Weng}. Exemplos de transformadores bem-sucedidos incluem o BERT (Bidirectional Encoder Representations from Transformers), que analisa o contexto de palavras à esquerda e à direita, e o OpenAI GPT (Generative Pre-trained Transformer), especializado na geração de texto de forma contextualizada.
\begin{figure}[h]
	\caption{\label{transformers-schema}Transformador - modelo arquitetural.}
	\begin{center}
	    \includegraphics[scale=0.8]{figs/image.png}
	\end{center}
	\legend{Fonte: \citeonline[Traduzido, p. 24]{vaswani_2017}}
\end{figure}


A combinação de embeddings e transformadores criou um novo paradigma no PLN. Enquanto os embeddings fornecem representações ricas de palavras, os transformadores utilizam essas representações para construir relações contextuais mais precisas. Modelos como o BERT e o OpenAI GPT-4 têm se mostrado eficazes na análise de textos científicos, melhorando a categorização, a identificação de tópicos e a integração de informações em diferentes idiomas e disciplinas \cite{Xie, Kim}.


\section{Abordagens Tradicionais de Modelagem de Tópicos}

Com o aumento exponencial de dados textuais disponíveis e a necessidade de organizar informações em escala, a modelagem de tópicos emergiu como uma ferramenta essencial para categorizar e identificar padrões temáticos. 
\begin{citacao}
    A modelagem de tópicos, um modelo estatístico empregado em PNL, visa identificar temas principais, conhecidos como tópicos, dentro de coleções de documentos. Em outras palavras, é uma técnica de mineração de texto que é utilizada para descobrir estruturas semânticas ocultas dentro de dados textuais e encontra aplicações em diversos campos.\cite[Traduzido]{Jung}. 
\end{citacao}


Entre as abordagens iniciais que marcaram o desenvolvimento desse campo estão a Latent Dirichlet Allocation (LDA) e a Probabilistic Latent Semantic Analysis (PLSA). Essas técnicas tradicionais desempenharam um papel fundamental na construção das bases para análises automatizadas de textos. No entanto, à medida que os desafios linguísticos e semânticos se tornaram mais complexos, suas limitações começaram a surgir, abrindo espaço para métodos mais avançados, como transformadores \cite{vaswani_2017, Devlin, Radford} e embeddings \cite{Mikolov, Pennington, Bengio}

A LDA, proposta por \cite{Blei}, é um modelo generativo probabilístico que parte do pressuposto de que cada documento é composto por uma combinação de temas, e cada tema, por uma distribuição específica de palavras. Aplicando o método de non-negative matrix factorization (NMF), uma matriz é dividida em duas matrizes não negativas. Cada documento é representado no NMF como uma combinação linear de vetores de tópicos para a modelagem de tópicos \cite{Datchanamoorthy_2023}. Estudos como os de \cite{Jung} mostram como o LDA ainda é amplamente utilizado para tarefas como análises de tratamento de doenças \cite{Mifrah} e eficiência energética \cite{Polyzos}, graças à sua flexibilidade em diferentes contextos e à simplicidade de sua implementação. No entanto, como observam \cite{George}, a LDA depende de pressupostos como a independência das palavras dentro dos tópicos, o que frequentemente resulta em análises superficiais, especialmente em textos multilíngues ou com vocabulários técnicos complexos. 

\begin{citacao}
    A sensibilidade do LDA ao parâmetro do número de temas (K) é uma de suas desvantagens. Encontrar o valor ideal para K pode ser desafiador. O modelo pode simplificar excessivamente e combinar diferentes temas em um só se K for configurado muito baixo. No entanto, se K for configurado muito alto, o modelo pode se tornar muito complexo e produzir temas errôneos.\cite[Traduzido]{Datchanamoorthy_2023}.
\end{citacao}

Por outro lado, a PLSA, introduzida por \cite{Deerwester}, utiliza técnicas de álgebra linear para mapear palavras e documentos em um espaço vetorial reduzido por meio da decomposição de valores singulares (SVD). Este é um modelo que gera ideias onde cada tema é assumido como uma distribuição probabilística sobre palavras e cada texto como uma mistura de temas. O PLSA é semelhante ao LDA, exceto que não faz a suposição de um prior de Dirichlet na distribuição de tópicos \cite{Datchanamoorthy_2023}. A LSA tem contribuído para simplificar representações textuais e eliminar ruídos em análises iniciais, sendo aplicada em grandes corpora de dados (George, 2023). Contudo, suas limitações são evidentes em cenários onde relações semânticas complexas são cruciais. \cite{Datchanamoorthy_2023} e  \cite{George} apontam que a linearidade da LSA e sua sensibilidade ao ruído dificultam a análise de textos extensos e diversificados, frequentemente distorcendo resultados temáticos.

\begin{citacao}
    Devido a limitações de processamento, gerenciar conjuntos de dados muito grandes é frequentemente difícil. Além disso, a ordem das palavras não é considerada pela LSA, o que pode fazer com que o contexto e a sutileza sejam perdidos no texto. Ela considera cada palavra isoladamente, uma simplificação que pode não refletir com precisão as nuances da linguagem genuína.\cite[Traduzido]{Datchanamoorthy_2023}.
\end{citacao}

Comparadas às técnicas modernas, como as baseadas em transformadores e embeddings, tanto a LDA quanto a LSA revelam várias deficiências. Khavita ressalta que tanto o LDA quanto a LSA têm o problema do “bag of words”, em que falham em levar em conta as informações estruturais e sequenciais dos textos. Modelos como o BERT superam a falta de contexto semântico dessas abordagens ao considerar o significado das palavras em todo o seu contexto textual \cite{George}. Além disso, técnicas tradicionais não conseguem lidar eficientemente com textos multilíngues e interdisciplinares, limitando sua aplicabilidade em corpora diversificados \cite{Xie}. Como demonstrado por \cite{Datchanamoorthy_2023}, o BERT captura relações semânticas complexas entre palavras, considerando seu contexto em uma frase. Como resultado, as descobertas de modelagem de tópicos são mais precisas e sensíveis ao contexto, permitindo ao modelo distinguir variações sutis no contexto e significado ao longo de palavras e frases.

Outro ponto crítico é a incapacidade dessas técnicas de capturar relações profundas e não lineares entre palavras e tópicos. Enquanto a simplicidade estatística do LDA e a linearidade do LSA limitam a profundidade analítica, métodos como o BERT e o GPT-4 utilizam embeddings para modelar dependências complexas, resultando em análises temáticas mais detalhadas e relevantes \cite{Datchanamoorthy_2023}. Além disso, a escalabilidade limitada dessas técnicas torna-as menos eficazes em ambientes com grandes volumes de dados não estruturados, enquanto abordagens modernas conseguem manter a qualidade da análise mesmo em corpora científicos globais \cite{Jung}.

Embora o LDA e a LSA tenham desempenhado papéis importantes no avanço inicial da modelagem de tópicos, suas limitações em termos de semântica, escalabilidade e contexto tornam necessário o uso de métodos mais avançados, como o BERT. Esses novos paradigmas oferecem soluções mais robustas e precisas, alinhadas às demandas contemporâneas de análise textual.

\section{BERTopic: Uma Abordagem Moderna}

Com o aumento da complexidade dos textos acadêmicos e o crescimento exponencial dos dados textuais disponíveis, é virtualmente impossível para os pesquisadores revisarem manualmente cada manuscrito \cite{Likhareva}. Abordagens de texto Multi-label é uma técnica popular que oferece amplas aplicações práticas, particularmente em recomendação de tags, análise de sentimentos e recuperação de informações. Nesse contexto, o BERT \cite{Devlin} se destaca como uma solução moderna que combina embeddings contextuais com técnicas de agrupamento e redução de dimensionalidade, promovendo uma análise mais detalhada e precisa dos dados textuais.

O BERT (Bidirectional Encoder Representations from Transformers) foi introduzido por \cite{Devlin} como um marco no Processamento de Linguagem Natural (PLN). Baseado na arquitetura de transformadores, o BERT utiliza um mecanismo de atenção bidirecional que permite compreender palavras no contexto de toda a frase, ao contrário de abordagens unidirecionais, como LSA, LDA e OpenAI GPT, que analisam o texto de maneira sequencial \cite{Datchanamoorthy_2023, Devlin}.

No BERT, embeddings contextuais são gerados para palavras e frases, capturando nuances semânticas específicas que dependem do contexto. Esses embeddings são calculados por meio de técnicas como c-TF-IDF (Class-based Term Frequency-Inverse Document Frequency), que ajusta os pesos das palavras com base em suas frequências e relevâncias dentro de um corpus \cite{George, Devlin}. Esse modelo demonstrou ser altamente eficiente em tarefas de análise semântica, como categorização e agrupamento de tópicos.

Nesse contexto \cite{Grootendorst} sugere uma nova abordagem do BERT, o BERTopic, que amplia o uso do BERT ao integrar embeddings contextuais chamadas de Sentence Transformers em um pipeline que inclui redução de dimensionalidade com UMAP (Uniform Manifold Approximation and Projection), agrupamentos com HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) e representações de tópicos (Topic Tokenizing) com CountVectorizer. Essa combinação permite que o BERTopic identifique tópicos de maneira dinâmica e precisa, mesmo em grandes volumes de dados textuais diversificados \cite{George, Jung}.

O BERTopic utiliza embeddings gerados pelo Sentence-BERT (S-BERT), uma variante do BERT projetada especificamente para capturar relações semânticas entre sentenças e documentos, permitindo a comparação direta de textos com alta precisão. Introduzido por \cite{Reimers}, o S-BERT modifica a arquitetura original do BERT ao incorporar uma camada de rede neural adicional para gerar embeddings otimizados para tarefas como classificação e agrupamento de textos. Esses embeddings são vetores densos que encapsulam as relações semânticas contextuais de forma mais compacta e eficaz, sendo ideais para tarefas de modelagem de tópicos, como destacam \cite{George} e \cite{Jung}.

Uma vez gerados os embeddings pelo S-BERT, o BERTopic aplica o algoritmo c-TF-IDF (Class-based Term Frequency-Inverse Document Frequency) para identificar as palavras mais representativas de cada cluster de tópicos. Como destacado por \cite{Gana, Datchanamoorthy_2023, Jung} o c-TF-IDF é uma extensão do tradicional TF-IDF (Term Frequency-Inverse Document Frequency), que ajusta a frequência dos termos não apenas com base em sua ocorrência no corpus geral, mas também em relação à distribuição de termos dentro de cada cluster temático identificado pelo modelo. Nesse método, cada cluster é tratado como um único documento, concatenando todos os documentos dentro desse cluster.

\begin{citacao}
   Ao alavancar o c-TF-IDF, o BERTopic facilita uma compreensão mais profunda e precisa da estrutura temática em grandes conjuntos de dados de texto. Essa abordagem não apenas melhora a qualidade e relevância dos tópicos identificados, mas também aprimora a interpretabilidade dos resultados, facilitando a compreensão dos padrões subjacentes nos dados.\cite[Traduzido]{Gana}.
\end{citacao}


Após o processo de Transformação de Sentenças (Embeddings) a redução de dimensionalidade é uma etapa essencial no BERTopic, pois simplifica embeddings de alta dimensionalidade, permitindo agrupamentos mais eficientes e análises detalhadas. Entre as diversas técnicas disponíveis, o BERTopic utiliza o Uniform Manifold Approximation and Projection (UMAP) como padrão devido à sua eficácia em preservar tanto as estruturas locais quanto globais dos dados durante a transformação para um espaço de menor dimensão \cite{Grootendorst}. Essa característica é essencial em modelagem de tópicos, onde a preservação de nuances semânticas é crucial para a formação de clusters temáticos precisos.

O UMAP funciona por meio da construção de um grafo de alta dimensionalidade que representa os dados originais. Ele então otimiza esse grafo para um espaço de menor dimensão, mantendo a similaridade estrutural entre os pontos de dados. Em termos práticos, o UMAP preserva:

\begin{itemize}
    \item \textbf{Estruturas Locais:} Relações entre pontos vizinhos, indicando proximidade semântica ou similaridade direta. \cite{Meng}

    \item \textbf{Estruturas Globais:} Conexões entre diferentes clusters, o que permite uma visão integrada e coesa dos dados em um espaço reduzido. \cite{Meng, Jung}
\end{itemize}

Diferentemente de outras técnicas, como o t-SNE \cite{Maaten}, o UMAP se destaca pela sua eficiência computacional e capacidade de escalabilidade. O t-SNE requer normalizações globais complexas e o uso de árvores espaciais, que apresentam dificuldades ao lidar com dados de alta dimensionalidade. Por outro lado, o UMAP evita esses gargalos ao adotar um método de otimização mais simples e rápido, tornando-o adequado para grandes corpora textuais e aplicações em tempo real \cite{Datchanamoorthy_2023}. Além disso, ele é flexível o suficiente para ser ajustado a diferentes tipos de dados e tarefas, oferecendo uma adaptabilidade superior em comparação com técnicas como LargeVis \cite{Weng}.

No contexto do BERTopic, o UMAP desempenha um papel essencial ao converter embeddings gerados pelo S-BERT em representações que podem ser analisadas e agrupadas com eficiência. Essa conversão facilita o trabalho do HDBSCAN na formação de clusters temáticos, garantindo que tanto a granularidade local quanto a coesão global sejam mantidas. Estudos como os de \cite{George, Jung, Meng} mostram que essa combinação de UMAP com algoritmos de agrupamento robustos resulta em tópicos mais coesos e representativos, especialmente em aplicações que envolvem grandes volumes de dados textuais acadêmicos.

O Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) \cite{Campello} complementa perfeitamente o UMAP no processo de modelagem de tópicos dentro do BERTopic. Este algoritmo é projetado para identificar clusters em dados de densidade variável, o que é comum em corpora textuais complexos, como publicações científicas. Diferentemente de métodos tradicionais como o k-means, que requerem a especificação prévia do número de clusters e assumem formas geométricas fixas, o HDBSCAN utiliza uma abordagem hierárquica e adaptativa que permite detectar padrões de agrupamento de maneira mais natural e precisa.

O funcionamento do HDBSCAN começa com a construção de uma árvore de alcance mínimo (minimum spanning tree) baseada em distâncias entre os pontos de dados. Em seguida, a árvore é processada por um método de agrupamento condensado, no qual as arestas de baixa densidade, representando ruídos, são removidas. Isso deixa uma estrutura que destaca os clusters de alta densidade, enquanto pontos fora desses agrupamentos são classificados como outliers. Essa capacidade de lidar explicitamente com ruídos é uma vantagem significativa em tarefas de modelagem de tópicos, pois evita que dados irrelevantes comprometam a qualidade dos clusters temáticos gerados \cite{Gana}.

Outro diferencial do HDBSCAN é sua habilidade de realizar um agrupamento suave (soft clustering), atribuindo a cada ponto uma probabilidade de pertencer a diferentes clusters. Essa abordagem é particularmente útil em contextos textuais, onde documentos podem abordar múltiplos tópicos de forma sobreposta. Além disso, o HDBSCAN ajusta automaticamente o número de clusters com base nos dados, eliminando a necessidade de parametrização manual, algo que pode ser desafiador em corpora complexos como os acadêmicos \cite{Gana}.

A eficiência do HDBSCAN, especialmente quando combinado com o UMAP, é amplamente reconhecida. Estudos como os de \cite{Jung, Gana, Meng} demonstram que essa combinação permite a formação de tópicos representativos mesmo em grandes conjuntos de dados. Aplicações práticas incluem a análise de textos acadêmicos, a identificação de argumentos em dados reais e a modelagem de tópicos em conteúdos diversificados, como redes sociais ou publicações multilíngues. Ao garantir a manutenção da granularidade local e das conexões globais, o HDBSCAN possibilita uma análise temática mais profunda e precisa, alinhando-se às demandas contemporâneas de Processamento de Linguagem Natural.

\begin{figure}[h]
    \caption{\label{bertopic-eschema}Diagrama esquemático detalhado da comparação de métricas de avaliação entre modelos.}
    \begin{center}
        \includegraphics[scale=0.8]{figs/Bertopic.png}
    \end{center}
    \legend{Fonte: \citeonline[Traduzido, p.7]{Jung}}
\end{figure}

Assim, no framework do BERTopic, o HDBSCAN não apenas complementa a redução de dimensionalidade proporcionada pelo UMAP, mas também aprimora significativamente a qualidade e a coerência dos clusters gerados. Essa integração estabelece um padrão robusto para a modelagem de tópicos, destacando-se como uma solução ideal para explorar grandes volumes de dados textuais acadêmicos.

\section{Modelos de Linguagem de Grande Escala (LLMs)}

Os Modelos de Linguagem de Grande Escala (LLMs) representam um avanço significativo no campo do Processamento de Linguagem Natural (PLN), permitindo análises textuais sofisticadas e interpretações semânticas em um volume de dados sem precedentes. Baseados em arquiteturas de transformadores , como o BERT, GPT, e outros, esses modelos utilizam aprendizado profundo para gerar representações contextuais e dinâmicas de palavras e frases. Essa abordagem transforma o texto em embeddings semânticos que capturam relações complexas entre elementos linguísticos, proporcionando uma base sólida para uma ampla gama de tarefas, incluindo resumo automático, classificação de textos e modelagem de tópicos \cite{Meng, Gana}.

Modelos de Linguagem de Grande Escala (LLMs) são sistemas avançados de processamento de linguagem natural (NLP) que utilizam redes neurais profundas para compreender e gerar texto de maneira semelhante ao humano. Esses modelos são treinados em vastas quantidades de dados textuais não anotados, permitindo-lhes adquirir um amplo conhecimento linguístico e contextual.

Os LLMs geralmente são baseados em arquiteturas de redes neurais, como as Redes Neurais Recorrentes (RNNs), Redes de Memória de Curto-Longo Prazo (LSTMs), e mais recentemente, os modelos Transformer. O Transformer, como já explicado nas seções anteriores, foi introduzido por \cite{vaswani_2017}, é uma arquitetura que utiliza mecanismos de atenção para capturar dependências contextuais em diferentes partes do texto, independentemente da distância entre elas. Nesse contexto, um dos algoritmos mais utilizados em LLMs é o Mecanismo de Atenção, como descrito em "Attention Is All You Need" por vaswani_2017, que permite que o modelo dê pesos diferentes às palavras do contexto, dependendo de sua relevância para a tarefa em questão. Isso é particularmente útil para tarefas como tradução automática e resumo de texto, onde a compreensão do contexto é crucial.

O desenvolvimento dos LLMs ocorre em duas etapas principais. A primeira é o pré-treinamento, que utiliza aprendizado não supervisionado para expor o modelo a grandes corpora de texto, sem a necessidade de anotações manuais. Entre essas tarefas, destacam-se a Modelagem de Linguagem Autorregressiva, como exemplificado no GPT (Generative Pre-trained Transformer), e a Modelagem de Linguagem Mascarada, aplicada no BERT (Bidirectional Encoder Representations from Transformers). Essa etapa estabelece uma base linguística ao identificar padrões gerais da linguagem. A segunda etapa, conhecida como ajuste fino (fine-tuning), é um processo supervisionado no qual o modelo é refinado para atender a tarefas específicas, como análise de sentimentos, classificação textual e sumarização. Essa combinação de pré-treinamento e ajuste fino é fundamental para que os modelos ofereçam uma performance robusta em diferentes aplicações \cite{Gana, Devlin, vaswani_2017, Jung}.

\begin{itemize}
    \item \textbf{Modelagem de Linguagem Autorregressiva:} Modelos autorregressivos, como o GPT, geram texto de forma sequencial, prevendo cada palavra com base nas palavras anteriores na sequência. Essa abordagem linear é eficiente para tarefas de geração de texto, garantindo coerência contextual e continuidade semântica \cite{Jung}.

    \item \textbf{Modelagem de Linguagem Mascarada:} Por outro lado, modelos como o BERT utilizam a técnica de modelagem de linguagem mascarada, em que lacunas em frases são preenchidas durante o treinamento. Isso permite que o modelo compreenda o contexto bidirecionalmente, considerando tanto as palavras anteriores quanto as posteriores para interpretar as relações semânticas entre os elementos textuais \cite{Datchanamoorthy_2023, Devlin}.
\end{itemize}

O OpenAI GPT (Generative Pre-trained Transformer) \cite{vaswani_2017}, especialmente em suas iterações mais avançadas como o GPT-4, é um exemplo notável de LLM que destaca sua eficácia na análise semântica e na rotulagem automática de tópicos. Projetado pela OpenAI \cite{Radford}, o GPT adota uma arquitetura baseada em transformadores, mas sua abordagem de pré-treinamento e geração é unidirecional, processando texto da esquerda para a direita, o que é particularmente eficaz na geração de texto coerente e contextualizado. Por outro lado, modelos como o BERT analisam o contexto de maneira bidirecional, considerando simultaneamente as palavras à esquerda e à direita \cite{Devlin}. Essa abordagem permite ao BERT capturar nuances semânticas mais complexas, tornando-o especialmente eficiente em tarefas como preenchimento de lacunas em textos e modelagem de tópicos \cite{Devlin, Weng, Gana}.

No contexto de algoritmos como o BERTopic, que utiliza embeddings baseados em transformers para identificar e agrupar tópicos em textos, os LLMs desempenham um papel crucial. O BERTopic processa documentos transformando-os em representações vetoriais, reduzindo sua dimensionalidade e agrupando-os por meio de algoritmos de clustering como o HDBSCAN. Posteriormente, LLMs como o GPT podem ser utilizados para gerar rótulos semânticos refinados para esses clusters, representando com maior precisão os temas subjacentes. Essa integração não apenas facilita a análise de grandes corpora, mas também aumenta a interpretabilidade dos tópicos identificados, como evidenciado em aplicações na triagem de literatura científica \cite{Galli, Gana, Meng, Jung}.

O impacto dos LLMs é amplamente reconhecido em áreas como a pesquisa biomédica \cite{Gana} onde algoritmos como o BERTopic têm sido utilizados para identificar tendências emergentes em estudos de regeneração óssea e outras áreas médicas. Essas ferramentas representam não apenas um avanço tecnológico, mas também uma transformação na forma como exploramos e compreendemos o vasto volume de conhecimento disponível na era digital.