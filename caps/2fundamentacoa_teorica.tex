\chapter[REFERENCIAL TEÓRICO]{REFERENCIAL TEÓRICO}
O referencial teórico deste estudo abordará diversos aspectos cruciais relacionados à Ciência da Informação, Análise de Publicações Científicas, Processamento de Linguagem Natural (PLN), Modelagem de Tópicos, e Modelos de Linguagem de Grande Escala (LLMs). 

\section{Ciência da Informação e Análise de Publicações Científicas}

A explosão da produção científica global nas últimas décadas, impulsionada pela maior acessibilidade à tecnologia e pela colaboração interdisciplinar, delineia um cenário desafiador para a área da Ciência da Informação. Como destacam \citeonline{Kim_2024}, o volume crescente de publicações dificulta a atualização contínua de pesquisadores e a identificação de áreas emergentes do conhecimento. Nesse contexto, estratégias tradicionais de busca baseadas em palavras-chave mostram-se limitadas, uma vez que desconsideram a complexidade semântica do léxico científico. Esse fator resulta não apenas na omissão de trabalhos relevantes, mas também na dificuldade de mapear de forma consistente o progresso em determinados campos.

Um aspecto que amplia essa complexidade é a diversidade linguística no ambiente científico. Segundo \citeonline{Xie_2020}, embora o inglês desempenhe papel predominante na comunicação acadêmica, uma parcela significativa da produção ocorre em outros idiomas. Os autores argumentam que metodologias convencionais baseadas em citações revelam-se insuficientes para a análise multilíngue, visto que publicações em inglês raramente fazem referência a pesquisas em outras línguas. Essa limitação restringe a circulação global do conhecimento, reduzindo a visibilidade e o impacto de estudos relevantes. Plataformas de indexação consolidadas, como Scopus e Web of Science, tendem a privilegiar artigos publicados em inglês, contribuindo para a sub-representação de pesquisas em outros idiomas. Além disso, abordagens tradicionais de mineração de dados e categorização apresentam dificuldades em alinhar conceitos e terminologias em diferentes línguas, o que frequentemente resulta em tópicos fragmentados e de menor valor analítico.

\begin{citacao}
A maioria dos estudos até agora sobre análise de tópicos tem sido baseada em publicações em inglês e tem dependido fortemente da análise de evolução de tópicos baseada em citações (\citeonline[Traduzido]{Xie_2020}).
\end{citacao}

Diante desse cenário, técnicas contemporâneas de \textit{Topic Modeling}, em especial aquelas fundamentadas em \textit{embeddings}, têm sido investigadas como alternativas promissoras. De acordo com \citeonline{Galli_2024}, a utilização de representações densas derivadas de modelos como o \gls{BERT} potencializa a análise de grandes volumes textuais, permitindo capturar aspectos semânticos que vão além da simples coincidência lexical. Essa capacidade favorece a identificação de padrões temáticos em documentos que não compartilham necessariamente o mesmo vocabulário. Nesse sentido, métodos como o \textit{BERTopic}, que constituem a primeira etapa do pipeline proposto neste trabalho, oferecem uma estrutura metodológica adequada para a extração de tópicos a partir de representações vetoriais densas dos textos científicos heterogêneos.

A aplicação dessas ferramentas em plataformas como o SIMCC, que contêm publicações em diversos idiomas — com destaque para o Português —, torna-se particularmente relevante. O pipeline delineado nesta pesquisa propõe uma abordagem híbrida que busca não apenas organizar o conhecimento de maneira mais sistemática, mas também contribuir para uma análise mais equitativa da produção científica, valorizando trabalhos independentemente do idioma em que foram originalmente publicados.

\section{Transformadores e Embeddings no Contexto do PLN}

O avanço no campo do \gls{PLN} tem sido marcado pela busca por representações vetoriais que capturem não apenas informações sintáticas, mas também aspectos semânticos e contextuais dos textos. As primeiras abordagens, como o \textit{Word2Vec} de \citeonline{Mikolov_2013} e o \textit{GloVe} de \citeonline{Pennington_2014}, consolidaram a noção de \textit{embeddings}, isto é, vetores em espaços de alta dimensionalidade capazes de representar o significado aproximado de uma palavra. Esses modelos, embora inovadores em seu período, apresentavam a limitação de atribuir um único vetor fixo a cada termo, independentemente do contexto de ocorrência. Por exemplo, a palavra ``banco'' pode referir-se a uma instituição financeira ou a um assento, dependendo do contexto. Tal restrição, usualmente referida como o problema da \textit{ambiguity of word meaning}, compromete a precisão em tarefas que exigem desambiguação semântica.

\begin{citacao}
Um componente essencial para alcançar a compreensão semântica são os embeddings — representações numéricas que codificam o significado de palavras ou mesmo frases — que são essenciais na PLN para capturar relacionamentos complexos entre palavras e frases usando arquiteturas especiais conhecidas como transformadores (\citeonline[Tradução nossa]{Galli_2024}).
\end{citacao}

A verdadeira virada de paradigma ocorreu com a introdução do modelo \textit{Transformer}, proposto por \citeonline{vaswani_2017} no artigo seminal \textit{Attention Is All You Need}. Essa arquitetura rompeu com o paradigma das arquiteturas de redes recorrentes (RNNs) e convolucionais, fundamentando-se inteiramente no mecanismo de atenção (\textit{attention mechanism}), permitindo que o modelo ponderasse a importância de diferentes palavras em uma sequência. Através dele, o modelo atribui pesos diferenciados a tokens em uma sequência, permitindo processar de forma simultânea e bidirecional a totalidade do contexto textual. Essa propriedade conferiu aos \textit{Transformer}-based models a capacidade de gerar representações contextuais, um avanço significativo em relação às técnicas anteriores.

\begin{figure}[h]
	\caption{\label{transformers-schema}Transformador - modelo arquitetural.}
	\begin{center}
	   \includegraphics[scale=0.8]{figs/image.png}
	\end{center}
	\legend{Fonte: \citeonline[p. 24, Tradução nossa]{vaswani_2017}}
\end{figure}

Sobre essa base arquitetônica foram desenvolvidos os \gls{LLM}s pré-treinados, entre os quais se destaca o \gls{BERT} (\textit{Bidirectional Encoder Representations from Transformers}), introduzido por \citeonline{Devlin_2019}. Diferentemente de abordagens anteriores, como o \textit{GPT-1} de \citeonline{Radford_2018}, que utilizava um treinamento unidirecional, o \gls{BERT} foi projetado com pré-treinamento bidirecional, possibilitando a modelagem simultânea do contexto à esquerda e à direita de cada token. Essa característica permite a geração de representações semânticas profundas e contextualmente dependentes, adequadas para tarefas de classificação, extração de relações e análise semântica. Em termos conceituais, essa bidirecionalidade constitui um elemento central para este trabalho, uma vez que fornece vetores de alta qualidade para unidades textuais de diferentes granularidades (palavras, sentenças e documentos).

No âmbito da modelagem de tópicos, os \textit{embeddings} derivados do \gls{BERT} são utilizados em variantes como o \textit{Sentence-BERT} (\gls{SBERT}), proposto por \citeonline{Reimers_2019}, cujo objetivo é otimizar a geração de \textit{sentence embeddings}. Tais representações são fundamentais para o funcionamento do \textit{BERTopic}, introduzido por \citeonline{Grootendorst_2022}, uma vez que o algoritmo se apoia em medidas de similaridade semântica para agrupar documentos. Essa abordagem, diferentemente de técnicas tradicionais baseadas em frequência de termos — como LDA e PLSA —, permite a organização de corpora heterogêneos a partir de relações de significado. Em cenários multilingues, como no caso da plataforma SIMCC, a utilização de modelos como o \textit{paraphrase-multilingual-minilm-l12-v2} é particularmente relevante, visto que tais modelos produzem \textit{embeddings} semanticamente consistentes mesmo em diferentes idiomas. Estudos como o de \citeonline{Weng_2022} reforçam a pertinência dessa estratégia ao demonstrarem que representações baseadas em transformadores, quando associadas a métodos de agrupamento, revelam-se eficazes para a detecção e visualização de tópicos em coleções científicas.

Ainda que ferramentas como o \textit{BERTopic} se mostrem adequadas para a identificação inicial de tópicos, uma limitação frequentemente relatada diz respeito à interpretabilidade dos rótulos gerados, que tendem a ser genéricos ou de difícil compreensão. Nesse ponto, a integração com modelos de geração de linguagem natural mais recentes, como o \gls{GPT-4}, torna-se pertinente. Ao empregar sua capacidade de compreensão contextual e síntese textual, o \gls{GPT-4} pode ser utilizado para refinar e enriquecer a rotulagem dos tópicos identificados, além de produzir sumarizações mais coerentes e descritivas. Essa etapa complementar insere-se como um mecanismo de aprimoramento da interpretabilidade dos resultados obtidos, contribuindo para análises mais consistentes do ponto de vista científico.

\section{Abordagens Tradicionais de Modelagem de Tópicos}

Com o crescimento exponencial de dados textuais e a consequente necessidade de organizar informação em larga escala, a modelagem de tópicos consolidou-se como uma técnica fundamental na área de \gls{PLN}. Em termos gerais, trata-se de um conjunto de métodos estatísticos cujo objetivo é identificar estruturas semânticas latentes — denominadas \textit{tópicos} — em coleções de documentos. Assim, essas técnicas permitem inferir distribuições temáticas que não são explicitamente observáveis, mas que emergem a partir de regularidades no uso do vocabulário. Essa perspectiva abriu caminho para aplicações em áreas diversas, desde ciências sociais até biomedicina (\citeonline{Jung_2024}).

Entre as abordagens iniciais destacam-se três marcos históricos: a \textit{Latent Semantic Analysis} \gls{LSA}, a \textit{Probabilistic Latent Semantic Analysis} \gls{PLSA} e a \textit{Latent Dirichlet Allocation} \gls{LDA}. Esses métodos não apenas moldaram a compreensão inicial sobre a representação semântica de textos, como também estabeleceram fundamentos conceituais e metodológicos que orientaram o desenvolvimento de modelos mais avançados.

A \gls{LSA}, proposta por \citeonline{Deerwester_1990}, parte da decomposição de matrizes termo-documento por meio da técnica de \textit{Singular Value Decomposition} (\gls{SVD}). Nesse enquadramento, documentos e termos são projetados em um espaço vetorial de dimensionalidade reduzida, o que permite atenuar ruídos lexicais e capturar relações de similaridade latentes. Apesar de sua relevância histórica, a linearidade da \gls{LSA} e sua insensibilidade a variações contextuais limitam seu desempenho em cenários onde relações semânticas complexas são determinantes (\citeonline{George_2023, Xie_2020}).

Com o intuito de superar parte dessas limitações, \citeonline{Hofmann_1999,Hofmann_2001} introduziram a \gls{PLSA}, que reformulou a representação semântica a partir de um modelo probabilístico. Nessa abordagem, cada ocorrência de palavra em um documento é modelada como proveniente de um tópico latente, de forma que a probabilidade conjunta de palavra $w$ e documento $d$ é expressa como:
\[
P(w, d) = \sum_{z \in Z} P(z|d) \, P(w|z),
\]
onde $z$ representa o conjunto de tópicos latentes. Embora tenha representado um avanço em relação à \gls{LSA}, a \gls{PLSA} apresenta limitações notáveis, em especial no que se refere à escalabilidade: o número de parâmetros cresce linearmente com a quantidade de documentos, o que compromete sua generalização e a torna suscetível a \textit{overfitting} (\citeonline{Datchanamoorthy_2023}).

A evolução natural desse paradigma ocorreu com a formulação da \gls{LDA}, proposta por \citeonline{Blei_2003}. Ao contrário da \gls{PLSA}, a \gls{LDA} incorpora uma camada Bayesiana por meio da utilização de distribuições de \textit{Dirichlet} como \textit{priors}. Essa estrutura permite regularizar o modelo e definir uma distribuição de tópicos não apenas a nível de documento, mas também a nível de corpus, resultando em maior robustez e interpretabilidade. A \gls{LDA} parte da premissa de que cada documento é representado como uma mistura de tópicos, e cada tópico, por sua vez, é caracterizado por uma distribuição de palavras. Essa formulação tornou o modelo amplamente aplicável em diferentes domínios, como saúde pública (\citeonline{Mifrah_2020}) e eficiência energética (\citeonline{Polyzos_2022}).

Apesar de sua influência, tanto a \gls{LSA} quanto a \gls{PLSA} e a \gls{LDA} compartilham limitações estruturais. Todas operam no paradigma de \textit{bag-of-words}, que ignora a ordem e o contexto local das palavras, o que frequentemente conduz a representações semânticas superficiais em textos técnicos ou multilíngues (\citeonline{George_2023, Xie_2020}). Além disso, a sensibilidade da \gls{LDA} à definição do número de tópicos ($K$) representa um desafio adicional: valores reduzidos podem fundir tópicos distintos em um único, enquanto valores elevados podem fragmentar temas coesos em subtemas artificiais (\citeonline{Datchanamoorthy_2023}).

\begin{citacao}
A sensibilidade do LDA ao parâmetro do número de temas ($K$) é uma de suas desvantagens. Encontrar o valor ideal para ($K$) pode ser desafiador. O modelo pode simplificar excessivamente e combinar diferentes temas em um só se ($K$) for configurado muito baixo. No entanto, se ($K$) for configurado muito alto, o modelo pode se tornar muito complexo e produzir temas errôneos (\citeonline[Traduzido]{Datchanamoorthy_2023}).
\end{citacao}

Essas restrições evidenciam que, embora fundamentais, tais técnicas não capturam relações profundas e não lineares entre palavras e tópicos. Esse cenário motivou a emergência de abordagens modernas baseadas em \textit{embedding} e arquiteturas de \textit{transformer} (\citeonline{vaswani_2017, Devlin_2019, Radford_2018}), que oferecem maior sensibilidade contextual e escalabilidade para corpora heterogêneos e de grande volume.

\section{BERTopic: Uma Abordagem Moderna}

O \textit{Bidirectional Encoder Representations from Transformers} \gls{BERT}, introduzido por \citeonline{Devlin_2019}, marcou um avanço significativo no campo do \textit{Natural Language Processing} \gls{NLP}. Baseado na arquitetura de \textit{Transformers} (\citeonline{vaswani_2017}), o BERT emprega o mecanismo de \textit{self-attention} para capturar relações contextuais entre palavras em um texto. Diferentemente de abordagens anteriores, que analisavam sequências de maneira unidirecional, o BERT considera simultaneamente o contexto à esquerda e à direita de cada palavra, resultando em \textit{embeddings} ricos e contextuais. Essa característica tornou o BERT amplamente utilizado em tarefas como classificação de texto, análise de sentimentos e resposta a perguntas.

Apesar de sua relevância, o BERT não foi projetado para tarefas de similaridade semântica entre sentenças ou documentos, pois os vetores que gera não são diretamente comparáveis em termos de proximidade semântica (\citeonline{Reimers_2019}). Essa limitação levou ao desenvolvimento do \textit{Sentence-BERT} \gls{S-BERT}, uma variante que adapta o BERT ao treinamento em redes siamesas (\textit{Siamese Networks}) e funções de perda específicas, como \textit{triplet loss}. O resultado é a produção de \textit{embeddings} que são calculados por meio de técnicas como \textit{Class-based Term Frequency-Inverse Document Frequency} (c-TF-IDF), que ajusta os pesos das palavras com base em suas frequências e relevâncias dentro de um corpus, elas podem ser comparadas de forma eficiente por meio de medidas de distância, como \textit{cosine similarity}, viabilizando tarefas de busca semântica e agrupamento de documentos.

Sobre essa base, \citeonline{Grootendorst_2022} propôs o \textit{BERTopic}, que não deve ser entendido como um único modelo, mas como um \textit{pipeline} que integra diferentes técnicas complementares para a modelagem de tópicos. Esse arranjo inicia-se pela geração de \textit{embeddings} com o S-BERT, etapa que garante representações semânticas adequadas para comparação entre artigos científicos. Essa combinação permite que o BERTopic identifique tópicos de maneira dinâmica e precisa, mesmo em grandes volumes de dados textuais diversificados (\citeonline{George_2023, Jung_2024, Datchanamoorthy_2023}).

\begin{figure}[h]
    \centering
    \caption{\label{bertopic-eschema}Diagrama esquemático detalhado da comparação de métricas de avaliação entre modelos.}
    \includegraphics[scale=0.8]{figs/Bertopic.png}
    \caption*{\footnotesize Fonte: (\citeonline[p. 7, Tradução nossa]{Jung_2024})}
\end{figure}

Como observado no diagrama comparativo entre modelos, a etapa de redução de dimensionalidade no pipeline utiliza o \textit{Uniform Manifold Approximation and Projection} \gls{UMAP} (\citeonline{McInnes_2018}, uma técnica que projeta vetores de alta dimensionalidade em um espaço reduzido. Essa abordagem se fundamenta em princípios teóricos de geometria Riemanniana e topologia algébrica , o que a diferencia de métodos mais antigos, como o \textit{t-SNE} (\citeonline{Maaten_2008}), e lhe confere maior escalabilidade e eficiência para a análise de grandes volumes de dados. O UMAP opera em duas fases principais: primeiro, constrói um grafo ponderado que representa a estrutura topológica dos dados em alta dimensão; em seguida, projeta esse grafo para um espaço de baixa dimensão, otimizando o layout para minimizar a entropia cruzada entre as duas representações. Essa metodologia é crucial para preservar tanto as estruturas locais quanto as globais do corpus, garantindo a coesão semântica dos dados. Ao aplicar o UMAP ao conjunto de publicações científicas da plataforma SIMCC, é possível manter a fidelidade das relações entre os documentos, um requisito fundamental para a subsequente fase de agrupamento do BERTopic.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/UMAP.png}
    \caption{Diagrama ilustrativo do UMAP, que demonstra a relação entre os hiperparâmetros \textit{n\_neighbors} e \textit{min\_dist} e a representação visual dos dados. O parâmetro \textit{n\_neighbors} controla a balança entre a preservação da estrutura global (valores altos) e local (valores baixos), enquanto \textit{min\_dist} ajusta a densidade dos agrupamentos, determinando a proximidade entre os pontos no espaço de baixa dimensionalidade. Esta visualização é crucial para otimizar o algoritmo e garantir que a estrutura semântica das publicações científicas seja fielmente representada para a subsequente etapa de agrupamento.}
    \caption*{\footnotesize Fonte: (\citeonline[p. 24]{McInnes_2018})}
    \label{fig:UMAP}
\end{figure}

Com os vetores de alta dimensionalidade reduzidos pelo UMAP, a etapa subsequente é o agrupamento por meio do \textit{Hierarchical Density-Based Spatial Clustering of Applications with Noise} \gls{HDBSCAN}. Diferentemente de métodos clássicos como o K-Means, que assume \textit{clusters} esféricos e de densidade uniforme, o HDBSCAN é um algoritmo de agrupamento baseado em densidade que não faz suposições prévias sobre a forma ou a densidade dos agrupamentos (\citeonline{Campello_2013}). Sua arquitetura hierárquica constrói uma árvore de conectividade que reflete a estrutura de densidade subjacente dos dados, permitindo a identificação de \textit{clusters} de densidade variável. Essa capacidade é particularmente relevante para a análise de publicações científicas, onde a distribuição dos tópicos tende a ser heterogênea. O HDBSCAN também se destaca por sua robustez ao tratar documentos que não se ajustam a nenhum padrão temático, classificando-os como outliers de forma intrínseca, sem a necessidade de um passo de pós-processamento. Essa característica é especialmente relevante em contextos de produção científica, onde coexistem tanto publicações centrais com alta densidade de tópicos quanto trabalhos periféricos ou com temas emergentes. Essa abordagem garante que a sua análise não apenas identifique os tópicos dominantes, mas também lide eficientemente com a diversidade e o ruído natural do corpus da plataforma SIMCC.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/HDBSCAN.png}
    \caption{Figura ilustrativa de um \textit{dataset} sintético com quatro \textit{clusters} e ruído de fundo. A imagem demonstra o tipo de desafio que o algoritmo HDBSCAN é capaz de superar, como a identificação de agrupamentos de densidades e formas variadas, além de tratar \textit{outliers} de forma eficiente. Este comportamento é ideal para a análise de publicações científicas, onde a distribuição dos tópicos tende a ser heterogênea e não segue padrões geométricos rígidos.}
    \caption*{\footnotesize Fonte: (\citeonline[p. 16]{Campello_2013})}
    \label{fig:HDBSCAN}
\end{figure}

Por fim, o BERTopic aplica o \textit{class-based Term Frequency-Inverse Document Frequency} (c-TF-IDF), que trata cada cluster como um único documento. Essa abordagem destaca termos distintivos de cada grupo, permitindo identificar palavras-chave representativas mesmo quando não são as mais frequentes (\citeonline{Gana_2024, Grootendorst_2022}). O c-TF-IDF, portanto, fornece uma base interpretável para a descrição de cada tópico.

A combinação dessas etapas — \textit{embeddings} com S-BERT, redução de dimensionalidade com UMAP, clusterização com HDBSCAN e representação com c-TF-IDF — estabelece um fluxo robusto para a modelagem de tópicos. No contexto deste trabalho, esse \textit{pipeline} constitui o núcleo do processo de análise das publicações científicas indexadas na plataforma SIMCC, servindo de ponto de partida para a integração com modelos de linguagem de grande escala, como o GPT-4, que será empregado para enriquecer semanticamente os rótulos dos tópicos e aprimorar sua interpretabilidade.


\section{Modelos de Linguagem de Grande Escala (LLMs)}

Os Modelos de Linguagem de Grande Escala (LLMs) constituem um marco no avanço do Processamento de Linguagem Natural (PLN), permitindo análises textuais sofisticadas e interpretações semânticas em volumes de dados sem precedentes. Fundamentados em arquiteturas baseadas em \textit{transformers}, como o BERT, GPT e suas variantes, esses modelos utilizam aprendizado profundo para construir representações contextuais dinâmicas de palavras e sentenças. Ao transformar o texto em \textit{embeddings} semânticos, capturam relações latentes complexas entre elementos linguísticos, servindo de alicerce para tarefas como sumarização automática, classificação de documentos e modelagem de tópicos (\citeonline{Meng_2024, Gana_2024}). 

Enquanto LLMs podem ser definidos de forma geral como sistemas de PLN capazes de aprender distribuições linguísticas a partir de grandes corpora não anotados, o modo como tais modelos realizam o pré-treinamento e o ajuste fino (\textit{fine-tuning}) difere significativamente entre arquiteturas. As primeiras tentativas de modelos sequenciais, como Redes Neurais Recorrentes (RNNs) e Redes de Memória de Longo-Curto Prazo (LSTMs), apresentavam limitações na captura de dependências de longo alcance. Esse problema foi mitigado com a introdução do \textit{Transformer} por \citeonline{vaswani_2017}, cuja operação se baseia no Mecanismo de Atenção (\textit{self-attention}, permitindo atribuir diferentes pesos às palavras do contexto e, consequentemente, capturar relações semânticas globais de maneira mais eficiente.

O treinamento de LLMs ocorre tipicamente em duas etapas complementares. Na fase de \textit{pré-treinamento}, emprega-se aprendizado não supervisionado para expor o modelo a trilhões de palavras em dados textuais como documentos e dados da internet, consolidando padrões gerais da linguagem. Dois paradigmas se destacam nesse processo: (i) a Modelagem de Linguagem Autorregressiva, como no GPT, onde o modelo aprende a prever o próximo token a partir de uma sequência de tokens anteriores; e (ii) a Modelagem de Linguagem Mascarada, como no BERT, em que lacunas são ocultadas e o modelo deve inferi-las a partir do contexto bidirecional (\citeonline{Devlin_2019, Jung_2024}). Em seguida, ocorre o \textit{fine-tuning}, etapa supervisionada em que o modelo é ajustado a tarefas específicas, como classificação de textos, análise de sentimentos ou sumarização, garantindo robustez e especialização (\cite{Gana_2024}).

\begin{itemize}
    \item \textbf{Modelagem de Linguagem Autorregressiva:} Modelos como o GPT (\textit{Generative Pre-trained Transformer}) seguem um fluxo sequencial unidirecional, prevendo cada token com base nos anteriores. Essa abordagem favorece a coerência narrativa e a fluidez na geração textual, aspectos essenciais em tarefas de criação de conteúdo (\citeonline{Radford_2018, Jung_2024}).
    
    \item \textbf{Modelagem de Linguagem Mascarada:} Modelos como o BERT (\textit{Bidirectional Encoder Representations from Transformers}) aplicam mascaramento aleatório em tokens, forçando o modelo a interpretar o contexto bidirecionalmente. Tal característica possibilita uma maior sensibilidade semântica, útil em tarefas como inferência textual e modelagem de tópicos (\citeonline{Devlin_2019, Datchanamoorthy_2023}).
\end{itemize}

Entre os LLMs mais avançados, destaca-se o \textbf{GPT-4}, evolução do \textbf{GPT-1} desenvolvido pela OpenAI por \citeonline{Radford_2018}, que introduziu sua arquitetura a partir de um trabalho seminal. Sua estrutura permanece fundamentada no paradigma \textit{transformer} \citeonline{vaswani_2017}, mas incorpora modificações substanciais em relação às versões anteriores. Embora a documentação oficial seja limitada por razões proprietárias, o \textit{Technical Report} da OpenAI \cite{OpenAI_2023} e análises independentes \citeonline{Liu_2023, Achiam_2023} sugerem que o GPT-4 conta com bilhões de parâmetros adicionais em comparação ao GPT-3, além de maior profundidade de camadas de atenção e mecanismos otimizados de paralelização no treinamento distribuído. Essas melhorias resultam em avanços na capacidade de raciocínio semântico, na robustez diante de contextos ambíguos e na generalização para tarefas pouco definidas.

Outro aspecto relevante é o aprimoramento nos métodos de alinhamento e segurança (\textit{alignment}), alcançados por meio de técnicas como o \textit{Reinforcement Learning with Human Feedback} (RLHF), que possibilitam ao modelo produzir respostas mais consistentes com critérios humanos de qualidade e relevância (\citeonline{OpenAI_2023, Ouyang_2022}). Além disso, o GPT-4 demonstra melhor desempenho em cenários multilíngues e em tarefas de alto nível cognitivo, como resolução de problemas em exames padronizados e síntese de conhecimento interdisciplinar (\citeonline{Achiam_2023}). Essas características tornam o modelo especialmente adequado para aplicações acadêmicas e científicas, onde a precisão semântica e a interpretabilidade das respostas são fundamentais.

Ao compararmos o \textbf{BERTopic} e o GPT-4, evidenciam-se diferenças fundamentais na natureza e aplicação de cada modelo. O BERTopic, embora baseado em \textit{embeddings} derivados de modelos como BERT, concentra-se em identificar e organizar tópicos latentes a partir de representações vetoriais de documentos, utilizando algoritmos já citados nas seções anteriores. Seu ponto forte está na capacidade de estruturar grandes volumes de dados em \textit{clusters} semanticamente coerentes \citeonline{Grootendorst_2022}. Já o GPT-4, além de gerar representações contextuais sofisticadas, pode ser utilizado para atribuir rótulos semânticos refinados a tais \textit{clusters}, ampliando a interpretabilidade dos tópicos e permitindo a construção de narrativas explicativas sobre tendências detectadas nos dados \citeonline{Meng_2024, Galli_2024}.

No contexto deste projeto, a integração de ambos os modelos se mostra justificada. Enquanto o BERTopic viabiliza a organização automática de grandes corpora textuais provenientes da plataforma SIMCC, o GPT-4 agrega valor na etapa de rotulagem, interpretação semântica e análise contextual aprofundada. Tal combinação potencializa tanto a acurácia quanto a inteligibilidade dos resultados, conciliando rigor metodológico com clareza interpretativa. Além disso, a aplicação conjunta favorece a detecção de padrões emergentes em múltiplos idiomas, aspecto essencial dada a heterogeneidade linguística base de dados.

Portanto, ao invés de restringir-se a abordagens puramente estatísticas ou unicamente gerativas, este trabalho adota uma perspectiva híbrida, combinando técnicas de modelagem de tópicos e de raciocínio semântico avançado, buscando suprir lacunas de interpretabilidade.
