\section[Projeto e Implementação da Solução]{Projeto e Implementação da Solução}
\label{sec:arquitetura_solucao}

Esta seção detalha a implementação prática do artefato computacional, detalhando o fluxo de dados e as etapas de processamento do \textit{pipeline}. Em conformidade com a fase do (Capítulo \ref{chap:metodologia}) da metodologia \gls{dsr}, o objetivo da implementação é processar os dados textuais do Observatório para gerar uma representação interativa do conhecimento científico.

Para operacionalizar essa tarefa, a solução foi estruturada em quatro etapas principais, conforme ilustrado na Figura \ref{fig:pipeline_artefato}:

\begin{enumerate}
    \item \textbf{Coleta e Pré-processamento dos Dados (Seção \ref{ssec:coleta_dados}):} Extração dos dados textuais (títulos e resumos) da base de dados e sua subsequente limpeza e normalização.
    
    \item \textbf{Modelagem de Tópicos (Seção \ref{ssec:modelagem_topicos}):} Geração dos \textit{embeddings} de sentenças (SBERT), redução de dimensionalidade (UMAP) e agrupamento (HDBSCAN) para a identificação dos \textit{clusters} temáticos.
    
    \item \textbf{Refinamento e Representação dos Tópicos (Seção \ref{ssec:refinamento_rotulos}):} Extração das palavras-chave via \gls{c-tf-idf} e aplicação do \gls{mmr} para gerar rótulos semanticamente diversos e interpretáveis.
    
    \item \textbf{Geração e Exportação para Visualização (Seção \ref{ssec:exportacao_wizmap}):} Formatação dos dados processados (coordenadas 2D, metadados e rótulos) e exportação para o formato JSON consumível pela ferramenta \gls{wizmap}.
\end{enumerate}

O fluxograma completo deste artefato, desde a ingestão da base de dados até a geração da visualização final no \gls{wizmap}, está representado esquematicamente na Figura \ref{fig:pipeline_artefato}.

\begin{figure}[H]
    \centering
    % Ajuste o 'width' conforme necessário. 1.0\textwidth ou 0.9\textwidth
    \includegraphics[width=1.0\textwidth]{figs/Arquitetura da solução.png} 
    \caption{Arquitetura do Pipeline de Modelagem e Visualização.}
    \label{fig:pipeline_artefato}
    \legend{Fonte: O Autor}
    % Lembre-se de salvar a imagem 'image_dff87f.png' na sua pasta 'figuras/' 
    % com um nome como 'arquitetura_pipeline_completo.png'
\end{figure}

\subsection[Coleta e Pré-processamento dos Dados]{Coleta e Pré-processamento dos Dados}
\label{ssec:coleta_dados}

A camada de ingestão de dados foi projetada para preparar o corpus textual para as tarefas subsequentes de NLP. A implementação utilizou uma extração estática dos metadados do Observatório, selecionando o título das publicações como atributo principal para a modelagem, devido ao seu alto valor semântico e densidade informativa.

O módulo de pré-processamento foi estruturado em dois estágios lógicos: saneamento (limpeza) do \textit{dataset} e normalização linguística.

No estágio de saneamento, implementado via biblioteca Pandas, aplicaram-se filtros para a exclusão de registros inconsistentes, como entradas nulas ou duplicatas exatas. Essa etapa foi crítica para assegurar que a frequência de termos (TF-IDF) não fosse distorcida por redundâncias na base de dados

No estágio de normalização, cada documento foi submetido a um pipeline de tratamento textual multilíngue, composto pelas seguintes rotinas:

\begin{enumerate}
    \item \textbf{Identificação do Idioma:} Classificação automática do texto via biblioteca \textit{langdetect} para direcionar o processamento específico (Português/Inglês).

    \item \textbf{Normalização e Limpeza de Caracteres:} Conversão para caixa baixa e remoção de acentuação (normalização Unicode NFKD) e caracteres especiais via expressões regulares, reduzindo a dimensionalidade do vocabulário .

    \item \textbf{Lematização e Tokenização (spaCy):} Aplicação de lematização via \textit{spaCy}, convertendo termos flexionados à sua forma canônica (lema) para agrupar variações da mesma palavra.

    \item \textbf{Filtragem Final de Tokens:} Remoção de \textit{stopwords} (termos funcionais sem carga semântica) e de \textit{tokens} com comprimento inferior a três caracteres
\end{enumerate}

Ao final desta etapa, as publicações que resultaram em textos vazios (ex: títulos que continham apenas \textit{stopwords} ou siglas curtas) foram removidas do conjunto final. O \textit{dataset} resultante, consistiu em uma lista de documentos processados, prontos para a etapa de geração de \textit{embeddings}.

\subsection[Modelagem de Tópicos]{Modelagem de Tópicos}
\label{ssec:modelagem_topicos}

O núcleo de processamento do artefato baseia-se na arquitetura \gls{bert}, configurada para operar como um pipeline sequencial de transformação de dados. A implementação técnica seguiu o fluxo de vetorização, redução dimensional e agrupamento.

\subsubsection{Geração de Embeddings}
\label{sssec:geracao_embeddings}

A representação vetorial dos documentos foi realizada através do modelo pré-treinado \textit{paraphrase-multilingual-MiniLM-L12-v2} da biblioteca \textit{Sentence-Transformers}. A escolha deste modelo justifica-se pela sua arquitetura otimizada para aferir similaridade semântica em contextos multilíngues, convertendo cada título em um vetor denso de 384 dimensões.

\subsubsection[Redução de Dimensionalidade (UMAP)]{Redução de Dimensionalidade (UMAP)}
\label{sssec:reducao_dimensionalidade}

A configuração do \gls{umap} priorizou a preservação da estrutura local dos dados em detrimento da estrutura global, visando a identificação de micro-tópicos. Os hiperparâmetros foram ajustados experimentalmente para projetar os vetores em um espaço bidimensional (\textit{n\_components=2}), requisito necessário para a compatibilidade com a camada de visualização WizMap. A métrica de distância cosseno foi adotada para capturar a orientação semântica dos vetores.

\subsubsection[Clusterização (HDBSCAN)]{Clusterização (HDBSCAN)}
\label{sssec:clusterizacao}

A definição dos tópicos foi executada pelo algoritmo \gls{hdbscan}. A opção por um método baseado em densidade, em oposição a métodos particionais como o \textit{K-Means}, deve-se à natureza não uniforme dos dados textuais científicos.

O algoritmo foi parametrizado para detectar agrupamentos de densidade variável e para isolar documentos desconexos como ruído (\textit{outliers}). Definiu-se um tamanho mínimo de \textit{cluster} (\textit{min\_cluster\_size=4}) reduzido, permitindo a emergência de temas de nicho ou incipientes na base de dados analisada.

\subsubsection{Configuração Final do BERTopic}
\label{sssec:configuracao_bertopic}

A instância final do modelo foi configurada para utilizar o \textbf{SentenceTransformer} como \textit{embedding\_model}, as instâncias customizadas de \textbf{UMAP} e \textbf{HDBSCAN} e, de forma notável, o \textbf{MaximalMarginalRelevance} (\gls{mmr}) como \textit{representation\_model} para o refinamento dos rótulos de tópicos, que será detalhado na Seção \ref{ssec:refinamento_rotulos}. O parâmetro \textbf{language="multilingual"} foi especificado, alinhando-se ao pré-processamento bilíngue.

Após a configuração, o modelo foi treinado com os textos processados e seus respectivos \textit{embeddings} pré-gerados. Os resultados, incluindo os IDs dos tópicos para cada documento e suas probabilidades, foram então obtidos. O modelo treinado foi salvo persistentemente para facilitar sua reutilização.

\subsection[Refinamento e Representação dos Tópicos (MMR)]{Refinamento e Representação dos Tópicos (MMR)}
\label{ssec:refinamento_rotulos}

Após a etapa de \textit{clusterização} (Seção \ref{sssec:clusterizacao}), o \textit{pipeline} produziu um conjunto de tópicos, cada um consistindo em um grupo de documentos. No entanto, esses tópicos eram apenas agrupamentos numéricos; para que tivessem valor analítico, precisavam de uma representação textual interpretável.

O \gls{bertopic} realiza essa tarefa através do \gls{c-tf-idf} (Class-based Term Frequency-Inverse Document Frequency), um algoritmo que, conforme descrito na Seção \ref{sec:bertopic}, trata todos os documentos de um tópico como um único documento grande e, em seguida, calcula as pontuações \gls{tf-idf} para extrair as palavras-chave mais representativas.

Embora eficaz, uma limitação conhecida da abordagem \gls{c-tf-idf} pura é a \textbf{redundância semântica}. Como o método se baseia unicamente na pontuação de relevância, ele frequentemente retorna termos que são variações uns dos outros (ex: \textit{\enquote{pesquisa}, \enquote{pesquisas}, \enquote{pesquisador}}) ou sinônimos próximos (ex: \textit{\enquote{modelo}, \enquote{modelagem}}). Isso polui a representação do tópico e compromete a sua interpretabilidade imediata, forçando o analista a inferir o conceito central a partir de palavras redundantes.

Para mitigar este problema e criar rótulos de tópicos mais claros e informativos, este artefato substituiu o modelo de representação padrão pelo \gls{mmr} (Maximal Marginal Relevance). O \gls{mmr} é um algoritmo de diversificação (fundamentado na Seção \ref{sec:bertopic}) que refina a lista de palavras-chave gerada pelo \gls{c-tf-idf}.

Conforme definido no código de treinamento, o modelo foi instanciado com o parâmetro \texttt{MaximalMarginalRelevance(diversity=0.3)}. O \gls{mmr} opera balanceando duas métricas:
\begin{enumerate}
    \item \textbf{Relevância:} A pontuação original da palavra (seu \textit{score} \gls{c-tf-idf}).
    \item \textbf{Diversidade:} A dissimilaridade (distância de cosseno) entre uma palavra candidata e as palavras já selecionadas para o rótulo.
\end{enumerate}

O hiperparâmetro \texttt{diversity=0.3} instrui o algoritmo a priorizar fortemente a relevância (70\% de peso), ao mesmo tempo em que introduz um fator moderado de diversidade (30\% de peso). Na prática, isso permite que o modelo selecione a palavra mais relevante (ex: \textit{\enquote{pesquisa}}), mas penalize termos semanticamente muito próximos (como \textit{\enquote{pesquisas}}), favorecendo a escolha de uma palavra subsequente que, embora ainda relevante, cubra uma faceta diferente do tópico.

A aplicação do \gls{mmr} como \texttt{representation\_model} (passado diretamente para a instância do \gls{bertopic}) busca garantir que a saída final do modelo seja um conjunto de rótulos de tópicos semanticamente diversos, menos redundantes e mais humanamente inteligíveis, resolvendo uma das fraquezas centrais do \gls{c-tf-idf} puro.

\subsection[Geração e Exportação para Visualização (WizMap)]{Geração e Exportação para Visualização (WizMap)}
\label{ssec:exportacao_wizmap}

A camada final da arquitetura é responsável pela interface com o usuário. A implementação utilizou a biblioteca wizmap para processar os artefatos gerados pelo núcleo (coordenadas UMAP, metadados e rótulos refinados) e gerar os arquivos de visualização.

Primeiramente, foi construída uma estrutura de dados unificada. As coordenadas bidimensionais (x, y) de cada publicação, que foram calculadas pelo \gls{umap} (Seção \ref{sssec:reducao_dimensionalidade}), foram consolidadas. A elas, foram associados os metadados originais (como o título da publicação) e os resultados da modelagem (o ID do tópico atribuído e o rótulo textual refinado pelo \gls{mmr}).

Para garantir a clareza na interface final, os rótulos dos tópicos passaram por uma etapa de pós-processamento para remover prefixos numéricos (ex: \texttt{1\_pesquisa\_dados} $\rightarrow$ \texttt{pesquisa dados}), tornando-os mais legíveis. Adicionalmente, foi formatado um texto de \textit{tooltip} (dica de contexto) para cada publicação, permitindo ao usuário final inspecionar o título e o tópico de um ponto específico ao interagir com o mapa.

Por fim, a biblioteca \texttt{wizmap} foi utilizada para processar essa estrutura de dados consolidada. Esta ferramenta gerou os dois arquivos \gls{json} essenciais para a renderização do mapa, conforme a arquitetura de visualização descrita na Seção \ref{sec:visualizacao_dados}:

\begin{enumerate}
    \item \textbf{Um arquivo de dados brutos:} Contendo a lista completa de todas as publicações, suas coordenadas 2D e o texto do \textit{tooltip} personalizado.
    \item \textbf{Um arquivo de grade (grid):} Contendo a estrutura de dados \textit{quadtree} de multi-resolução. Este arquivo é a inovação técnica do \gls{wizmap}, pois permite ao navegador renderizar de forma eficiente milhões de pontos e agregar rótulos de forma hierárquica em diferentes níveis de \textit{zoom}.
\end{enumerate}

Estes dois arquivos \gls{json} representam o produto final do \textit{pipeline} de engenharia de dados, prontos para serem hospedados (conforme Seção \ref{ssec:pipeline_artefato}) e consumidos pela interface \textit{html} do \gls{wizmap}, que renderiza o mapa de conhecimento interativo.