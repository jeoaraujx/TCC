\section[Projeto e Implementação da Solução]{Projeto e Implementação da Solução}
\label{sec:arquitetura_solucao}

Esta seção detalha a implementação prática do artefato computacional, detalhando o fluxo de dados e as etapas de processamento do \textit{pipeline}. Em conformidade com a fase do (Capítulo \ref{chap:metodologia}) da metodologia \gls{dsr}, o objetivo da implementação é processar os dados textuais do Observatório para gerar uma representação interativa do conhecimento científico.

Para operacionalizar essa tarefa, a solução foi estruturada em quatro etapas principais, conforme ilustrado na Figura \ref{fig:pipeline_artefato}:

\begin{enumerate}
    \item \textbf{Coleta e Pré-processamento dos Dados (Seção \ref{ssec:coleta_dados}):} Extração dos dados textuais (títulos e resumos) da base de dados e sua subsequente limpeza e normalização.
    
    \item \textbf{Modelagem de Tópicos (Seção \ref{ssec:modelagem_topicos}):} Geração dos \textit{embeddings} de sentenças (SBERT), redução de dimensionalidade (UMAP) e agrupamento (HDBSCAN) para a identificação dos \textit{clusters} temáticos.
    
    \item \textbf{Refinamento e Representação dos Tópicos (Seção \ref{ssec:refinamento_rotulos}):} Extração das palavras-chave via \gls{c-tf-idf} e aplicação do \gls{mmr} para gerar rótulos semanticamente diversos e interpretáveis.
    
    \item \textbf{Geração e Exportação para Visualização (Seção \ref{ssec:exportacao_wizmap}):} Formatação dos dados processados (coordenadas 2D, metadados e rótulos) e exportação para o formato JSON consumível pela ferramenta \gls{wizmap}.
\end{enumerate}

O fluxograma completo deste artefato, desde a ingestão da base de dados até a geração da visualização final no \gls{wizmap}, está representado esquematicamente na Figura \ref{fig:pipeline_artefato}.

\begin{figure}[H]
    \centering
    % Ajuste o 'width' conforme necessário. 1.0\textwidth ou 0.9\textwidth
    \includegraphics[width=1.0\textwidth]{figs/Arquitetura da solução.png} 
    \caption{Arquitetura do Pipeline de Modelagem e Visualização.}
    \label{fig:pipeline_artefato}
    \legend{Fonte: O Autor}
\end{figure}

\subsection[Coleta e Pré-processamento dos Dados]{Coleta e Pré-processamento dos Dados}
\label{ssec:coleta_dados}

A camada de ingestão de dados foi projetada utilizando uma extração estática (dump) dos metadados do Núcleo de Pesquisa Aplicada e Inovação (NPAI). A opção por um dataset estático e isolado do ambiente de produção foi uma decisão metodológica para garantir a integridade e a reprodutibilidade dos experimentos de otimização de hiperparâmetros. O uso do banco completo e dinâmico nesta fase de desenvolvimento introduziria variáveis de instabilidade e custo computacional desnecessários para a validação da prova de conceito (PoC) da arquitetura proposta.

\subsubsection{Caracterização do Corpus de Teste}
\label{sssec:caracterizacao_corpus}

Para a validação técnica do pipeline, utilizou-se um corpus de teste extraído do repositório do NPAI. Este conjunto de dados consiste em uma amostra de \textit{343 registros}, selecionados a partir dos critérios de relevância bibliográfica: publicações em livros, capítulos de livros e artigos completos. A utilização deste volume reduzido foi intencional para viabilizar a execução iterativa dos testes de otimização de hiperparâmetros em ambiente de desenvolvimento, sem comprometer a estabilidade do sistema de produção.

A amostra compreende um intervalo temporal de \textit{26 anos}, com publicações registradas entre \textit{1998 e 2024}. Quanto à distribuição linguística, o corpus é predominantemente bilíngue, composto por \textit{62,39\%} de títulos em Português e \textit{35,28\%} em Inglês, contendo ainda uma fração de \textit{2,33\%} em Espanhol. Essa heterogeneidade linguística foi fundamental para testar a capacidade do modelo \textit{paraphrase-multilingual-MiniLM-L12-v2} em agrupar semanticamente documentos de mesmo tema, independentemente do idioma de origem. Para fins de transparência e reprodutibilidade, o dataset completo encontra-se disponibilizado no repositório GitHub\footnote{Disponível em: https://github.com/jeoaraujx/TCC} referenciado na nota de rodapé deste capítulo.

\subsubsection{Etapas de Pré-processamento}

O módulo de pré-processamento foi estruturado em dois estágios lógicos: saneamento (limpeza) do \textit{dataset} e normalização linguística.

No estágio de saneamento, implementado via biblioteca Pandas, aplicaram-se filtros para a exclusão de registros inconsistentes, como entradas nulas ou duplicatas exatas. Essa etapa foi crítica para assegurar que a frequência de termos (TF-IDF) não fosse distorcida por redundâncias na base de dados.

No estágio de normalização, cada documento foi submetido a um pipeline de tratamento textual multilíngue, composto pelas seguintes rotinas:

\begin{enumerate}
    \item \textbf{Identificação do Idioma:} Classificação automática do texto via biblioteca \textit{langdetect} para direcionar o processamento específico (Português/Inglês).

    \item \textbf{Normalização e Limpeza de Caracteres:} Conversão para caixa baixa e remoção de acentuação (normalização Unicode NFKD) e caracteres especiais via expressões regulares, reduzindo a dimensionalidade do vocabulário.

    \item \textbf{Lematização e Tokenização (spaCy):} Aplicação de lematização via \textit{spaCy}, convertendo termos flexionados à sua forma canônica (lema) para agrupar variações da mesma palavra.

    \item \textbf{Filtragem Final de Tokens:} Remoção de \textit{stopwords} (termos funcionais sem carga semântica) e de \textit{tokens} com comprimento inferior a três caracteres.
\end{enumerate}

Ao final desta etapa, as publicações que resultaram em textos vazios (ex: títulos que continham apenas \textit{stopwords} ou siglas curtas) foram removidas do conjunto final. O \textit{dataset} resultante, consistiu em uma lista de documentos processados, prontos para a etapa de geração de \textit{embeddings}.
\subsection[Modelagem de Tópicos]{Modelagem de Tópicos}
\label{ssec:modelagem_topicos}

A arquitetura do pipeline foi concebida de forma modular, seguindo o padrão de injeção de dependências permitido pela biblioteca BERTopic. Conforme ilustrado na \ref{fig:pipeline_artefato}, o \enquote{Núcleo BERTopic} não atua como um monólito rígido, mas sim como um orquestrador do fluxo de dados. As etapas de Redução de Dimensionalidade e Clusterização são executadas por instâncias customizadas dos algoritmos UMAP e HDBSCAN, respectivamente, que são configuradas externamente (com hiperparâmetros ajustados para o cenário de textos curtos) e injetadas no pipeline principal. Essa abordagem garante a reprodutibilidade e o controle fino sobre cada etapa do processamento, permitindo que o BERTopic gerencie a integração entre os embeddings gerados e os clusters resultantes.

\subsubsection{Geração de Embeddings}
\label{sssec:geracao_embeddings}

A representação vetorial dos documentos foi realizada através do modelo pré-treinado \textit{paraphrase-multilingual-MiniLM-L12-v2} da biblioteca \textit{Sentence-Transformers}. A escolha deste modelo justifica-se pela sua arquitetura otimizada para aferir similaridade semântica em contextos multilíngues, convertendo cada título em um vetor denso de 384 dimensões.

\subsubsection[Redução de Dimensionalidade (UMAP)]{Redução de Dimensionalidade (UMAP)}
\label{sssec:reducao_dimensionalidade}

A configuração do \gls{umap} priorizou a preservação da estrutura local dos dados em detrimento da estrutura global, visando a identificação de micro-tópicos. Os hiperparâmetros foram ajustados experimentalmente para projetar os vetores em um espaço bidimensional (\textit{n\_components=2}), requisito necessário para a compatibilidade com a camada de visualização WizMap. A métrica de distância cosseno foi adotada para capturar a orientação semântica dos vetores.

\subsubsection[Clusterização (HDBSCAN) e Otimização de Hiperparâmetros]{Clusterização (HDBSCAN) e Otimização de Hiperparâmetros}
\label{sssec:clusterizacao}

A definição dos agrupamentos foi executada pelo algoritmo \gls{hdbscan}. Para determinar o valor ideal do hiperparâmetro \textit{min\_cluster\_size}, que controla o tamanho mínimo para a formação de um tópico, foi conduzida uma análise de sensibilidade quantitativa.

O experimento consistiu no treinamento iterativo do modelo variando o parâmetro \textit{min\_cluster\_size} em um intervalo de 4 a 25. Para cada iteração, foram coletadas duas métricas de desempenho:

\begin{enumerate}
    \item \textbf{Coerência Semântica ($C_v$):} Utilizando a medida $c\_v$ (baseada em janelas deslizantes) para avaliar o grau de associação semântica entre as palavras principais de cada tópico.
    \item \textbf{Taxa de Ruído (\textit{Outliers}):} Percentual de documentos classificados como -1 (sem tópico atribuído) pelo algoritmo.
\end{enumerate}

O critério de seleção adotado buscou o ponto de equilíbrio que maximizasse a coerência dos tópicos gerados, mantendo a taxa de descarte de documentos (perda de dados) abaixo de 10\%.

\subsubsection{Configuração Final do BERTopic}
\label{sssec:configuracao_bertopic}

A instância final do modelo foi configurada para utilizar o \textit{SentenceTransformer} como \textit{embedding\_model}, as instâncias customizadas de \textit{UMAP} e \textit{HDBSCAN} e, de forma notável, o \textit{MaximalMarginalRelevance} (\gls{mmr}) como \textit{representation\_model} para o refinamento dos rótulos de tópicos, que será detalhado na Seção \ref{ssec:refinamento_rotulos}. O parâmetro \textit{language="multilingual"} foi especificado, alinhando-se ao pré-processamento bilíngue.

Após a configuração, o modelo foi treinado com os textos processados e seus respectivos \textit{embeddings} pré-gerados. Os resultados, incluindo os IDs dos tópicos para cada documento e suas probabilidades, foram então obtidos. O modelo treinado foi salvo persistentemente para facilitar sua reutilização.

\subsection[Refinamento e Representação dos Tópicos (MMR)]{Refinamento e Representação dos Tópicos (MMR)}
\label{ssec:refinamento_rotulos}

Para avaliar a qualidade semântica e a interpretabilidade dos tópicos gerados, conduziu-se um experimento comparativo focado na etapa de representação de palavras-chave do pipeline. O objetivo foi contrastar a abordagem padrão baseada em frequência (\gls{c-tf-idf}) com a abordagem baseada em diversidade (Maximal Marginal Relevance - MMR).

Para garantir a validade interna da comparação, as etapas antecedentes à representação foram mantidas constantes em ambos os cenários de teste:

\begin{itemize}
    \item \textbf{Embeddings:} Utilizou-se o modelo pré-treinado \textit{paraphrase-multilingual-MiniLM-L12-v2}.
    \item \textbf{Redução e Clusterização:} O algoritmo UMAP foi configurado com 5 vizinhos e 2 componentes. O algoritmo HDBSCAN foi fixado com \textit{min\_cluster\_size=8}, valor determinado previamente na análise de sensibilidade (Seção 6.1) por apresentar o maior índice de coerência ($C_v \approx 0.43$) e taxa de outliers reduzida (< 9\%).
\end{itemize}

Foram estabelecidos dois cenários de representação:
\begin{enumerate}
    \item \textbf{Cenário de Controle (Padrão):} Utilização exclusiva do c-TF-IDF. Este método prioriza palavras frequentes dentro de um cluster específico, mas raras no restante do \textit{corpus}. A hipótese é que este método gere rótulos fiéis estatisticamente, porém com tendência à redundância morfológica.
    \item \textbf{Cenário Experimental (MMR):} Aplicação do MMR sobre os resultados do c-TF-IDF, com fator de diversidade ($\lambda$) configurado em \textit{0.3}. A hipótese é que o reranqueamento reduza a sinonímia e revele subtemas latentes ofuscados por termos genéricos de alta frequência.
\end{enumerate}

O hiperparâmetro \texttt{diversity=0.3} instrui o algoritmo a priorizar fortemente a relevância (70\% de peso), ao mesmo tempo em que introduz um fator moderado de diversidade (30\% de peso). Na prática, isso permite que o modelo selecione a palavra mais relevante (ex: \textit{\enquote{pesquisa}}), mas penalize termos semanticamente muito próximos (como \textit{\enquote{pesquisas}}), favorecendo a escolha de uma palavra subsequente que, embora ainda relevante, cubra uma faceta diferente do tópico.

A aplicação do \gls{mmr} como \texttt{representation\_model} (passado diretamente para a instância do \gls{bertopic}) busca garantir que a saída final do modelo seja um conjunto de rótulos de tópicos semanticamente diversos, menos redundantes e mais humanamente inteligíveis, resolvendo uma das fraquezas centrais do \gls{c-tf-idf} puro.

\subsection[Geração e Exportação para Visualização (WizMap)]{Geração e Exportação para Visualização (WizMap)}
\label{ssec:exportacao_wizmap}

A camada final da arquitetura é responsável pela interface com o usuário. A implementação utilizou a biblioteca wizmap para processar os artefatos gerados pelo núcleo (coordenadas UMAP, metadados e rótulos refinados) e gerar os arquivos de visualização.

Primeiramente, foi construída uma estrutura de dados unificada. As coordenadas bidimensionais (x, y) de cada publicação, que foram calculadas pelo \gls{umap} (Seção \ref{sssec:reducao_dimensionalidade}), foram consolidadas. A elas, foram associados os metadados originais (como o título da publicação) e os resultados da modelagem (o ID do tópico atribuído e o rótulo textual refinado pelo \gls{mmr}).

Para garantir a clareza na interface final, os rótulos dos tópicos passaram por uma etapa de pós-processamento para remover prefixos numéricos (ex: \texttt{1\_pesquisa\_dados} $\rightarrow$ \texttt{pesquisa dados}), tornando-os mais legíveis. Adicionalmente, foi formatado um texto de \textit{tooltip} (dica de contexto) para cada publicação, permitindo ao usuário final inspecionar o título e o tópico de um ponto específico ao interagir com o mapa.

Por fim, a biblioteca \texttt{wizmap} foi utilizada para processar essa estrutura de dados consolidada. Esta ferramenta gerou os dois arquivos \gls{json} essenciais para a renderização do mapa, conforme a arquitetura de visualização descrita na Seção \ref{sec:visualizacao_dados}:

\begin{enumerate}
    \item \textbf{Um arquivo de dados brutos:} Contendo a lista completa de todas as publicações, suas coordenadas 2D e o texto do \textit{tooltip} personalizado.
    \item \textbf{Um arquivo de grade (grid):} Contendo a estrutura de dados \textit{quadtree} de multi-resolução. Este arquivo é a inovação técnica do \gls{wizmap}, pois permite ao navegador renderizar de forma eficiente milhões de pontos e agregar rótulos de forma hierárquica em diferentes níveis de \textit{zoom}.
\end{enumerate}

Estes dois arquivos \gls{json} representam o produto final do \textit{pipeline} de engenharia de dados, prontos para serem hospedados (conforme Seção \ref{ssec:pipeline_artefato}) e consumidos pela interface \textit{html} do \gls{wizmap}, que renderiza o mapa de conhecimento interativo.