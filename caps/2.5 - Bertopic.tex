\section{BERTopic: Uma Abordagem Moderna}
\label{sec:bertopic}

As limitações das abordagens tradicionais de modelagem de tópicos, especialmente sua dependência do paradigma \textit{bag-of-words} e a falha em capturar o contexto semântico, motivaram o desenvolvimento de novos métodos. Pesquisas recentes indicam a viabilidade de tratar a modelagem de tópicos como uma tarefa de \textit{clustering} (agrupamento) de \textit{embeddings}, notavelmente nos trabalhos que introduziram o \textit{Top2Vec} \cite{Angelov_2020} e em estudos comparativos como o de \citeonline{Sia_2020}.

Nesse contexto, \citeonline{Grootendorst_2022} propôs o \gls{bertopic}, um modelo que estende a abordagem de \textit{clustering} ao introduzir uma variação do \gls{tf-idf} baseada em classes para extrair representações de tópicos. O \gls{bertopic} funciona como um \textit{pipeline} modular que consiste em três etapas principais: 1) geração de \textit{embeddings} de documentos, 2) \textit{clustering} desses \textit{embeddings} e 3) representação dos tópicos com \gls{c-tf-idf} \cite[p. 1-2]{Grootendorst_2022}.

A Figura \ref{fig:bertopic_pipeline} ilustra o fluxo geral dessa arquitetura.

\begin{figure}[H]
    \centering
    % Nota: Figura 2 do seu PDF (p. 20)
    % A fonte original é \cite{Jung_2024}
    \includegraphics[width=0.4\textwidth]{figs/Bertopic_arch.png} 
    \caption{Diagrama esquemático do \textit{pipeline} BERTopic.}
    \label{fig:bertopic_pipeline}
    \legend{Fonte: \citeonline[p. 7, Traduzido]{Jung_2024}}
\end{figure}

Na primeira etapa, \textit{Document embeddings}, os documentos são convertidos em representações vetoriais (embeddings). O \gls{bertopic} utiliza nativamente a biblioteca \gls{sbert} (\textit{Sentence-BERT}) proposta por \citeonline{Reimers_2019}, garantindo que documentos semanticamente similares sejam posicionados próximos no espaço vetorial \cite[p. 2]{Grootendorst_2022}. 

A segunda etapa, \textit{Dimension reduction}, é o \textit{reduction} desses \textit{embeddings} de alta dimensionalidade. Para que os algoritmos de \textit{reduction} funcionem de forma eficiente, é necessário primeiro combater a \enquote{maldição da dimensionalidade} (\textit{curse of dimensionality}), um fenômeno onde as distâncias entre os pontos se tornam pouco significativas em espaços com muitas dimensões \cite[p. 2]{Grootendorst_2022}. Para isso, o \gls{bertopic} emprega o \gls{umap} \cite{McInnes_2018}.

Antes de passar para as próximas etapas, precisamos contextualizar o \gls{umap}, que é uma técnica de redução de dimensionalidade que se destaca por preservar tanto a estrutura local quanto a estrutura global dos dados em um espaço de dimensão reduzida\footnote{O \gls{umap} é fundamentado em geometria Riemanniana e topologia algébrica. Ele constrói uma representação topológica dos dados em alta dimensão e busca uma representação em baixa dimensão que tenha uma estrutura topológica o mais equivalente possível \cite[p. 3-4]{McInnes_2018}.} \cite[p. 2-3]{Grootendorst_2022}. A Figura \ref{fig:umap_params} demonstra o impacto de seus dois principais hiperparâmetros.

\begin{figure}[H]
    \centering
    % Nota: Figura 3 do seu PDF (p. 21)
    % A fonte original é \cite{McInnes_2018}
    \includegraphics[width=0.7\textwidth]{figs/UMAP.png} 
    \caption{Diagrama ilustrativo do \gls{umap}, demonstrando a relação entre os hiperparâmetros \texttt{n\_neighbors} e \texttt{min\_dist} e a representação visual dos dados.}
    \label{fig:umap_params}
    \legend{Fonte: \citeonline[p. 24]{McInnes_2018}}
\end{figure}

Conforme ilustrado na Figura \ref{fig:umap_params}, o parâmetro \texttt{n\_neighbors} (número de vizinhos) controla o equilíbrio entre a preservação da estrutura global (valores altos) e local (valores baixos). O parâmetro \texttt{min\_dist} (distância mínima) ajusta a densidade dos agrupamentos, determinando a proximidade entre os pontos no espaço de baixa dimensionalidade.

Com os vetores em dimensão reduzida pelo \gls{umap}, a etapa seguinte da Figura \ref{fig:bertopic_pipeline}, \textit{Document clustering}, é o \textit{clustering} através do \gls{hdbscan} \cite{McInnes_2018}. A escolha deste método justifica-se pelas limitações dos algoritmos tradicionais de particionamento, como o \textit{K-Means}\footnote{O \textit{K-Means} é um dos algoritmos de \textit{clustering} mais populares. Ele particiona $n$ observações em $k$ agrupamentos, onde cada observação pertence ao \textit{cluster} cujo centro (média) é o mais próximo. Sua simplicidade é uma vantagem, mas ele assume \textit{clusters} de forma esférica e sensibilidade à inicialização dos centroides \cite{MacQueen_1967}.}.

O \textit{K-Means} assume que todos os agrupamentos (clusters) possuem formato esférico e densidades similares, além de forçar a inclusão de todos os pontos em algum grupo . No entanto, dados reais de publicações científicas raramente seguem esse padrão: tópicos podem ter formatos irregulares e muitos documentos podem não pertencer a nenhum tema específico (ruído).

A Figura \ref{fig:hdbscan_dataset} apresenta um cenário sintético que ilustra exatamente esse desafio de \enquote{densidade variável e ruído}, típico de dados não estruturados. Ao analisar a Figura \ref{fig:hdbscan_dataset}, observa-se a coexistência de três situações distintas no mesmo conjunto de dados:
\begin{enumerate}
    \item \textbf{Clusters de Alta Densidade:} Agrupamentos compactos (topo e esquerda), representando temas muito específicos e coesos.
    \item \textbf{Clusters de Baixa Densidade:} Agrupamentos mais dispersos (direita), representando temas mais amplos ou menos consolidados.
    \item \textbf{Ruído (\textit{Noise})} Pontos isolados espalhados pelo fundo, que não se conectam claramente a nenhum grupo.
\end{enumerate}

O HDBSCAN supera esse desafio por ser um algoritmo baseado em densidade. Diferentemente de métodos que buscam apenas a distância até um centro, o HDBSCAN identifica \enquote{ilhas} de alta densidade em um \enquote{mar} de pontos dispersos. Essa característica permite que o algoritmo:
\begin{itemize}
    \item Identifique clusters de formatos arbitrários e densidades variadas simultaneamente;
    \item Classifique pontos isolados como outliers (ruído), atribuindo-lhes o rótulo -1, em vez de força-los a integrar um tópico incoerente.
\end{itemize}

A Figura \ref{fig:hdbscan_dataset} ilustra a capacidadade de identificar agrupamentos de densidades e formas variadas, demonstrando o tipo de desafio que o algoritmo HDBSCAN é capaz de superar, como a identificação de agrupamentos de densidades e formas variadas, além de tratar outliers de forma eficiente. Essa característica é especialmente relevante em contextos de produção científica, onde coexistem tanto publicações centrais com alta densidade de tópicos quanto trabalhos periféricos ou com temas emergentes.

\begin{figure}[H]
    \centering
    % Nota: Figura 4 do seu PDF (p. 22)
    % A fonte original é \cite{Campello_2013}
    \includegraphics[width=0.4\textwidth]{figs/HDBSCAN.png} 
    \caption{Figura ilustrativa de um \textit{dataset} sintético com quatro \textit{clusters} e ruído de fundo.}
    \label{fig:hdbscan_dataset}
    \legend{Fonte: \citeonline[p. 16]{Campello_2013}}
\end{figure}

As duas etapas finais do \textit{pipeline}, é onde ocorre a geração da representação dos tópicos. Abordagens anteriores, como o \textit{Top2Vec} \cite{Angelov_2020}, baseiam-se em encontrar o centroide (o ponto médio) do \textit{cluster} e identificar as palavras mais próximas a ele. \citeonline{Grootendorst_2022} argumenta que essa abordagem é falha, pois \enquote{um \textit{cluster} nem sempre se situa dentro de uma esfera ao redor de um centroide} \cite[p. 1, Traduzido]{Grootendorst_2022}.

Para resolver o cénario dos clusters ao redor do centroide, o \gls{bertopic} introduz o \gls{c-tf-idf} (\textit{Class-based Term Frequency-Inverse Document Frequency}). A abordagem primeiro trata todos os documentos dentro de um \textit{cluster} (tópico) como um único documento concatenado. Em seguida, modifica a fórmula padrão do \gls{tf-idf} para operar a nível de classe, e não de documento.

O \gls{tf-idf} clássico é definido por \citeonline{Joachims_1997} como:
\begin{equation}
W_{t,d} = tf_{t,d} \cdot \log\left(\frac{N}{df_{t}}\right)
\label{eq:tfidf_classico}
\end{equation}
onde $W_{t,d}$ é a pontuação da palavra $t$ no documento $d$, $tf_{t,d}$ é a frequência da palavra $t$ no documento $d$, $N$ é o número total de documentos e $df_{t}$ é o número de documentos que contêm a palavra $t$.

O \gls{c-tf-idf} adapta essa lógica, onde a frequência do termo ($tf$) é calculada para a palavra $t$ dentro da classe $c$ inteira (o \textit{cluster} de documentos concatenados). A frequência inversa do documento ($idf$) é substituída pela \enquote{frequência inversa da classe}, que mede a importância da palavra $t$ em relação a todas as outras classes. A fórmula é então ajustada para:
\begin{equation}
W_{t,c} = tf_{t,c} \cdot \log\left(1 + \frac{A}{tf_{t}}\right)
\label{eq:c-tfidf}
\end{equation}
onde $tf_{t,c}$ é a frequência da palavra $t$ na classe $c$, $A$ é o número médio de palavras por classe (total de palavras dividido pelo número de classes), e $tf_{t}$ é a frequência total da palavra $t$ em todas as classes \cite[p. 3]{Grootendorst_2022}. O resultado é uma lista de palavras que destaca os termos que são mais distintivos e representativos de um tópico específico.

Embora eficaz, o \gls{c-tf-idf} pode gerar palavras-chave redundantes (ex: \enquote{modelo}, \enquote{modelagem}). O \citeonline[p. 8]{Grootendorst_2022} sugere que isso pode ser resolvido \enquote{aplicando \gls{mmr} às n palavras principais de um tópico}. O \gls{mmr}, introduzido por \citeonline{Carbonell_1998}, é uma técnica projetada especificamente para otimizar o equilíbrio entre relevância e diversidade na recuperação de informações. O algoritmo funciona de forma iterativa: ele primeiro seleciona o termo de maior relevância (maior pontuação \gls{c-tf-idf}); em seguida, para cada termo candidato subsequente, ele aplica uma penalidade com base na similaridade desse candidato com os termos já selecionados. O resultado é um conjunto de palavras-chave que não apenas representa o tema central, mas também cobre diferentes facetas semânticas desse tema, aumentando significativamente a interpretabilidade humana.

Contudo, mesmo com representações de tópicos robustas e rótulos semanticamente diversos, analisar a estrutura latente e as inter-relações de centenas de tópicos em um \textit{corpus} massivo permanece um desafio. A geração de um modelo de tópicos é apenas a primeira etapa; a descoberta de conhecimento emerge da capacidade de explorar esses resultados de forma intuitiva. Isso destaca a necessidade de técnicas que superem listas estáticas e permitam uma análise exploratória, um desafio que é central no campo da Visualização Científica e de Dados.